<!DOCTYPE html>
<html data-markdown-preview-plus-context="html-export">
  <head>
    <meta charset="utf-8" />
    <title>summary.md</title>
    <style>.emoji {
  max-width: 1em !important;
}
del {
  text-decoration: none;
  position: relative;
}
del::after {
  border-bottom: 1px solid black;
  content: '';
  left: 0;
  position: absolute;
  right: 0;
  top: 50%;
}
ul.contains-task-list li.task-list-item {
  position: relative;
  list-style-type: none;
}
ul.contains-task-list li.task-list-item input.task-list-item-checkbox {
  position: absolute;
  transform: translateX(-100%);
  width: 26px;
}
span.critic.comment {
  position: relative;
}
span.critic.comment::before {
  content: '\1f4ac';
  position: initial;
}
span.critic.comment > span {
  display: none;
}
span.critic.comment:hover > span {
  display: initial;
  position: absolute;
  top: 100%;
  left: 0;
  border: 1px solid;
  border-radius: 5px;
  max-height: 4em;
  overflow: auto;
}
span.critic.comment:focus > span {
  display: initial;
  text-decoration: underline;
  position: initial;
  top: auto;
  left: auto;
  border: initial;
  border-radius: initial;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
  background-color: transparent;
}

body {
  overflow: initial !important;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  line-height: 1.6;
  word-wrap: break-word;
  padding: 30px;
  font-size: 16px;
  color: #333;
  background-color: #fff;
}
body > *:first-child {
  margin-top: 0 !important;
}
body > *:last-child {
  margin-bottom: 0 !important;
}
body a:not([href]) {
  color: inherit;
  text-decoration: none;
}
body .absent {
  color: #c00;
}
body .anchor {
  position: absolute;
  top: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}
body .anchor:focus {
  outline: none;
}
body h1,
body h2,
body h3,
body h4,
body h5,
body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}
body h1 .octicon-link,
body h2 .octicon-link,
body h3 .octicon-link,
body h4 .octicon-link,
body h5 .octicon-link,
body h6 .octicon-link {
  display: none;
  color: #000;
  vertical-align: middle;
}
body h1:hover .anchor,
body h2:hover .anchor,
body h3:hover .anchor,
body h4:hover .anchor,
body h5:hover .anchor,
body h6:hover .anchor {
  padding-left: 8px;
  margin-left: -30px;
  text-decoration: none;
}
body h1:hover .anchor .octicon-link,
body h2:hover .anchor .octicon-link,
body h3:hover .anchor .octicon-link,
body h4:hover .anchor .octicon-link,
body h5:hover .anchor .octicon-link,
body h6:hover .anchor .octicon-link {
  display: inline-block;
}
body h1 tt,
body h2 tt,
body h3 tt,
body h4 tt,
body h5 tt,
body h6 tt,
body h1 code,
body h2 code,
body h3 code,
body h4 code,
body h5 code,
body h6 code {
  font-size: inherit;
}
body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}
body h1 .anchor {
  line-height: 1;
}
body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}
body h2 .anchor {
  line-height: 1;
}
body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}
body h3 .anchor {
  line-height: 1.2;
}
body h4 {
  font-size: 1.25em;
}
body h4 .anchor {
  line-height: 1.2;
}
body h5 {
  font-size: 1em;
}
body h5 .anchor {
  line-height: 1.1;
}
body h6 {
  font-size: 1em;
  color: #777;
}
body h6 .anchor {
  line-height: 1.1;
}
body p,
body blockquote,
body ul,
body ol,
body dl,
body table,
body pre {
  margin-top: 0;
  margin-bottom: 16px;
}
body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}
body ul,
body ol {
  padding-left: 2em;
}
body ul.no-list,
body ol.no-list {
  padding: 0;
  list-style-type: none;
}
body ul ul,
body ul ol,
body ol ol,
body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}
body li > p {
  margin-top: 16px;
}
body dl {
  padding: 0;
}
body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}
body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}
body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}
body blockquote > :first-child {
  margin-top: 0;
}
body blockquote > :last-child {
  margin-bottom: 0;
}
body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}
body table th {
  font-weight: bold;
}
body table th,
body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}
body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}
body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}
body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
body .emoji {
  max-width: none;
}
body span.frame {
  display: block;
  overflow: hidden;
}
body span.frame > span {
  display: block;
  float: left;
  width: auto;
  padding: 7px;
  margin: 13px 0 0;
  overflow: hidden;
  border: 1px solid #ddd;
}
body span.frame span img {
  display: block;
  float: left;
}
body span.frame span span {
  display: block;
  padding: 5px 0 0;
  clear: both;
  color: #333;
}
body span.align-center {
  display: block;
  overflow: hidden;
  clear: both;
}
body span.align-center > span {
  display: block;
  margin: 13px auto 0;
  overflow: hidden;
  text-align: center;
}
body span.align-center span img {
  margin: 0 auto;
  text-align: center;
}
body span.align-right {
  display: block;
  overflow: hidden;
  clear: both;
}
body span.align-right > span {
  display: block;
  margin: 13px 0 0;
  overflow: hidden;
  text-align: right;
}
body span.align-right span img {
  margin: 0;
  text-align: right;
}
body span.float-left {
  display: block;
  float: left;
  margin-right: 13px;
  overflow: hidden;
}
body span.float-left span {
  margin: 13px 0 0;
}
body span.float-right {
  display: block;
  float: right;
  margin-left: 13px;
  overflow: hidden;
}
body span.float-right > span {
  display: block;
  margin: 13px auto 0;
  overflow: hidden;
  text-align: right;
}
body code,
body tt {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0, 0, 0, 0.04);
  border-radius: 3px;
}
body code:before,
body tt:before,
body code:after,
body tt:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}
body code br,
body tt br {
  display: none;
}
body del code {
  text-decoration: inherit;
}
body pre > code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}
body .highlight {
  margin-bottom: 16px;
}
body .highlight pre,
body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}
body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}
body pre {
  word-wrap: normal;
}
body pre code,
body pre tt {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}
body pre code:before,
body pre tt:before,
body pre code:after,
body pre tt:after {
  content: normal;
}
body kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}
span.critic.comment > span {
  background-color: #fff;
}
a {
  color: #337ab7;
}

.bracket-matcher .region {
  border-bottom: 1px dotted lime;
  position: absolute;
}
.line-number.bracket-matcher.bracket-matcher {
  color: #c5c8c6;
  background-color: rgba(255, 255, 255, 0.14);
}

.spell-check-misspelling .region {
  border-bottom: 2px dotted rgba(255, 51, 51, 0.75);
}
.spell-check-corrections {
  width: 25em !important;
}

pre.editor-colors {
  background-color: #1d1f21;
  color: #c5c8c6;
}
pre.editor-colors .invisible-character {
  color: rgba(197, 200, 198, 0.2);
}
pre.editor-colors .indent-guide {
  color: rgba(197, 200, 198, 0.2);
}
pre.editor-colors .wrap-guide {
  background-color: rgba(197, 200, 198, 0.1);
}
pre.editor-colors .gutter {
  background-color: #292c2f;
}
pre.editor-colors .gutter .cursor-line {
  background-color: rgba(255, 255, 255, 0.14);
}
pre.editor-colors .line-number.cursor-line-no-selection {
  background-color: rgba(255, 255, 255, 0.14);
}
pre.editor-colors .gutter .line-number.folded,
pre.editor-colors .gutter .line-number:after,
pre.editor-colors .fold-marker:after {
  color: #fba0e3;
}
pre.editor-colors .invisible {
  color: #c5c8c6;
}
pre.editor-colors .cursor {
  border-color: white;
}
pre.editor-colors .selection .region {
  background-color: #444;
}
pre.editor-colors .bracket-matcher .region {
  border-bottom: 1px solid #f8de7e;
  margin-top: -1px;
  opacity: .7;
}
.syntax--comment {
  color: #8a8a8a;
}
.syntax--entity {
  color: #FFD2A7;
}
.syntax--entity.syntax--name.syntax--type {
  text-decoration: underline;
  color: #FFFFB6;
}
.syntax--entity.syntax--other.syntax--inherited-class {
  color: #9B5C2E;
}
.syntax--keyword {
  color: #96CBFE;
}
.syntax--keyword.syntax--control {
  color: #96CBFE;
}
.syntax--keyword.syntax--operator {
  color: #EDEDED;
}
.syntax--storage {
  color: #CFCB90;
}
.syntax--storage.syntax--modifier {
  color: #96CBFE;
}
.syntax--constant {
  color: #99CC99;
}
.syntax--constant.syntax--numeric {
  color: #FF73FD;
}
.syntax--variable {
  color: #C6C5FE;
}
.syntax--invalid.syntax--deprecated {
  text-decoration: underline;
  color: #FD5FF1;
}
.syntax--invalid.syntax--illegal {
  color: #FD5FF1;
  background-color: rgba(86, 45, 86, 0.75);
}
.syntax--string .syntax--source,
.syntax--string .syntax--meta.syntax--embedded.syntax--line {
  color: #EDEDED;
}
.syntax--string .syntax--punctuation.syntax--section.syntax--embedded {
  color: #00A0A0;
}
.syntax--string .syntax--punctuation.syntax--section.syntax--embedded .syntax--source {
  color: #00A0A0;
}
.syntax--string {
  color: #A8FF60;
}
.syntax--string .syntax--constant {
  color: #00A0A0;
}
.syntax--string.syntax--regexp {
  color: #E9C062;
}
.syntax--string.syntax--regexp .syntax--constant.syntax--character.syntax--escape,
.syntax--string.syntax--regexp .syntax--source.syntax--ruby.syntax--embedded,
.syntax--string.syntax--regexp .syntax--string.syntax--regexp.syntax--arbitrary-repetition {
  color: #FF8000;
}
.syntax--string.syntax--regexp.syntax--group {
  color: #C6A24F;
  background-color: rgba(255, 255, 255, 0.06);
}
.syntax--string.syntax--regexp.syntax--character-class {
  color: #B18A3D;
}
.syntax--string .syntax--variable {
  color: #8A9A95;
}
.syntax--support {
  color: #FFFFB6;
}
.syntax--support.syntax--function {
  color: #DAD085;
}
.syntax--support.syntax--constant {
  color: #FFD2A7;
}
.syntax--support.syntax--type.syntax--property-name.syntax--css {
  color: #EDEDED;
}
.syntax--source .syntax--entity.syntax--name.syntax--tag,
.syntax--source .syntax--punctuation.syntax--tag {
  color: #96CBFE;
}
.syntax--source .syntax--entity.syntax--other.syntax--attribute-name {
  color: #FF73FD;
}
.syntax--entity.syntax--other.syntax--attribute-name {
  color: #FF73FD;
}
.syntax--entity.syntax--name.syntax--tag.syntax--namespace,
.syntax--entity.syntax--other.syntax--attribute-name.syntax--namespace {
  color: #E18964;
}
.syntax--meta.syntax--preprocessor.syntax--c {
  color: #8996A8;
}
.syntax--meta.syntax--preprocessor.syntax--c .syntax--keyword {
  color: #AFC4DB;
}
.syntax--meta.syntax--cast {
  color: #676767;
}
.syntax--meta.syntax--sgml.syntax--html .syntax--meta.syntax--doctype,
.syntax--meta.syntax--sgml.syntax--html .syntax--meta.syntax--doctype .syntax--entity,
.syntax--meta.syntax--sgml.syntax--html .syntax--meta.syntax--doctype .syntax--string,
.syntax--meta.syntax--xml-processing,
.syntax--meta.syntax--xml-processing .syntax--entity,
.syntax--meta.syntax--xml-processing .syntax--string {
  color: #8a8a8a;
}
.syntax--meta.syntax--tag .syntax--entity,
.syntax--meta.syntax--tag > .syntax--punctuation,
.syntax--meta.syntax--tag.syntax--inline .syntax--entity {
  color: #FF73FD;
}
.syntax--meta.syntax--tag .syntax--name,
.syntax--meta.syntax--tag.syntax--inline .syntax--name,
.syntax--meta.syntax--tag > .syntax--punctuation {
  color: #96CBFE;
}
.syntax--meta.syntax--selector.syntax--css .syntax--entity.syntax--name.syntax--tag {
  text-decoration: underline;
  color: #96CBFE;
}
.syntax--meta.syntax--selector.syntax--css .syntax--entity.syntax--other.syntax--attribute-name.syntax--tag.syntax--pseudo-class {
  color: #8F9D6A;
}
.syntax--meta.syntax--selector.syntax--css .syntax--entity.syntax--other.syntax--attribute-name.syntax--id {
  color: #8B98AB;
}
.syntax--meta.syntax--selector.syntax--css .syntax--entity.syntax--other.syntax--attribute-name.syntax--class {
  color: #62B1FE;
}
.syntax--meta.syntax--property-group .syntax--support.syntax--constant.syntax--property-value.syntax--css,
.syntax--meta.syntax--property-value .syntax--support.syntax--constant.syntax--property-value.syntax--css {
  color: #F9EE98;
}
.syntax--meta.syntax--preprocessor.syntax--at-rule .syntax--keyword.syntax--control.syntax--at-rule {
  color: #8693A5;
}
.syntax--meta.syntax--property-value .syntax--support.syntax--constant.syntax--named-color.syntax--css,
.syntax--meta.syntax--property-value .syntax--constant {
  color: #87C38A;
}
.syntax--meta.syntax--constructor.syntax--argument.syntax--css {
  color: #8F9D6A;
}
.syntax--meta.syntax--diff,
.syntax--meta.syntax--diff.syntax--header {
  color: #F8F8F8;
  background-color: #0E2231;
}
.syntax--meta.syntax--separator {
  color: #60A633;
  background-color: #242424;
}
.syntax--meta.syntax--line.syntax--entry.syntax--logfile,
.syntax--meta.syntax--line.syntax--exit.syntax--logfile {
  background-color: rgba(238, 238, 238, 0.16);
}
.syntax--meta.syntax--line.syntax--error.syntax--logfile {
  background-color: #751012;
}
.syntax--source.syntax--gfm {
  color: #999;
}
.syntax--gfm .syntax--markup.syntax--heading {
  color: #eee;
}
.syntax--gfm .syntax--link {
  color: #555;
}
.syntax--gfm .syntax--variable.syntax--list,
.syntax--gfm .syntax--support.syntax--quote {
  color: #555;
}
.syntax--gfm .syntax--link .syntax--entity {
  color: #ddd;
}
.syntax--gfm .syntax--raw {
  color: #aaa;
}
.syntax--markdown .syntax--paragraph {
  color: #999;
}
.syntax--markdown .syntax--heading {
  color: #eee;
}
.syntax--markdown .syntax--raw {
  color: #aaa;
}
.syntax--markdown .syntax--link {
  color: #555;
}
.syntax--markdown .syntax--link .syntax--string {
  color: #555;
}
.syntax--markdown .syntax--link .syntax--string.syntax--title {
  color: #ddd;
}

/*
 * Your Stylesheet
 *
 * This stylesheet is loaded when Atom starts up and is reloaded automatically
 * when it is changed and saved.
 *
 * Add your own CSS or Less to fully customize Atom.
 * If you are unfamiliar with Less, you can read more about it here:
 * http://lesscss.org
 */
/*
 * Examples
 * (To see them, uncomment and save)
 */
</style>

  </head>
  <body>
    <h1><center>Person Re-Identification</center></h1>
<p>新的论文改为在谷歌文档 <a href="                                                                                                    ">Person re-ID</a> 中总结（预览见下方），后续有时间的话会把旧总结迁移过去。</p>
<iframe width="100%" height="500" src="                                                                                                                                                                     "></iframe>
<details>
<summary>入门</summary>
<h4>论文：</h4>
<ul>
<li>An Improved Deep Learning Architecture for Person Re-identification</li>
<li>Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification</li>
<li>Gated Siamese Convolutional Neural Network Architecture for Huam Re-identification</li>
<li>In Defense of the Triplet Loss for Person Re-identification</li>
<li>Recurrent Convolutional Network for Video-based Person Re-identification</li>
<li>Person Re-identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function</li>
<li>Eliminating Background-bias for Robust Person Re-identification</li>
<li>Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</li>
</ul>
<h4>代码：</h4>
<ul>
<li>deep ReID [<a href="                                               ">code</a>]</li>
<li>Baseline Code (with bottleneck) for Person-reID (pytorch) [<a href="                                                         ">code</a>]</li>
</ul>
</details>
<details>
<summary>Image-based</summary>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:center">Tag</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:center">CUHK03</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Viewpoint Invariant Pedestrian Recognition with an Ensemble of Localized Features</td>
<td style="text-align:center">Hai Tao<br>University of California, Santa Cruz</td>
<td style="text-align:center">ECCV 2008</td>
<td style="text-align:center">-</td>
<td style="text-align:left">定义了一个特征空间，让机器学习算法去寻找最好的表达</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)<br>Feature Regions<br>Feature Binning</td>
<td style="text-align:center">L1 Distance</td>
<td style="text-align:left">使用了AdaBoost</td>
<td style="text-align:center">–</td>
<td style="text-align:left"><strong>VIPeR</strong> 12</td>
</tr>
<tr>
<td style="text-align:center">DeepReID: Deep Filter Pairing Neural Network for Person Re-identification</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2014</td>
<td style="text-align:center">-</td>
<td style="text-align:left">1. 学习到的特征对能编码光照变化<br>2. 提出一个新的数据集</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">网络输出为二分类，直接判断两者是否为同一个人</td>
<td style="text-align:center">manually 20.65<br>detected 19.89</td>
<td style="text-align:left"><strong>CUHK01</strong>(100 testID) 27.87</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification with Discriminatively Trained Viewpoint Invariant Dictionaries</td>
<td style="text-align:center">Richard J. Radke<br>RPI</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:center">Dictionary</td>
<td style="text-align:left">学习一个字典，能同时学习 Probe 和 Gallery 的表达</td>
<td style="text-align:center">Color Histograms<br>Schmid &amp; Gabor Filters</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 用LFDA为特征降维<br>2. 训练时，在特征向量的稀疏表达上加上明确的限制去训练一个字典<br>3. 在测试时，从库图片中找出与探测图片，两者的稀疏表达在欧氏距离最近的一个</td>
<td style="text-align:center">–</td>
<td style="text-align:left"><strong>PRID</strong> 40.6<br><strong>iLIDS</strong> 25.9</td>
</tr>
<tr>
<td style="text-align:center"><em>An Improved Deep Learning Architecture for Person Re-identification</em> [<a href="                                                                 ">code</a>]</td>
<td style="text-align:center">Tim K. Marks<br>MERL</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:center">Distance Metric</td>
<td style="text-align:left">新的块匹配方法</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. probe某区域块与同位置的邻域内gallery块皆做差分<br>2. 这样对于错位有一定的容忍性</td>
<td style="text-align:center">manually 54.74<br>detected 44.96</td>
<td style="text-align:left"><strong>CUHK01</strong> (100) 65 (486) 47.53<br><strong>VIPeR</strong> 34.81</td>
</tr>
<tr>
<td style="text-align:center">Deep Feature Learning with Relative Distance Comparison for Person Re-identification</td>
<td style="text-align:center">Hongyang Chao<br>Sun Yat-sen University</td>
<td style="text-align:center">PR 2015</td>
<td style="text-align:center">Triplet Sample Strategy</td>
<td style="text-align:left">基于深度学习的三路网络框架</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 在反向传播上做了优化，减少重复计算<br>2. 提供了构建Triplet三元组的方法</td>
<td style="text-align:center">–</td>
<td style="text-align:left"><strong>iLIDS</strong> 52.1<br> <strong>VIPeR</strong> 40.5</td>
</tr>
<tr>
<td style="text-align:center">Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification [<a href="                                       ">code</a>]</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:center">跨数据集</td>
<td style="text-align:left">当在多个数据集上学习时，会发现有一些神经元跨域表达而一些神经元只会对指定的域有效</td>
<td style="text-align:center">inception</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">通过比较某个神经元被置零前后loss的变化得到其是否为域敏感</td>
<td style="text-align:center">all 75.3</td>
<td style="text-align:left"><strong>CUHK01</strong>(485 testID) 66.6<br><strong>PRID</strong> 64</td>
</tr>
<tr>
<td style="text-align:center">Deep Neural Networks with Inexact Matching for Person Re-identification [<a href="                                                 ">code</a>]</td>
<td style="text-align:center">Anurag Mittal<br>IITM</td>
<td style="text-align:center">NIPS 2016</td>
<td style="text-align:center">Distance Metric</td>
<td style="text-align:left">用相关系数改进块相似性度量</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">对于Probe图片，将区域块与Gallery相应位置整个条带上的块做相关性系数计算</td>
<td style="text-align:center">manually 72.43<br>detected 72.04</td>
<td style="text-align:left"><strong>CUHK01</strong> (100) 81.23 (486) 65.04<br><strong>GRID</strong> 19.20</td>
</tr>
<tr>
<td style="text-align:center"><em>Gated Siamese Convolutional Neural Network Architecture for Huam Re-identification</em></td>
<td style="text-align:center">Gang Wang<br>Nanyang Technology University</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:center">Multi-level Attention</td>
<td style="text-align:left">提出一个门函数，通过对比图片的中层特征，能选择性增强精细的局部模式</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 双路结构，信息向上传播时会有门结构对特征进行选择<br>2. 门结构是用两者的特征差，借助高斯激活函数作为门值</td>
<td style="text-align:center">detected SQ 61.8 MQ 68.1</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R 65.88 mAP 39.55<br>MQ R 76.4 mAP 48.45<br><strong>VIPeR</strong> 37.8</td>
</tr>
<tr>
<td style="text-align:center">Semantics-Aware Deep Correspondence Structure Learning for Robust Person Re-identification</td>
<td style="text-align:center">Zhongfei Zhang<br>Zhejiang University</td>
<td style="text-align:center">IJCAL 2016</td>
<td style="text-align:center">-</td>
<td style="text-align:left">希望得到语义层面的图像表达</td>
<td style="text-align:center">改编的GoogleNet</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 双路融合为一路<br>2. 融合时对两者特征图分别构建金字塔特征图<br>3. 两者同尺度的特征取Max操作<br>4. 网络最后为二分类</td>
<td style="text-align:center">manually 80.2</td>
<td style="text-align:left"><strong>CUHK01</strong><br>(100 testID) 89.60<br>(486 testID) 76.54<br><strong>VIPeR</strong>(316 testID) 44.62</td>
</tr>
<tr>
<td style="text-align:center">Joint Learning of Single-image and Cross-image Representations for Person Re-identification</td>
<td style="text-align:center">Lei Zhang<br>Sun Yat-sen University</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:center">-</td>
<td style="text-align:left">将分类与验证相结合</td>
<td style="text-align:center">三层CNN</td>
<td style="text-align:center">Euclidean Distance+RankSVM</td>
<td style="text-align:left">1. 可以应用到双路网络与三路网络</td>
<td style="text-align:center">detected 52.17</td>
<td style="text-align:left"><strong>CUHK01</strong>(100 testID) 71.80<br><strong>VIPeR</strong> 35.76</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function</td>
<td style="text-align:center">Nanning Zheng<br>Xi’an JiaoTong University</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">1. 多通道学习整个身体与局部肢体<br>2. 用新的Triplet loss 去加强学习效果</td>
<td style="text-align:center">三层CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. Conv1对整个图提特征<br>2. 沿高度将Conv1分为四部分，分别用四个支路去学习局部特征<br>3. 再训练一个支路对Conv1直接学习<br>4. 五个支路输出特征级联<br>5. 不仅要求正样本对距离小于负样本对，还要求正样本对距离小于某个值</td>
<td style="text-align:center">–</td>
<td style="text-align:left"><strong>iLIDS</strong> 60.4<br><strong>PRID</strong> 22.0<br><strong>VIPeR</strong> 47.8<br><strong>CUHK01</strong>(486 testID) 53.7</td>
</tr>
<tr>
<td style="text-align:center">A siamese Long Short-Term Memory Architecture for Human Re-identification</td>
<td style="text-align:center">Gang Wang<br>University of Sydney</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:center">上下文</td>
<td style="text-align:left">通过上下文信息增强区分局部特征的能力</td>
<td style="text-align:center">LOMO</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">沿高度分为多个水平条带，并依次送入LSTM聚合</td>
<td style="text-align:center">detected 57.3</td>
<td style="text-align:left"><strong>Market1501</strong><br>MQ R 61.6 mAP 35.3<br><strong>VIPeR</strong> 42.4</td>
</tr>
<tr>
<td style="text-align:center">End-to-End Comparative Attention Networks for Person Re-identification</td>
<td style="text-align:center">Shucheng Yan<br></td>
<td style="text-align:center">TIP 2016</td>
<td style="text-align:center">Attention</td>
<td style="text-align:left">学习到注意力模型</td>
<td style="text-align:center">AlexNet VGG</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">CNN提取特征，再送入LSTM，用LSTM每一步的h学习mask矩阵，利用mask矩阵与原特征相乘得到有注意力的结果</td>
<td style="text-align:center">manually 77.6<br>detected 69.2</td>
<td style="text-align:left"><strong>CUHK01</strong> (100 testID) 87.2<br><strong>Market1501</strong><br>SQ R 60.3 mAP 35.9<br>MQ R 72.1 mAP 47.9<br><strong>VIPeR</strong> 54.1</td>
</tr>
<tr>
<td style="text-align:center">Multi-scale Triplet CNN for Person Re-identification [<a href="                                                                                                                                                         ">pdf</a>]</td>
<td style="text-align:center">Tao Mei<br>MSRA</td>
<td style="text-align:center">MM 2016</td>
<td style="text-align:center">Multi-scale</td>
<td style="text-align:left">结合多尺度信息</td>
<td style="text-align:center">AlexNet</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 整体为三路网络，修改的Triplet Loss外加正样本对的距离Loss<br>2. 多尺度信息体现在将原始图片下采样为两种尺度，加上原尺寸，共三种尺寸，分别训三个AlexNet，再融合</td>
<td style="text-align:center">–</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R 45.1<br>MQ 55.4</td>
</tr>
<tr>
<td style="text-align:center">Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">利用行人的身体关键点辅助识别</td>
<td style="text-align:center">inception</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 将人的肢体分为不同的粒度去提取特征，三个大区域，四个小区域<br>2. 先用CPM对图片提取关节点，再根据关节点位置框出7个区域<br> 3. 在提取特征时考虑了不同粒度，融合不同粒度时也有先后之分</td>
<td style="text-align:center">all 88.5</td>
<td style="text-align:left"><strong>CUHK01</strong>(485 testID) 79.9<br><strong>PRID</strong> 67<br><strong>VIPeR</strong> 53.8<br><strong>3DPeS</strong> 62.1<br><strong>iLIDS</strong> 66.3<br><strong>Market1501</strong> SQ R 76.9</td>
</tr>
<tr>
<td style="text-align:center">Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification</td>
<td style="text-align:center">Kaiqihuang<br>CRIPAC &amp; NLPR, CASIA</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">学习更好的基于整个身体和局部身体的特征</td>
<td style="text-align:center">四层CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 用不同膨胀率的卷积核构建类inception结构，可得到不同大小的感受野<br>2. 用google的STN网络学习抠图，得到身体划分，划分为三个部分<br>3. 整体与局部的特征级联的到最后的表达</td>
<td style="text-align:center">manually 74.21<br>detected 67.99</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R 80.31 mAP 57.53<br>MQ R 86.79 mAP 66.7<br><strong>MARS</strong> SQ 71.77 MQ 83.03</td>
</tr>
<tr>
<td style="text-align:center">Beyond Triplet Loss: a Deep Quadruplet Network for Person Re-identification</td>
<td style="text-align:center">Kaiqi Huang<br>CRIPAC&amp;NLPR,CASIA</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:center">Triplet Loss</td>
<td style="text-align:left">四路网络+基于阈值的hard negative mining</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 测试时相当于二分类<br>2. 相对于三路网络增加了负样本与负样本的限制</td>
<td style="text-align:center">manually 75.53</td>
<td style="text-align:left"><strong>CUHK01</strong><br>(486 testID) 62.55<br>(100 testID) 81<br><strong>VIPeR</strong> 49.05</td>
</tr>
<tr>
<td style="text-align:center"><em>A Multi-task Deep Network for Person Re-identification</em></td>
<td style="text-align:center">Kaiqihuang<br>CRIPAC &amp; NLPR, CASIA</td>
<td style="text-align:center">AAAI 2017</td>
<td style="text-align:center">Multi-task</td>
<td style="text-align:left">多任务框架，二分类模型与排序模型同时做，同时也考虑了跨数据集的半监督学习</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">主体为三路网络，在其基础上，正对与负对也被用来训练一个二分类分支</td>
<td style="text-align:center">manually 74.68</td>
<td style="text-align:left"><strong>CUHK01</strong> (100) 78.5 (486) 59.67<br><strong>VIPeR</strong> 47.47<br><strong>iLIDS</strong> 58.38<br><strong>PRID</strong> 31</td>
</tr>
<tr>
<td style="text-align:center">Point to Set Similarity Based Deep Feature Learning for Person Re-identification</td>
<td style="text-align:center">Nanning Zheng<br> Xi’an Jiaotong University</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:center">Loss</td>
<td style="text-align:left">用点对集合来作为相似性度量</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. P2S改进的Triplet Loss + Contrastive Loss<br>2. 身体局部与整体的不同尺度学习</td>
<td style="text-align:center">–</td>
<td style="text-align:left"><strong>3DPeS</strong> 71.16<br><strong>CUHK01</strong> 77.34<br><strong>PRID</strong> 70.71<br><strong>Market1501</strong> SQ R 70.72 mAP 44.27<br>MQ R 85.78 mAP 55.73</td>
</tr>
<tr>
<td style="text-align:center">Consistent-Aware Deep Learning for Person Re-identification in a Cammera Network</td>
<td style="text-align:center">Jie Zhou<br>Tsinghua University</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:center">摄像头之间的关系</td>
<td style="text-align:left">最大化整个网络的人物匹配，而不是每次只关注一个图片对或几个图片对</td>
<td style="text-align:center">训练好的Domain Guide Model</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. 用网络提取的特征计算余弦距离，构建相似性矩阵C，行为一个摄像头下的人，列为另一个摄像头下的人<br>2. 构建C相对应的邻接矩阵，同一个人则为1否则为0<br>3. 训练时要最大化C与H的点点相乘，并使H的预测值与真实值误差尽可能小<br>4. C与H是交替优化的</td>
<td style="text-align:center">–</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R 73.84 mAP 47.11<br>MQ R 80.85 mAP 55.58</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification by Deep Joint Learning of Multi-Loss Classification</td>
<td style="text-align:center">Shaogang Gong<br>Queen Mary University of London</td>
<td style="text-align:center">IJCAI 2017</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">局部特征与整体特征一起学习</td>
<td style="text-align:center">改编的ResNet</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 单路网络，作为多分类任务<br>2. 先在ImageNet上预训练，再在目标数据集上训练<br>3. 在两层公用结构之后便分为两部分，一部分是整体特征学习，另一部分是四个水平条带对应学习局部特征<br>5. 这两个部分各自有一个分类loss，并不融合,并用实验表明不融合更好<br>6. 测试时将两部分特征级联作为最后表达</td>
<td style="text-align:center">manually 83.2<br>detected 80.6</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ ( R 85.1 ) ( mAP 65.5 )<br>MQ ( R 89.7 ) ( mAP 74.5 )<br><strong>CUHK01</strong><br>(100) SQ 87.0 MQ 91.2 <br>(486) SQ 69.8 MQ 76.7<br><strong>VIPeR</strong> 50.2<br><strong>GRID</strong> 37.5</td>
</tr>
<tr>
<td style="text-align:center"><em>Deeply-Learned Part-Aligned Representations for Person Re-identification</em> [<a href="                                   ">code</a>]</td>
<td style="text-align:center">Jingdong Wang<br>MSRA</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:center">Part<br>Attention</td>
<td style="text-align:left">学习对特征图加权，以此选出特征图中较为显著的区域</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 用GoogleNet提取的特征 HxWxC,用一个卷积层学习k个HxW的特征图:M<br>2. M 可以视为mask,即为对原特征的不同部分的响应，用每个HxW响应对原HxWxC加权得到新的k个HxWxC<br>对于新的特征图，经过Global Average Pooling和全连接层得到固定长度表达</td>
<td style="text-align:center">manually 85.4<br>detected 81.6</td>
<td style="text-align:left"><strong>Market1501</strong> SQ R 81.0 mAP 63.4<br><strong>CUHK01</strong><br>(100) 88.5<br>(486) 75<br><strong>VIPeR</strong> 48.7</td>
</tr>
<tr>
<td style="text-align:center">Multi-scale Deep Learning Architectures for Person Re-identification</td>
<td style="text-align:center">Xiangyang Xue<br>Fudan University</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:center">Multi-scale</td>
<td style="text-align:left">利用多尺度特征来充分利用图片的细节信息，同时在级联多尺度信息时，利用加权做了选择筛选</td>
<td style="text-align:center">GoogleNet修改版</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 整体为双路网路，两个分支各有一个分类Loss，中间是将两路的特征相减后取平方值，经一个全连接层得到最后表达，再接二分类。<br>2. 多尺度信息是利用不同大小的卷积核实现的，整体结构类似GoogleNet<br>3. 对于最后的特征，每个channel都学习一个对应的加权值。加权值是两个支路共享的,直接学习，未加先验和限制。<br>4. <strong>从作者的实验中可以看出多尺度网络对于detected的图片效果依旧很好，可能图中只有小部分是人的，但是因为多尺度而能被网络注意到</strong></td>
<td style="text-align:center">manually 76.87<br>detected 75.64</td>
<td style="text-align:left"><strong>CUHK01</strong> (100)79.01<br><strong>VIPeR</strong> 43.03</td>
</tr>
<tr>
<td style="text-align:center">AlignedReID: Surpassing Human-Level Performance in Person Re-identification [<a href="                                    ">pdf</a>] [<a href="                                                                 ">code</a>]</td>
<td style="text-align:center">Jian Sun<br>Face++</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:center">Distance Metric</td>
<td style="text-align:left">用局部特征去帮助全局特征的学习</td>
<td style="text-align:center">ResNet50-X</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. triplet loss(in denfense of the triplet loss for ReID)<br>2. 局部特征是最后特征图水平方向GAP。全局特征是水平垂直都GAP<br>3. 比较两者局部特征使用了动态规划<br> 4. 训练时loss由全局特征距离与局部特征距离共同组成<br>5. 用两个这种网络协同学习<br>6. 测试时只是用全局特征算欧氏距离</td>
<td style="text-align:center">manually 96.1</td>
<td style="text-align:left"><strong>Market1501</strong> SQ R 94.0 mAP 91.2<br><strong>MARS</strong> SQ R 87.5 mAP 85.6<br><strong>CUHK-SYSU</strong> R 95.3 mAP 93.7</td>
</tr>
<tr>
<td style="text-align:center">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis [<a href="                                       ">code</a>]</td>
<td style="text-align:center">Lu Sheng<br>CUHK</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:center">Attention</td>
<td style="text-align:left">不同语义层次的Attention</td>
<td style="text-align:center">更改之后的Inception-v2</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. 一个主分支，三个旁分支，每个分支分别从不同的inception block得到的特征中提取attention map。然后给三个block的特征加mask。2. 需要分阶段训练，因为且产生attention的block之前的网络不微调</td>
<td style="text-align:center">91.8</td>
<td style="text-align:left"><strong>VIPeR</strong> 56.6<br><strong>Market1501</strong> SQ R1 76.9</td>
</tr>
<tr>
<td style="text-align:center">Deep Siamese Network with Multi-level Similarity Perception for Person Re-identification [<a href="                                                                                                               ">pdf</a>] [<a href="                                                 ">code</a>]</td>
<td style="text-align:center">Yaowu Chen, Xian-Sheng Hua<br>Zhejiang University, Alibaba</td>
<td style="text-align:center">MM 2017</td>
<td style="text-align:center">-</td>
<td style="text-align:left">在low-level上也加入对匹配的优化，组成多层次的优化网络</td>
<td style="text-align:center">inception</td>
<td style="text-align:center">Euclidean Distance + L2norm</td>
<td style="text-align:left">1. 在第一个卷积层之后加入对低层次特征块的匹配的优化，用的结构主要是NIPS16那篇求相关性系数的方法。<br>2. 对正负样本块的相关性系数，设置阈值，做置零操作，主要是防止噪声块以及无区分性块的影响<br>3. 优化目标是使正样本先关系数最大化吗，负样本相关系数最小化<br>4. 前期不加low-level的匹配优化，训练稳定之后再加入low-level的优化。<be>5. 测试的时候并不需要low-level优化网络<br>6. <strong>只对anchor图片计算分类损失</strong></be></td>
<td style="text-align:center">manually 85.7<br>detected 83.6</td>
<td style="text-align:left"><strong>CUHK01</strong> (100)79.3 (486)63.7<br><strong>Market1501</strong> SQ R 81.9 mAP 63.6</td>
</tr>
<tr>
<td style="text-align:center"><em>Deep Representation Learning with Part Loss for Person Re-Identification</em> [<a href="                                    ">pdf</a>]</td>
<td style="text-align:center">Qi Tain<br>UTSA</td>
<td style="text-align:center">Arxiv 2017.06</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">针对人的不同部位设置不同的loss，让得到的特征更有区分性</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">–</td>
<td style="text-align:left">1. 对于最后的特征图，找出每一个特诊图为响应最大的点的位置，并将这些点聚为k类。<br>2. 对于每一类的特征图，得到均值特征图，并标准化，大于0.5的点视为前景，最小的闭合矩形框作为 part bounding box.<br>3. 对于C个特征图，k个框，经过ROI Pooling,得到k个Cx4x4的特征，并分别训练k个part loss。<br>4. part loss同时也会提升全局特征表达。<br>5. 最后的特征表达是全局特征与局部特征级联</td>
<td style="text-align:center">manually 82,75</td>
<td style="text-align:left"><strong>Market1501</strong> SQ mAP 69.3 R1 88.2<br><strong>VIPeR</strong> 56.65</td>
</tr>
<tr>
<td style="text-align:center"><em>Harmonious Attention Network for Person Re-Identification</em> [<a href="                                    ">pdf</a>]</td>
<td style="text-align:center">Shaogang Gong<br>QMUL</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Attention</td>
<td style="text-align:left">在空间上，通道上做soft attention，又用STN选出T个区域做Hard的attention</td>
<td style="text-align:center">Inception</td>
<td style="text-align:center">L2norm + Euclidean Distance</td>
<td style="text-align:left">1. 空间注意力：对通道取均值，只保留空间分辨率，在用一个卷积核,resize,缩放参数得到最后的空间注意力值。通道注意力：用的是squeeze-and-excitation结构。<br>2. 在每个Inception模块后面，用STN学习T个仿射变换矩阵，对此模块前面的特征图进行采样，得到T种特征图，分别对应T个分支。分支网络用单独的Inception结构训，每一模块之后都会加上STN对总网络此层的T个采样。最后通过级联全连接得到512维的特征。<br>3. 总网络的特征与分支网络的特征级联得到总的特征表达，为1024维。<br>4. 无数据增强和预训练。</td>
<td style="text-align:center">(767/700)<br>manually R1 44.4 mAP 41.0<br>detected R1 41.7 mAP 38.6</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 91.2 mAP 75.7<br>MQ R1 93.8 mAP 82.8<br><strong>DukeMTMC-ReID</strong> R1 80.5 mAP 63.8</td>
</tr>
<tr>
<td style="text-align:center">Multi-Channel Pyramid Person Matching Network for Person Re-Identification</td>
<td style="text-align:center">Xi li<br>Zhejiang University&amp;Alibaba</td>
<td style="text-align:center">AAAI 2018</td>
<td style="text-align:center">CNN+手工特征</td>
<td style="text-align:left">分别学习语义表达和颜色纹理表达。语义表达用CNN，而颜色纹理基于手工特征，再输入到网络中，用两个全连接综合这两方面信息预测是否为同一个人</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 语义部分，输入RGB信息，用Googlenet提取特征，再将两个人的特征级联起来以融合信息，用atrous卷积得到3种尺度的特征表达，将级联后的信息通过卷积和池化得到最后表达。<br>2. 颜色纹理表达与语义表达的处理在模型结构上相同，只是输入时手工特征。<br>3. 将语义特征与颜色纹理特征级联再通过全连接等进行分类。</td>
<td style="text-align:center">manually 86.36<br>detected 81.88</td>
<td style="text-align:left"><strong>CUHK01</strong> (100)93.45 (486)78.95<br><strong>VIPeR</strong> 50.13<br><strong>PRID2011</strong> 34<br><strong>iLIDS</strong> 62.69</td>
</tr>
<tr>
<td style="text-align:center">SVDNet for Pedestrian Retrieval</td>
<td style="text-align:center">Shengjin Wang<br>Liang Zheng</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:center">-</td>
<td style="text-align:left">去最后的全连接层权值矩阵的相关性，得到更高层度的正交性</td>
<td style="text-align:center">CaffeNet or ResNet50</td>
<td style="text-align:center">Euclidean distance</td>
<td style="text-align:left">1. 用在倒数第二个全连接层<br>2. 步骤: 向网络中加入一个线性全连接层，并微调至收敛。将线性全连接层的权值矩阵进行SVD分解，<br>W = USV<br>用W和其自身的转置矩阵乘作为本征层。固定本征层，并微调网络至收敛。再不固定本征层，微调整个网络至收敛</td>
<td style="text-align:center">detected R1 81.8 mAP 84.8</td>
<td style="text-align:left"><strong>Market1501</strong> R1 82.3 mAP 62.1<br><strong>DukeMTMC-reID</strong> R1 76.7 mAP 56.8</td>
</tr>
<tr>
<td style="text-align:center">Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification</td>
<td style="text-align:center">Jason Kuen<br>NTUS</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Attention</td>
<td style="text-align:left">提出一个用多个序列算距离的方法</td>
<td style="text-align:center">DenseNet-121</td>
<td style="text-align:center">Euclidean distance</td>
<td style="text-align:left">1. 序列的定义是特征图中将不同位置的特征视为一个序列。<br>2. 同一个图片内，利用attention用各个位置的特征的加权值代表自己，加权由转移网络根据此位置生成的特征和各个位置的特征的內积大小决定。<br>3. 不同图片之间的也可以用相似方法得到。<br>4. 最后的距离是特征距离取平均</td>
<td style="text-align:center">-</td>
<td style="text-align:left"><strong>Market1501</strong>SQ R1 91.42 mAP 76.62<br><strong>DukeMTMC</strong>R1 81.82 mAP 64.58<br><strong>MARS</strong>R1 78.74 mAP 62.26</td>
</tr>
<tr>
<td style="text-align:center">Efficient and Deep Person Re-Identification using Multi-Level Similarity</td>
<td style="text-align:center">Ngai-Man Cheung<br>SUTD</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Distance Metric</td>
<td style="text-align:left">多层次的相似性度量</td>
<td style="text-align:center">六层CNN</td>
<td style="text-align:center">Softmax Score与Cosine Distance的加权结果</td>
<td style="text-align:left">1. 双路结构<br>2. 第二层与第三层输出的特征划分为三个水平条带，每个水平条带用一个STN提取一个关键patch，此三个块级联起来再经过两个卷积层得到的表达被用来接对比loss。<br>3. 用一个STN得到的特征在另一个特征图上做卷积得到相似性度量，级联三个个part的卷积得到的相似性度量得到此层特征之间的相似性度量。再级联多层特征的相似性度量，再用三层卷积得到最后的二分类结果。<br>4. 最后的距离是由二分类的结果与两侧用于对比损失函数的特征之间的距离的的加权值</td>
<td style="text-align:center">manually 87.5<br>detected 86.45</td>
<td style="text-align:left"><strong>CUHK01</strong> 88.2<br><strong>VIPeR</strong> 50.10</td>
</tr>
<tr>
<td style="text-align:center">Adversarially Occluded Samples for Person Re-identification</td>
<td style="text-align:center">Kaiqi Huang<br>UCAS</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Data Augmentation</td>
<td style="text-align:left">对重要区域添加mask以让模型关注到其他的区域</td>
<td style="text-align:center">IDE(ResNet)</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 先正常训练模型<br>2. 选择矩形区域区域遮挡，再用相同的配置训练模型<br>3. 选择方法：固定遮挡框的大小，划窗遮挡，对于导致正确标签预测概率降低的位置，以降低的数值为其重要性表达，各个位置的重要性标准化之后，以之为概率随机选择遮挡</td>
<td style="text-align:center">detected(767) R1 54.56 mAP 56.09</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 88.66 mAP 83.3<br>MQ R1 92.5 mAP 88.6<br><strong>DukeMTMC-reID</strong> R1 84.11 mAP 78.19</td>
</tr>
<tr>
<td style="text-align:center">Attention-Aware Compositional Network for Person Re-identification</td>
<td style="text-align:center">Wanli Ouyang<br>The University of Sydney</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Part Attention</td>
<td style="text-align:left">利用关键点检测，得到不同部位的特征，并加权</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">–</td>
<td style="text-align:left">1.主要有两路，一路用于检测关键点，一路用于提特征，两路先分别训练，然后合起来训练。<br>2. pose分支现在MPII上预训练，输出关键点位置，part场，以及根据关键点划分的三个大块。part场是real time那篇预测两个关键点之间的矩形区域的，这里和划分出的三个大块一样，相当于一个mask用于对另一个支路的特征做掩模，通过各个mask掩模得到的特征pooling再级联起来，再加权(利用mask的响应以及特征学到)，然后就得到了最后的特征。</td>
<td style="text-align:center">manually 91.39 detected 89.51</td>
<td style="text-align:left"><strong>CUHK01</strong> 88.07<br><strong>Market1501</strong><br>SQ R1 88.69 mAP 82.96<br>MQ R1 92.16 mAP 87.32<br><strong>CUHK03-NP</strong><br>labeld R1 81.86 mAP 81.61<br>detected R1 79.14 mAP 78.37<br><strong>DukeMTMC-reID</strong> R1 76.84 mAP 59.25<br><strong>SenseReID</strong> 41.37</td>
</tr>
<tr>
<td style="text-align:center"><em>Deep Group-shuffling Random Walk for Person Re-identification</em> [<a href="                                                ">code</a>]</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">re-ranking</td>
<td style="text-align:left">将re-ranking嵌入到网络中</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Euclidean Distance + reranking</td>
<td style="text-align:left">1. 计算gallery之间的相似度以及query与gallery之间的相似度，并用随机游走更新query与gallery之间的相似度，并用此结果做预测以及计算loss<br>2. 将特征分为几组，每一组内计算各种相似度，组与组之间可以组合用于随机游走</td>
<td style="text-align:center">manually mAP 94.0 R1 94.9</td>
<td style="text-align:left"><strong>Market1501</strong> SQ mAP 82.5 R1 92.7<br><strong>DukeMTMC</strong> mAP 66.4 R1 80.7</td>
</tr>
<tr>
<td style="text-align:center"><em>Eliminating Background-bias for Robust Person Re-identification</em></td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Data Augmentation</td>
<td style="text-align:left">探究背景的影响</td>
<td style="text-align:center">Inception</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. 先在其他数据上预训练一个人的parsing的网络，可以提人体的掩模。固定参数，之后不再训练<br>2. 训练主分支<br>3. 对于一张图片，将掩模上下分为三个部分，分别对应头，上身，下身。二值化并对第一个inception的特征做mask。三个分支用单独的网络训练，此时主分支不变。<br>4. 主分支与三个支路一起训练。<br>数据增强：输入数据时，随机选择是否替换背景</td>
<td style="text-align:center">all 92.5</td>
<td style="text-align:left"><strong>CUHK01</strong> 82.5<br><strong>VIPeR</strong> 51.9<br><strong>3DPeS</strong> 65.6<br><strong>Market1501</strong>SQ R1 81.2</td>
</tr>
<tr>
<td style="text-align:center">End-to-End Deep Kronecker-Product Matching for Person Re-identification [<a href="                                                ">code</a>]</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Distance Metric</td>
<td style="text-align:left">构建匹配用的net，能起到对齐的效果</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 两张图片的特征x与y。分别计算各个空间位置的特征之间的內积，作为相似性度量，对于y中位置为(p,q)的点，利用x中(p,q)位置的特征与y中各个位置的相似性，做加权平均，得到y的(p,q)为位置重排的特征。对y重排之后，计算x与y的差值。利用x的特征算空间mask对差值加权。<br>2. 卷积加上采样，构建三种尺度，分别对三种尺度下两个特征做以上操作，最后特征级联得到最后预测</td>
<td style="text-align:center">manually R1 93.4 mAP 89.2</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 90.1 mAP 75.3<br><strong>DukeMTMC</strong> R1 80.3 mAP 63.2</td>
</tr>
<tr>
<td style="text-align:center">Human Semantic Parsing for Person Re-identification</td>
<td style="text-align:center">Muhittin Gokmen<br>MEF</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">human parsing</td>
<td style="text-align:center">Inception-V3</td>
<td style="text-align:center">-</td>
<td style="text-align:left">一路产生parsing，一路提特征，然后做mask。用了很多re-id的数据集做辅助训练</td>
<td style="text-align:center">manually R1 91.8</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 94.63 mAP 90.96<br><strong>DukeMTMC-reID</strong> R1 88.96 mAP 84.99</td>
</tr>
<tr>
<td style="text-align:center">Mask-guided Contrastive Attention Model for Person Re-Identification</td>
<td style="text-align:center">Wanli Ouyang<br>sydney</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">检测行人轮廓，区分前景与背景</td>
<td style="text-align:center">MGCAM</td>
<td style="text-align:center">Re-ranking</td>
<td style="text-align:left">1. 用预训练的FCN提取人的轮廓，用以区分前景和背景<br>2. 输入是RGB与mask的叠加，是4个通道<br>3. 一个主分支，一个body分支，一个背景分支。主分支第二个模块提出的特征被送到两个分支，送入之前用此分支学习到的mask区分前景与背景，分别mask。<br>4. mask用用真实mask作为label来帮助训练，此外要求主分支的特征与body分支的特征相近，与背景分支的距离相远</td>
<td style="text-align:center">manually R1 50.14 mAP 50.21<br>detected R1 46.71 mAP 46.87</td>
<td style="text-align:left"><strong>Market1501</strong>SQ R1 83.79 mAP 74.33<br><strong>MARS</strong> R1 77.17 mAP 71.17</td>
</tr>
<tr>
<td style="text-align:center">Resource Aware Person Re-identification across Multiple Resolutions</td>
<td style="text-align:center">Vincent Chen<br>Tsinghua University</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Multiple Resolutions</td>
<td style="text-align:left">从网络的不同层提取不同语义的特征组成最后的特征，并可以设计规则提前结束，即直接利用低语义特征计算相似度，节省开支</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">-</td>
<td style="text-align:left">1. 从ResNet50的四个阶段提取特征，分别过两个全连接，映射到一个128维的特征，训练四个加权参数，得到加权平均值，作为最后表达，其次四个特征也用triplet loss辅助训练。<br>2. 设计规则，通过判断阶段距离，决定是否继续提取特征。</td>
<td style="text-align:center">manually R1 73.8 mAP 74.7<br>detected R1 70.6 mAP 71.6</td>
<td style="text-align:left"><strong>Market1501</strong>SQ R1 90.9 mAP 86.7<br><strong>MARS</strong> R1 85.1 mAP 81.9<br><strong>Duke</strong> R1 84.4 mAP 80.0</td>
</tr>
<tr>
<td style="text-align:center">Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</td>
<td style="text-align:center">Wei Li<br>QMUL</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Transfer Learning</td>
<td style="text-align:left">通过ID与属性的融合学习，得到更好地跨数据集效果</td>
<td style="text-align:center">MobileNet</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 源数据集有身份标签和属性标签，目标数据集没有标签<br>2. 网络有两个分支，一个提取属性特征，一个提取身份特征<br>3. encoder与decoder结构，先用encoder将身份特征嵌入到与属性特征相同的维度，然后在decoder。其中低维的身份特征经过sigmoid要与属性特征相似，用loss去限制。decoder出的身份特征要与原身份特征相似，同样用loss限制。<br>4. 扩展到目标数据集时，先提取属性特征作为伪标签去更新编解码结构和属性分支</td>
<td style="text-align:center">-</td>
<td style="text-align:left"><strong>Market1501</strong>SQ R1 58.2 mAP 26.5<br><strong>DukeMTMC</strong> R1 44.3 mAP 23.0<br><strong>VIPeR</strong> 38.5<br><strong>PRID</strong> 34.8</td>
</tr>
<tr>
<td style="text-align:center">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</td>
<td style="text-align:center">Shiliang Zhang<br>Peking University</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Transfer Learning<br>Background Change</td>
<td style="text-align:left">提出新的数据集，用CyCleGAN变换背景</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">-</td>
<td style="text-align:left">1. 用预训练网络提取人的mask。然后训练CycleGAN，要求变换前后人物的方差要小。</td>
<td style="text-align:center">-</td>
<td style="text-align:left">-</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification with Cascaded Pairwise Convolutions</td>
<td style="text-align:center">Zhenzhong Chen<br>Wuhan University</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Architecture</td>
<td style="text-align:left">两个特征相互交织算出最后的相似性</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">-</td>
<td style="text-align:left">两个特征分别经过一个卷积再相加，重复两次，分别得到两个融合之后的特征，以此类推</td>
<td style="text-align:center">manually(100) 88.18<br>detected(100) 85.85</td>
<td style="text-align:left"><strong>CUHK01</strong>(100) 93.04<br><strong>Market1501</strong>SQ R1 83.07 mAP 69.48<br><strong>DukeMTMC-reID</strong>R1 76.44 mAP 59.49</td>
</tr>
<tr>
<td style="text-align:center">Multi-Level Factorisation Net for Person Re-Identification</td>
<td style="text-align:center">Tao Xiang<br>QMUL</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Architecture</td>
<td style="text-align:left">Multi-level</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 多层block。每个block内有k个小块，小块的输入相同，并有单独一个小块学习k个加权值，得k个小块的加权平均，然后输出值与输入值相加得到block的输出。相当于跨层连接。最后每一block的加权值被级联起来也作为一个特征，与最后一个block的输出都经过一个映射，映射到同一维度取平均，再经过一个全连接得到最后的表达</td>
<td style="text-align:center">detected(100) 82.8<br>manually(half) R1 54.7 mAP 49.2<br>detected(half) R1 52.8 mAP 47.8</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 90.0 mAP 74.3<br>MQ R1 92.3 mAP 82.4<br><strong>DukeMTMC-reID</strong> R1 81.0 mAP 62.8</td>
</tr>
<tr>
<td style="text-align:center">Learning Discriminative Features with Multiple Granularity for Person Re-Identification [<a href="                                   ">code</a>]</td>
<td style="text-align:center">Xi Zhou<br>SITU</td>
<td style="text-align:center">Arxiv 2018.07</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">分多个水平条带，整体特征与局部特征级联</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">-</td>
<td style="text-align:left">1. 从ResNet50的res_conv4_1之后开始分为三个支路，这三个支路是ResNet50在res_conv4_1之后网络复制的，各个支路都是预训练的参数初始化，但是参数不共享<br>2. 第一个每一个支路都算一个全局特征，除此之外，第二个支路最后的特征分为上下两部分，得到两个块特征，第三个支路分为三个块。<br>3. 其中part特征各用一个分类损失函数训练，整体特征用triplet loss训练</td>
<td style="text-align:center">manually R1 68.0 mAP 67.4<br>detected R1 66.8 mAP 66.0</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 95.7 mAP 86.9<br>MQ R1 96.9 mAP 90.7<br><strong>DukeMTMC-reID</strong> R1 88.7 mAP 78.4</td>
</tr>
<tr>
<td style="text-align:center">Weighted Bilinear Coding over Salient Body Parts for Person Re-identification</td>
<td style="text-align:center">Haibin Ling<br>Temple University</td>
<td style="text-align:center">Arxiv 2018.04</td>
<td style="text-align:center">Part Mask</td>
<td style="text-align:left">用多个attention mask对空间加权</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">-</td>
<td style="text-align:left">1. 先获取一些列的attention mask。对特征进行加权，然后利用Bilinear coding获取最后的表达。<br>2. Bilinear coding是将一维向量的转置与自己相乘，得到各维度相互关联的特征。</td>
<td style="text-align:center">manually R1 50.1 mAP 47.7<br>detected R1 43.9 mAP 42.1</td>
<td style="text-align:left"><strong>Market1501</strong> SQ R1 84.5 mAP 68.7<br><strong>DukeMTMC-reID</strong> R1 76.2 mAP 56.9</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification with Deep Similarity-Guided Graph Neural Network</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:center">1. Graph Neural Network<br>2. Distance Metric</td>
<td style="text-align:left">借助GNN，获得更好的相似性度量</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 先预训练一个二分类网络，然后开始训练SGGNN。<br>2. 通过两层全连接得到辅助表达，再 利用二分类网络得到两个图片之间的相似性，归一化之后作为加权值，对辅助表达求加权平均值，然后对当前特征进行更新。得到最后表达之后再训练二分类</td>
<td style="text-align:center">manually(100) R1 95.3 mAP 94.3</td>
<td style="text-align:left"><strong>Market1501</strong> SQ R1 92.3 mAP 82.8<br><strong>DukeMTMC</strong> SQ R1 81.1 mAP 68.2</td>
</tr>
<tr>
<td style="text-align:center">In Defense of the Classification Loss for Person Re-identification</td>
<td style="text-align:center">Yan Lu<br>MSRA</td>
<td style="text-align:center">Arxiv 2018.09</td>
<td style="text-align:center">Multi Branch</td>
<td style="text-align:left">将特征分为几组，每一组接一个分类损失函数</td>
<td style="text-align:center">PCB</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 提取特征，沿通道分为若干组，每一组分别接一个全连接（权值共享），再分别接分类损失函数（含全连接）。</td>
<td style="text-align:center">detected(700) R1 61.6 mAP 54.8</td>
<td style="text-align:left"><strong>Market1501</strong>SQ R1 93.5 mAP 79.8<br><strong>DukeMTMC</strong> R1 84.7 mAP 68.1</td>
</tr>
<tr>
<td style="text-align:center">FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification</td>
<td style="text-align:center">Hongsheng Li<br>CUHK</td>
<td style="text-align:center">NIPS 2018</td>
<td style="text-align:center">Pose GAN</td>
<td style="text-align:left">希望获得不含有pose信息的图片特征，这样比较相似性时能减少姿态的干扰。</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">-</td>
<td style="text-align:left">1. 给定一个pose和一个图片，生成此人的指定pose的图片，要求生成后的图片还是原来的人，生成后的图片的pose是指定的pose,相同人生成的指定的pose图片应当尽可能相似。<br>2. 测试时，只用image encoder提取到的特征</td>
<td style="text-align:center">detected(100) R1 92.6 mAP 91.3</td>
<td style="text-align:left"><strong>Market1501</strong> R1 90.5 mAP 77.7<br><strong>DukeMTMC-reID</strong> R1 80.0 mAP 64.5</td>
</tr>
<tr>
<td style="text-align:center">Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-identification</td>
<td style="text-align:center">Xinggang Wang<br>HUST</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:center">Attention</td>
<td style="text-align:left">渐进采样策略，由易到难。网络用了attention</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Euclidean distance</td>
<td style="text-align:left">1. attention是根据SeNet的思想改进过来的，由仅通道维度扩展到通道与空间维度。2. 渐进采样会给样本分配根据距离定义的采样概率。3. 使用了重排序</td>
<td style="text-align:center">700 manually R1 69.0 mAP 63.9<br>detected R1 65.5 mAP 60.5<br>100 manually R1 93.8 detected R1 92.4</td>
<td style="text-align:left"><strong>Market1501</strong> SQ R1 93.1 mAP 82.3<br>MQ R1 95.4 mAP 87.5<br><strong>DukeMTMC-reID</strong> R1 84.9 mAP 71.8</td>
</tr>
<tr>
<td style="text-align:center">Part-Aligned Bilinear Representations for Person Re-identification</td>
<td style="text-align:center">Jingdong Wang<br>MSRA</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:center">Part</td>
<td style="text-align:left">利用part帮助计算相似性</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">Net</td>
<td style="text-align:left">1. 用预训练的关键点检测器得到关键点相应图。计算两个人各个part相应图之间的相似性，用以加权特征描述之间的相似性</td>
<td style="text-align:center">manually 91.5 detected 88.0</td>
<td style="text-align:left"><strong>Market1501</strong><br> SQ R1 93.4 mAP 89.9<br>MQ R1 95.4 mAP 93.1<br><strong>CUHK01</strong> (100) R1 90.4 <br>(486) R1 80.7<br><strong>DukeMTMC</strong>R1 88.3 mAP 83.9<br><strong>MARS</strong> R1 85.1 mAP 83.9</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Video-based</summary>
<h2>Video</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:center">Tag</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Feature</th>
<th style="text-align:center">Fusion</th>
<th style="text-align:center">Metric</th>
<th style="text-align:center">Detail</th>
<th style="text-align:left">iLIDS</th>
<th style="text-align:center">PRID</th>
<th style="text-align:center">MARS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Person Re-identification by Video Ranking</td>
<td style="text-align:center">Shenjing Wang<br>Queen Mary University of London</td>
<td style="text-align:center">ECCV 2014</td>
<td style="text-align:center">Key frames</td>
<td style="text-align:left">1. 能从有噪声的帧序列中选出关键帧<br> 2. 学习一个视频排序函数</td>
<td style="text-align:left">HOG3D</td>
<td style="text-align:center">不融合，放在特征池中，供比较.</td>
<td style="text-align:center">1. 学习一个矩阵，矩阵与两人特征差的乘即代表距离<br>2. 将两人的特征两两比较距离，最大的距离代表最后的距离</td>
<td style="text-align:center">1. 只取图片的下半部分，定义能量函数FEP，能量值随帧变化<br> 2. 对每个图，在极大值与极小值点前后取共10帧<br></td>
<td style="text-align:left">28.9</td>
<td style="text-align:center">23.3</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Sparse Re-ID: Block Sparsity for Person Re-identification</td>
<td style="text-align:center">Richard J. Radke<br>RPI</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:center">Dictionary</td>
<td style="text-align:left">Probe图片的特征向量可以近似看成<br>处于Gallery图片特征向量所处的embedding space</td>
<td style="text-align:left">Color Histograms<br>Schmid &amp; Gabor Filters</td>
<td style="text-align:center">级联构成字典的一部分</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">构建一个字典</td>
<td style="text-align:left">24.9</td>
<td style="text-align:center">35.1</td>
<td style="text-align:center">—</td>
</tr>
<tr>
<td style="text-align:center">A Spatio-temporal Appearance Representation for <br>Video-based Pedestrian Re-identification</td>
<td style="text-align:center">Rui Huang<br>Shandong University &amp; UCAS</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:center">时间对齐</td>
<td style="text-align:left">1. 处理时间空间对齐问题 <br>2. 空间上按身体部位划分为不同的块<br>3. 时间上用FEP</td>
<td style="text-align:left">Fiser vector</td>
<td style="text-align:center">1. 空间上6部分特征级联得到帧表达<br>2. 时间上级联得到视频表达</td>
<td style="text-align:center">最近邻分类器</td>
<td style="text-align:center">1. 用傅里叶变换对FEP去噪 <br>2. 空间上按头，四肢，上身，将身体分为6个部分</td>
<td style="text-align:left">44.3</td>
<td style="text-align:center">64.1</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Deep Recurrent Convolutional Networks for Video-based <br>Person Re-identification: An End-to-End Approach</td>
<td style="text-align:center">Chunhua Shen<br>The University of Adelaide</td>
<td style="text-align:center">Arxiv 2016</td>
<td style="text-align:center">GRU</td>
<td style="text-align:left">同时学习时间空间特征和相似性矩阵</td>
<td style="text-align:left">四层卷积网络</td>
<td style="text-align:center">GRU+average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">GRU中用卷积操作代替了全连接层</td>
<td style="text-align:left">42.6</td>
<td style="text-align:center">49.8</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Top-push Video-based Person Re-identification</td>
<td style="text-align:center">Weishi Zheng<br>Sun Yat-sen University</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:center">Loss</td>
<td style="text-align:left">不同的人有相似的表现而引发类间距离较小</td>
<td style="text-align:left">HOG3D+color histograms+LBP</td>
<td style="text-align:center">级联</td>
<td style="text-align:center">马氏距离</td>
<td style="text-align:center">学习马氏距离的矩阵M</td>
<td style="text-align:left">56.33</td>
<td style="text-align:center">56.74</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification by Exploiting <br>Spatio-temporal Cues and Multi-view Metric Learning</td>
<td style="text-align:center">Yuanyan Wang<br>Bejing Forestry University</td>
<td style="text-align:center">IEEE SRL 2016</td>
<td style="text-align:center">Distance Metric</td>
<td style="text-align:left">提出新的时空特征及匹配方法</td>
<td style="text-align:left">从标准化的光流能量图中提取LBP</td>
<td style="text-align:center">级联</td>
<td style="text-align:center">马氏距离</td>
<td style="text-align:center">优化马氏距离中的W</td>
<td style="text-align:left">69.13</td>
<td style="text-align:center">66.78</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification via Recurrent Feature Aggregation [<a href="                                       ">code</a>]</td>
<td style="text-align:center">Xiaokang Yang<br>Shanghai Jioa Tong University</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:left">用LSTM融合时间信息</td>
<td style="text-align:left">LBP+HSV+lab color channels</td>
<td style="text-align:center">LSTM融合后再级联各时间输出</td>
<td style="text-align:center">RankSVM</td>
<td style="text-align:center">对噪声鲁棒性强</td>
<td style="text-align:left">49.3</td>
<td style="text-align:center">64.1</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center"><em>Recurrent Convolutional Network for Video-based Person Re-identification</em> [<a href="                                                              ">code</a>]</td>
<td style="text-align:center">Paul Miller<br>Queen’s University Belfast</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:center">RNN</td>
<td style="text-align:left">利用CNN提取空间特征，RNN提取空间特征</td>
<td style="text-align:left">CNN</td>
<td style="text-align:center">RNN + average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">Softmax Loss + Contrastive Loss</td>
<td style="text-align:left">58</td>
<td style="text-align:center">70</td>
<td style="text-align:center">R1 40</td>
</tr>
<tr>
<td style="text-align:center">Video-based Person Re-identification with Accumulative Motion Context</td>
<td style="text-align:center">JiaShi Feng<br>Hefei University of Technology</td>
<td style="text-align:center">TCSVT 2017</td>
<td style="text-align:center">光流</td>
<td style="text-align:left">用网络学习光流，并整合到网络中</td>
<td style="text-align:left">CNN</td>
<td style="text-align:center">RNN+average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">1. 先用光流训练一个网络，让其能预测光流<br>2. 将光流网络加入到原网络中一起使用</td>
<td style="text-align:left">65.3</td>
<td style="text-align:center">78</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Learning Compact Appearance Representation for Video-based Person Re-identification</td>
<td style="text-align:center">Kan Liu<br>Shandong University</td>
<td style="text-align:center">Arxiv 2017.02</td>
<td style="text-align:center">Key frames</td>
<td style="text-align:left">从若干帧中提取特征而不是使用整个视频</td>
<td style="text-align:left">五层卷积网络</td>
<td style="text-align:center">Max Pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">1. 利用对于不同视频段提取到的特征，可以得到两个人之间的平均距离和最小距离<br>2. FEP选取关键帧</td>
<td style="text-align:left">60.4</td>
<td style="text-align:center">83.3</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">See the Forest for the Trees: Joint Spatial and Temporal <br>Recurrent Neural Networks for Video-based Person Re-identification [<a href="                                                                   ">pdf</a>]</td>
<td style="text-align:center">Tieniu Tan<br>UCAS,CASIA,CEBSIT</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:center">-</td>
<td style="text-align:left">能挑出关键帧并充分利用环境信息</td>
<td style="text-align:left">CaffeNet</td>
<td style="text-align:center">TAM+SRM</td>
<td style="text-align:center">1. TAM输出特征间的标准化的欧氏距离<br>2.SRM输出的相似概率<br>3. 两个相似性度量的加权和</td>
<td style="text-align:center">1.时间循环网络对两个人各时间空间特征级联后的差值的六个方向用RNNN聚合，最后二分类，得到相似概率 <br>2. 时间上，每一步都接收所有时间的特征，学习加权值，得到特征的加权和，并将其送入RNN得到此时的表达，最终表达是各时间特征均值<br>3. 整体结构为三路与双路的结合</td>
<td style="text-align:left">55.2</td>
<td style="text-align:center">79.4</td>
<td style="text-align:center">R1 70.6<br>mAP 50.7</td>
</tr>
<tr>
<td style="text-align:center"><em>Quality Aware Network for Set to Set Recognition</em> [<a href="                                                    ">code</a>]</td>
<td style="text-align:center">Wanli Ouyang<br>University of Sydney</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:center">Quality</td>
<td style="text-align:left">能自动学到图片的质量并用以加权图片特征</td>
<td style="text-align:left">GoogleNet</td>
<td style="text-align:center">通过学习到的质量分数加权</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">代码中的升级版本<br>1. 三路网络，Triplet Loss,每一路又有一个分类Loss,正样本对又构建Contrastive loss(相当于只有正样本情况)<br>2. 对于每一个支路，都由GoogleNet组成，其后便是分类loss。每个支路中还有一个QAN网络，用于产生质量分数<br>3. QAN是两层卷积网络加全连接层，全连接输出维度为3，结构CPCPF，直接由原始图片数据学得<br>4. GoogleNet中间特征中沿高度均分得到三个特征，每个特征再均值池化压缩h维度。每个支路的QAN输出的3个数值标准化后分别对其加权<br>5. 加权后的特征经过L2 Norm便得到最后表达，进入Triplet Loss与Contrastive Loss</td>
<td style="text-align:left">68.0</td>
<td style="text-align:center">90.3</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-identification [<a href="                                                                                                               ">pdf</a>] [<a href="                                                                     ">code</a>]</td>
<td style="text-align:center">Pan Zhou<br>Huazhong University of Science and Technology</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:center">-</td>
<td style="text-align:left">在空间上与时间上都是注意力模型</td>
<td style="text-align:left">三层CNN</td>
<td style="text-align:center">RNN+注意力时间池化</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">1. 双路结构，分类loss+Contrastive Loss<br>2. 对每一个支路，输入为原始图片加光流，对于每一帧的特征用SPP得到不同尺度的特征并级联，得到单帧表达<br>3. 将每一帧的表达依次送入RNN，每一步的输出为每一帧的最终表达<br>4. 利用注意力模型得到每一帧的加权值，利用加权求和得到视频表达</td>
<td style="text-align:left">62</td>
<td style="text-align:center">77</td>
<td style="text-align:center">R1 44</td>
</tr>
<tr>
<td style="text-align:center">A Two Stream Siamese Convolutional Neural Network For Person Re-identification</td>
<td style="text-align:center">Dahjung Chuang<br>Purdue</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:center">-</td>
<td style="text-align:left">将光流与RGB分开，分别在两个Siamese网络中</td>
<td style="text-align:left">RNN+注意力时间池化</td>
<td style="text-align:center">光流与RGB的Euclidean Distance</td>
<td style="text-align:center">3层CNN</td>
<td style="text-align:center">在RNN-ReID结构上，用两个相同结构的Siamese网络，分别提取RGB与光流中的特征，loss与特征是两者的加权</td>
<td style="text-align:left">60</td>
<td style="text-align:center">78</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Region-based Quality Estimation Network for Large-scale Person Re-identification</td>
<td style="text-align:center">Shaofan Cai<br>SenseTime</td>
<td style="text-align:center">AAAI 2018</td>
<td style="text-align:center">Quality</td>
<td style="text-align:left">借助关键点检测，基于区域的质量估计，并提出新的视频数据集</td>
<td style="text-align:left">GoogleNet</td>
<td style="text-align:center">Region-based quality</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:center">1. 之前的数据集因为检测或跟踪失败而导致清洁度太低，人工标注的又对齐的太好<br>2. 新数据集特点：590000张图片，检测子检测，场景拥挤，年龄分布大<br>3. 用CPM检测关键点，产生上中下三个框，基于框预测三个框的质量分数<br>4. 对所有帧的同一个框的质量分数L1标准化，求特征加权和，最后级联三个框的特征</td>
<td style="text-align:left">76.1</td>
<td style="text-align:center">92.4</td>
<td style="text-align:center">R1 77.83 mAP 71.14</td>
</tr>
<tr>
<td style="text-align:center">Deep Cross-Modality Alignmeant for Multi-Shot Person Re-Identification [<a href="                                                                                                                ">pdf</a>]</td>
<td style="text-align:center">Xiaokang Yang<br>Shanghai Jiao Tong University</td>
<td style="text-align:center">MM 2017</td>
<td style="text-align:center">数据扩增</td>
<td style="text-align:left">现有的视频数据集较小，为了利用现有的基于图片的数据集，设计了伪序列生成的结构，利用一整图片生成一个视频</td>
<td style="text-align:left">三层CNN</td>
<td style="text-align:center">RNN + average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">1. 视频生成主要是依靠随机的Crop，随机选择剪切起始的点。随机性是利用马尔可夫链蒙特卡罗方法从一个固定坐标开始，一步步走动。<br>2. 为了模拟遮挡等复杂因素，在第一个卷积层的结果上加入了Dropout。<br>3. 直接使用单张图片预训练反而会使效果变差</td>
<td style="text-align:left">60</td>
<td style="text-align:center">80</td>
<td style="text-align:center">R1 63</td>
</tr>
<tr>
<td style="text-align:center">Data Generation for Improving Person Re-identification [<a href="                                                                                                                                                        ">pdf</a>]</td>
<td style="text-align:center">Zhiyong Gao<br>Shanghai Jiao Tong University</td>
<td style="text-align:center">MM 2017</td>
<td style="text-align:center">数据扩增</td>
<td style="text-align:left">为了解决视频数据集不充分的问题，提出两种结构，一种针对类内，能生成可保持物体运动信息的视频，另一种针对类间，可替换背景。</td>
<td style="text-align:left">三层CNN</td>
<td style="text-align:center">RNN + average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">1. 类内：<a href="                                    ">预测神经网络</a>，结构是四层的网络，每一层分为四个基础部分：输入卷积模块，循环表达模块，输出预测模块，误差表达模块。训练时，先自顶向下求循化表达模块R的值，在自下而上更新其他值。输入为T帧视频，输出为生成的T-1帧视频。<br>2. 类间：<a href="                                    ">背景置换网络</a></td>
<td style="text-align:left">66</td>
<td style="text-align:center">79</td>
<td style="text-align:center">–</td>
</tr>
<tr>
<td style="text-align:center">Three-Stream Convolutional Networks for Video-based Person Re-Identification</td>
<td style="text-align:center">Yi Pab<br>Southwest Jiaotong University</td>
<td style="text-align:center">Arxiv 2017.11</td>
<td style="text-align:center">-</td>
<td style="text-align:left">降低空间分辨率有很多方法，最大值池化，均值池化，增加卷积步距等等，这些结构对特征的利用情况不同。基于这一点，作者设计了多支路结构的·网络，充分利用这些结构的特点。</td>
<td style="text-align:left">四层CNN</td>
<td style="text-align:center">RNN + average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">1. 网络分三条支路，每个支路三层卷积，三个支路分别使用最大值池化，均值池化，增加卷积步距来降低分辨率。<br>2. 三条支路得到的特征大小相同，在宽度维度上拼接再经过一层卷积层和均值池化得到最后的表达，无全连接。<br>3. 作者经过试验表明，虽然可能某一条支路不如另一条支路效果好，但是共同使用时，依旧可以提升整体的性能。<br>4。 在宽度上叠加效果比在通道上叠加好。</td>
<td style="text-align:left">67.5</td>
<td style="text-align:center">79.7</td>
<td style="text-align:center">45.6</td>
</tr>
<tr>
<td style="text-align:center">Video Person Re-identification by Temporal Residual Learning</td>
<td style="text-align:center">Hongyu Wang</td>
<td style="text-align:center">Arxiv 2018.02.22</td>
<td style="text-align:center">BiLSTM</td>
<td style="text-align:left">利用STN做空间上的对齐，BiLSTM融合时间信息</td>
<td style="text-align:left">GoogleNet</td>
<td style="text-align:center">BiLSTM</td>
<td style="text-align:center">L2norm + Euclidean Distance</td>
<td style="text-align:center">1.只用分类来训练<br>2. 网络所有部分先用MARS预训练（主要是因为STN部分）</td>
<td style="text-align:left">57.7</td>
<td style="text-align:center">87.8</td>
<td style="text-align:center">79.3</td>
</tr>
<tr>
<td style="text-align:center">Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">Attention</td>
<td style="text-align:left">空间与时间上的显著性</td>
<td style="text-align:left">ResNet50(用图片reid数据集预训练)</td>
<td style="text-align:center">加权平均</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:center">1. 对输入视频，均分为6份，训练时是从每一份随机取出一个，组成6帧的视频送入网络，测试时用每一份的第一个构成6帧视频代表整个视频<br>2. 用ResNet提取特征后，训练k个空间注意力网络，每一个网络输入是空间上各个位置的特征，每一个位置输出一个分值，这样每个注意力网络会对每个空间位置得到一个分值，用softmax做处理取到最显著的位置。相当于训了k个mask，加权后空间取平均，则每一帧图像得到的特征是(K,C)大小的。<br>3. 对于整个视频特征是(D,K,C)。训一个打分模型，对于每一个C打一个分数，共有D*K个，时间维度上取softmax。对特征加权后取平均得到(K,C)大小的最后的特征表达</td>
<td style="text-align:left">80.2</td>
<td style="text-align:center">93.2</td>
<td style="text-align:center">R1 82.3<br>mAP 65.8</td>
</tr>
<tr>
<td style="text-align:center">Multi-shot Pedestrian Re-identification via Sequential Decision Making [<a href="                                             ">code</a>]</td>
<td style="text-align:center">Liqing Zhang<br>Shanghai Jiao Tong University</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">增强学习</td>
<td style="text-align:left">用增强学习做redi，在效率与准确率之间做平衡</td>
<td style="text-align:left">Inception-BN or AlexNet</td>
<td style="text-align:center">增强学习</td>
<td style="text-align:center">预测为同一个人与不同人的Q值得差</td>
<td style="text-align:center">1。 先用预训练的网络训并提取图片的特征。<br>2. Action：判断两张图片为同一个人，不同人，未确定。Reward: 预测正确为1，预测错误或者到了最大步数结果还是未确定时为-1，未确定时为0.2。Q设置为rt 与下一步的Q的最大值之和。<br>3. 送入增强学习部分的数据是当前特征，记忆加权特征与根据当前特征计算的手工特征(差值欧氏距离，均值)，然后提特征网络与后面增强学习网络一起训练</td>
<td style="text-align:left">60.2</td>
<td style="text-align:center">85.2</td>
<td style="text-align:center">71.2</td>
</tr>
<tr>
<td style="text-align:center">Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning [<a href="                                                  ">code</a>]</td>
<td style="text-align:center">Wanli Ouyang<br>The University of Sydney</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">One-Shot</td>
<td style="text-align:left">逐渐给无标签数据添加伪标签</td>
<td style="text-align:left">ResNet50</td>
<td style="text-align:center">Average Pooling</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:center">1. 先用有标签数据训练一个模型。<br>2. 依次扩大训练数据的大小，每次增大m。每一步，对于给定的数据集大小，选择伪数据与真实数据特征欧氏距离最小，然后决定使用那些数据，并打上与之最近的数据的标签，并训练。<br>3.迭代进行直至用完数据。</td>
<td style="text-align:left">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">R1 62.67 mAP 42.45<br><strong>DukeMTMC-VideoReID</strong> R1 72.79 mAP 63.23</td>
</tr>
<tr>
<td style="text-align:center">Video Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:center">1. Attention<br>2. Distance Metric</td>
<td style="text-align:left">将长视频等间距划分为短视频，短视频之间比较距离</td>
<td style="text-align:left">ResNet50</td>
<td style="text-align:center">加权平均</td>
<td style="text-align:center">FC+sigmoid</td>
<td style="text-align:center">1. 输入是RGB+光流，用ResNet50提取2048维的特征向量。<br>2. Co-attention embedding: value与key feature由全连接层生成，query feature是用LSTM聚合probe视频片段生成。query freature 也被用于gallery特征生成，即gallery的特征加权是与query相关的。</td>
<td style="text-align:left">85.4</td>
<td style="text-align:center">93.0</td>
<td style="text-align:center">R1 86.3<br> mAP 76.1</td>
</tr>
<tr>
<td style="text-align:center">SCAN: Self-and-Collaborative Attention Network for Video Person Re-identification</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">Arxiv 2018.07</td>
<td style="text-align:center">Distance Matric</td>
<td style="text-align:left">self attention + collaborative attention</td>
<td style="text-align:left">ResNet50</td>
<td style="text-align:center">加权平均</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:center">1. self attention: 用帧的特征与平均特征之间的关系建立加权值。<br>2. collaborative attention:用帧特征与其他视频平均特征的关系得到加权值。<br>3. 加权值经过softmax标准化，然后求和<br>4. 光流，不是直接级联，而是先经过一层卷积，然后与经过卷积后RGB图像的特征相加</td>
<td style="text-align:left">w/o optical 81.3<br>w/ optical 88.0</td>
<td style="text-align:center">w/o optical 92.0 <br>w/ 95.3</td>
<td style="text-align:center">w/o optical R1 86.6 mAP 76.7<br>w/ optical R1 87.2 mAP 77.2</td>
</tr>
<tr>
<td style="text-align:center">Video-based Person Re-identification via 3D Convolutional Networks and Non-local Attention</td>
<td style="text-align:center">Zhouwang Yang<br>USTC</td>
<td style="text-align:center">Arxiv 2018.07</td>
<td style="text-align:center">3D Convolutional Network</td>
<td style="text-align:left">ResNet50-3D + Non-local Attention</td>
<td style="text-align:left">ResNet50-3D</td>
<td style="text-align:center">加权平均</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">ResNet50-3D(pretrained in Kinetics) + Non-local Attention</td>
<td style="text-align:left">81.3</td>
<td style="text-align:center">91.2</td>
<td style="text-align:center">R1 84.3 mAP 77</td>
</tr>
<tr>
<td style="text-align:center">Spatial-Temporal Synergic Residual Learning for Video Person Re-Identification</td>
<td style="text-align:center">Pan Zhou<br>HUST</td>
<td style="text-align:center">Arxiv 2018.07</td>
<td style="text-align:center">Architecture</td>
<td style="text-align:left">时间平滑+RNN跨层连接</td>
<td style="text-align:left">CNN</td>
<td style="text-align:center">RNN+Avg pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">1. 学习一个空间mask参数，作为相邻帧之间空间平滑。<br>2. RNN处理前后有跨层连接</td>
<td style="text-align:left">70</td>
<td style="text-align:center">88</td>
<td style="text-align:center">R1 76.7</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Metric</summary>
<h2>Metric</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Relaxed Pairwise Learned Metric for Person Re-identification</td>
<td style="text-align:center">Horst Bischof<br>Graz University of Technology</td>
<td style="text-align:center">ECCV 2012</td>
<td style="text-align:left">从不同摄像头下的采样中学习矩阵，注重摄像头之间的变换</td>
<td style="text-align:center">Color + LBP</td>
<td style="text-align:center">马氏距离</td>
<td style="text-align:left">在距离度量学习前先对特征进行PCA降维</td>
<td style="text-align:left"><strong>VIPeR</strong> 27<br><strong>PRID</strong> 15</td>
</tr>
<tr>
<td style="text-align:center">Deep Metric Learning for Practical Person Re-identification</td>
<td style="text-align:center">Stan Z. Li<br> NLPR, CASIA</td>
<td style="text-align:center">ICPR 2014</td>
<td style="text-align:left">提出一个更通用的方式去从原始图片上学习距离度量</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Cosine + Binomial Distance</td>
<td style="text-align:left">1. 双路网络，当做二分类，输出相似度<br>2. 每一支路分为三个小支路，分别输入图片的上中下三部分，最后级联再经全连接得到最后表达</td>
<td style="text-align:left"><strong>VIPeR</strong> 34.4<br><strong>PRID</strong> 17.9</td>
</tr>
<tr>
<td style="text-align:center">Multi-shot Re-identification with Random-Projection-Based Random Forests</td>
<td style="text-align:center">Richard J. Radke<br>RPI</td>
<td style="text-align:center">WACV 2015</td>
<td style="text-align:left">基于视频的距离度量学习</td>
<td style="text-align:center">Color Histograms<br>Schmid &amp; Gabor Filters</td>
<td style="text-align:center">随机森林输出的相似性值</td>
<td style="text-align:left">1. 通过随机投影对图片的特征向量降维<br>2. 在投影出的亚空间中，基于对层面训练随机森林<br>3. 随机投影增加了随机森林的分类多样性<br>4. 融合多个视频帧的方法：计算两者所有图片对的相似性值，再取平均</td>
<td style="text-align:left"><strong>3DPeS</strong> 43(估计)</td>
</tr>
<tr>
<td style="text-align:center"><em>Person Re-identification by Local Maximal Occurrence Representation and Metric Learning</em> [<a href="                                 ">code</a>]</td>
<td style="text-align:center">Stan Z. Li<br>NLPR</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:left">新的手工特征和距离学习方法</td>
<td style="text-align:center">SILTP histograms<br>Color Bins</td>
<td style="text-align:center">在kissme的基础上加入了低维投影</td>
<td style="text-align:left">1. 选取特征时有一系列的子窗口，并对窗口特征做max pooling<br>为了获得多尺度信息，用了有三种大小的图片金字塔</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 52.20 detected 46.25<br><strong>VIPeR</strong> 40.00<br><strong>GRID</strong> 16.56</td>
</tr>
<tr>
<td style="text-align:center"><em>Embedding Deep Metric for Person Re-identification: A Study Against Large Variations</em></td>
<td style="text-align:center">Stan Z. Li<br>NLPR</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">提供了新的正样本对采集方法以及距离度量的方法</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 构成正样本对时，应选取与样本距离小的一些图片，距离太大的样本对会有害训练<br>2. 用全连接层将马氏距离的学习转化为欧氏距离</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 61.32 detected 52.09<br><strong>CUHK01</strong> (100) 86.59<br><strong>VIPeR</strong> 40.91</td>
</tr>
<tr>
<td style="text-align:center">Re-ranking Person Re-identification with k-reciprocal Encoding [<a href="                                                ">code</a>]</td>
<td style="text-align:center">Shaozi Li<br>Xiamen University</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">对排序得到的结果再次处理重排</td>
<td style="text-align:center">CaffeNet</td>
<td style="text-align:center">Jaccard Distance + L2 Distance</td>
<td style="text-align:left">1. 利用近邻关系组成集合，生成Jaccard Distance<br>2. 最后的距离是两种距离的加权和</td>
<td style="text-align:left"><strong>Market1501</strong> SQ 77.11<br><strong>CUHK03</strong> detected 61.6 manually 58.5<br><strong>MARS</strong> 73.94<br><strong>PRW</strong> 52.54</td>
</tr>
<tr>
<td style="text-align:center">Scalable Person Re-identification on Supervised Smoothed Manifold [<a href="                                    ">pdf</a>]</td>
<td style="text-align:center">Qi Tian<br>UTSA</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">对获得的相似性矩阵再处理，获得平滑的流形相似性度量</td>
<td style="text-align:center">LOMO,GOG,ELF6</td>
<td style="text-align:center">欧氏距离及其他相似性度量方式</td>
<td style="text-align:left">1. 通过转移矩阵不断迭代<br>2. 可以和其他距离度量方法协同使用，先提取特征，再进行距离度量学习，然后用这个方法优化相似性矩阵，得到最后的结果</td>
<td style="text-align:left"><strong>CUHK03</strong>SQ manually 76.6 detected 72.7<br><strong>VIPeR</strong> 53.73<br><strong>PRID450S</strong> 72.98</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Loss</summary>
<h2>Loss</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">loss</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification [<a href="                                    ">pdf</a>]</td>
<td style="text-align:center">Chi Zhang<br>Megvii</td>
<td style="text-align:center">Arxiv 2017.10</td>
<td style="text-align:left">限制最大的正样本对距离小于最小的负样本对距离</td>
<td style="text-align:center">ResNet50-X</td>
<td style="text-align:center">标准化的欧式距离</td>
<td style="text-align:left">对于整个batch，找到最大的正样本对距离，和最小的副样本对距离，让他们距离超过margin</td>
<td style="text-align:left">输入为P个人，每人K个图片</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 87.5<br><strong>Market1501</strong><br>SQ R 88.9 mAP 76.7<br><strong>MARS</strong><br>SQ R 84.2 mAP 74.6</td>
</tr>
<tr>
<td style="text-align:center"><em>In Defense of the Triplet Loss for Person Re-identification</em> [<a href="                                                        ">code</a>]</td>
<td style="text-align:center">Bastian Leibe<br>RWTH Aachen University</td>
<td style="text-align:center">Arxiv 201711</td>
<td style="text-align:left">在一个batch中，寻找最困难的正负样本组成三元组</td>
<td style="text-align:center">ResNet50 or LuNet</td>
<td style="text-align:center">欧氏距离</td>
<td style="text-align:left">1. 一个batch中有P个人，每个人K张图片<br>2. 对每个人，每一张图片，在batch内寻找最困难的正样本与负样本计算triplet loss<br>3. 最后一共有PK个loss用于计算和平均</td>
<td style="text-align:left">用了soft-margin</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 89.63 detected 87.58<br><strong>Market1501</strong><br>SQ R 86.67 mAP 81.07<br>MQ R 91.75 mAP 87.18<br><strong>MARS</strong><br>MQ R 81.21 mAP 77.43</td>
</tr>
<tr>
<td style="text-align:center">Support Neighbor Loss for Person Re-identification</td>
<td style="text-align:center">Yun Fu<br>Northeastern University</td>
<td style="text-align:center">MM 2018</td>
<td style="text-align:left">在近邻内部构建损失对</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">Support Neighbor Loss</td>
<td style="text-align:left">对于每个样本，得到其K近邻，在K近邻计算query与各个近邻的欧式距离，然后利用softmax，将此距离处理为概率，要求近邻内正样本的概率和越大越好。同时每个近邻内，最远正样本距离与最近正样本距离的差值要小，这样能让样本分布在query的周围</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">Hard-Aware Point-to-Set Deep Metric for Person Re-identification</td>
<td style="text-align:center">Xiang Bail<br>Huazhong University of Science and Technology</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:left">对与triplet，正负样本根据距离加权</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">Triplet Loss</td>
<td style="text-align:left">对正负样本的距离采用多项式加权的形式</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ mAP 69.43 R1 84.59<br>MQ mAP 76.75 R1 90.20<br><strong>CUHK03</strong> manually 90.4 detected 88.9<br><strong>DukeMTMC</strong> mAP 60.64 R1 75.94</td>
</tr>
<tr>
<td style="text-align:center">Tripletcenter loss for multi-view 3d object retrieval</td>
<td style="text-align:center">Song Bai<br>HUST</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:left">将triplet loss与center loss结合在一起使用</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:left">Triplet-center loss</td>
<td style="text-align:left">每一类有个随机初始化的类中心，对于一个batch中的一个样本，距离类中心的距离要比距离另一个类的类中心的距离小m</td>
<td style="text-align:left">-</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Unsupervised</summary>
<h2>Unsupervised</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Unsupervised Person Re-identification by Deep Learning Tracklet Association</td>
<td style="text-align:center">Shaogang Gong<br>QMUL</td>
<td style="text-align:center">ECCV 2018 oral</td>
<td style="text-align:left">结合视频每一帧的时间标签和相机标签，尽可能降低同一个人的视频分配了不同的label的情况</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left">1. 同一时刻下，不同的采样ID不同，这是因为同一时刻人不会出现在不同位置。<br>2. 同一个camera下，过了一定的时间，重新分配标签。这是基于行人行走的速度，在经过一定时间后，便会出了摄像头的拍摄范围。<br>3. 对于带有tracking的数据集，要求采样时，选择场景空间中，同时出现且位置距离较远的目标。<br>4. 跨摄像头情况下，对于一个短视频，搜寻其他摄像头下的近邻视频，视为正样本，否则为负样本。并设计loss使得正样本距离变小，负样本距离变大</td>
<td style="text-align:left"><strong>CUHK03</strong>(700) R1 44.7 mAP 31.2<br><strong>Market1501</strong> R1 63.7 mAP 41.2<br><strong>DukeMTMC</strong> R1 61.7 mAP 43.5<br><strong>PRID2011</strong> R1 49.4 <br><strong>iLIDS-VID</strong> R1 26.7<br><strong>MARS</strong> R1 43.8 mAP 29.1</td>
</tr>
<tr>
<td style="text-align:center">Domain Adaptation through Synthesis for Unsupervised Person Re-identification</td>
<td style="text-align:center">Jean-Francois<br>ULAVAL</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:left">提出新的数据集，使用合成数据</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:left">扫描真人，生成模型，虚拟光源，使用合成的人来创建数据集</td>
<td style="text-align:left">-</td>
</tr>
<tr>
<td style="text-align:center">Generalizing A Person Retrieval Model Hetero- and Homogeneously</td>
<td style="text-align:center">Liang Zheng<br>UTS</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:left">生成不同摄像头下的样本，可以当作正样本对</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">-</td>
<td style="text-align:left">1. 目标数据集只知道训练集的摄像头ID。<br>2. 对于给定的目标图片，生成不同摄像头下的样本，这些便视为正样本对。<br>3. 对于目标数据集，训练集随机选取多张图片，每一张视为不同的行人，这样虽然可能会有两张图片属于同一个人的情况，但是目标数据集比较大的时候，影响比较小。</td>
<td style="text-align:left"><strong>DukeMTMC -&gt; Market1501</strong><br>R1 62.2 mAP 31.4<br><strong>Market1501 -&gt; DukeMTMC</strong><br>R1 46.9 mAP 27.2</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Person Search</summary>
<h2>Person Search</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:center">Tag</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Feature</th>
<th style="text-align:center">Detail</th>
<th style="text-align:center">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">RCAA: Relational Context-Aware Agents for Person Search</td>
<td style="text-align:center">Yi Yang<br>UTS</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:center">增强学习</td>
<td style="text-align:left">用增强学习得到候选框</td>
<td style="text-align:left">ResNet50</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">Person Search by Multi-Scale Matching</td>
<td style="text-align:center">Shaogang Gong<br>QMUL</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:center">多尺度</td>
<td style="text-align:left">多尺度匹配，利用不同层次的特征</td>
<td style="text-align:left">ResNet50</td>
<td style="text-align:center">1. person detection: Faster-RCNN</td>
<td style="text-align:center"><strong>PRW</strong> R1 65.0 mAP 38.7<br><strong>CUHK-SYSU</strong> R1 88.5 mAP 87.2</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>New Perspective</summary>
<h2>New Perspective</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Recurrent Attention Models for Depth-Based Person Identification</td>
<td style="text-align:center">Li FeiFei<br>Stanford University</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">数据集是人的深度信息，立体的，无RGB信息</td>
<td style="text-align:center">–</td>
<td style="text-align:center">–</td>
<td style="text-align:left">因数据集较大，结合了循环注意力模型，自动选择下一个关注点</td>
<td style="text-align:left">–</td>
</tr>
<tr>
<td style="text-align:center">End-to-End Deep Learning for Person Search</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">将检测与匹配结合起来做</td>
<td style="text-align:center">Faster RCNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 分类的时候，一个batch只有少数图片，但整体类别很多，所以Softmax目标会很稀疏<br>2. 提出随机采样的Softmax loss，即每次随机选取Softmax神经元的一个子集</td>
<td style="text-align:left">–</td>
</tr>
<tr>
<td style="text-align:center">Person Search with Natural Language Description</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">根据自然语言描述去搜索人物</td>
<td style="text-align:center">VGG16</td>
<td style="text-align:center">–</td>
<td style="text-align:left">1. 单元级的注意力与单词级的门控制</td>
<td style="text-align:left">–</td>
</tr>
<tr>
<td style="text-align:center">Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro [<a href="                                         ">code</a>]</td>
<td style="text-align:center">Liang Zheng<br>University of Technology Sydney</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:left">借助于GAN产生训练图片，缓解过拟合</td>
<td style="text-align:center">ResNet</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. 用DCGAN产生图片，产生的图片不属于任何类，使用 label smoothing regularization (加权0.1)方法学习针。对于正常的真实图片，用交叉熵损失(加权1)学习</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 83.97 mAP 66.07<br>MQ R1 88.42 mAP 76.10<br><strong>CUHK03</strong> detected<br>R1 84.6 mAP 87.4<br><strong>DukeMTMC</strong><br>R1 67.68 mAP 47.13</td>
</tr>
<tr>
<td style="text-align:center">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns [<a href="                                    ">code</a>]</td>
<td style="text-align:center">Jianming Lv<br>South China University of Technology</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:left">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:left">-</td>
<td style="text-align:left">-</td>
</tr>
<tr>
<td style="text-align:center">Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-free Approach</td>
<td style="text-align:center">Zhenan Sun<br>CASIA</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:left">部分人体找完整人体</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean distance</td>
<td style="text-align:left">将稀疏字典表达嵌入到网络中，完整人体特征视为字典，稀疏矩阵是要学习的权值，这些用以重建部分人体的特征</td>
<td style="text-align:left"><strong>Partial REID</strong>MQ 53.67<br><strong>Partial-iLIDS</strong> MQ 55.46</td>
</tr>
<tr>
<td style="text-align:center">Person Search via Mask-Guided Two-Stream CNN Model</td>
<td style="text-align:center">Wanli Ouyang, Ying Tai<br>The University of Sydney</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:left">检测出图片，然后利用原始图片与mask一起做</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">-</td>
<td style="text-align:left">人的图片检测出来后，用mask将前景分出来，然后将包括背景的图片和前景图片的特征级联，用SEBlock做一次加权。</td>
<td style="text-align:left"><strong>CUHK-SYSU</strong>(100) mAP 83.0 R1 83.7<br><strong>PRW</strong> mAP 32.6 R1 72.1</td>
</tr>
<tr>
<td style="text-align:center">Person Search in Videos with One Portrait Through Visual and Temporal Links</td>
<td style="text-align:center">Dahua Lin<br>CUHK</td>
<td style="text-align:center">ECCV 2018</td>
<td style="text-align:left">给人脸寻找电影中的人</td>
<td style="text-align:center">IDE…</td>
<td style="text-align:center">-</td>
<td style="text-align:left">-</td>
<td style="text-align:left">-</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>GAN</summary>
<h2>GAN</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">G &amp; D</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro [<a href="                                         ">code</a>]</td>
<td style="text-align:center">Liang Zheng<br>University of Technology Sydney</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:left">借助于GAN产生训练图片，缓解过拟合</td>
<td style="text-align:center">DCGAN</td>
<td style="text-align:center">ResNet</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. 用DCGAN产生图片，产生的图片不属于任何类，使用 label smoothing regularization (加权0.1)方法学习针。对于正常的真实图片，用交叉熵损失(加权1)学习</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 83.97 mAP 66.07<br>MQ R1 88.42 mAP 76.10<br><strong>CUHK03</strong> detected<br>R1 84.6 mAP 87.4<br><strong>DukeMTMC</strong><br>R1 67.68 mAP 47.13</td>
</tr>
<tr>
<td style="text-align:center">Camera Style Adaptation for Person Re-identification</td>
<td style="text-align:center">Liang Zheng<br>UTS</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:left">做不同摄像头之间的数据增强</td>
<td style="text-align:center">CycleGAN</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. 用CycleGAN，利用一个摄像头下的数据生成另一个摄像头下的数据。<br>2. 伪数据使用LSR Loss</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 89.49 mAP 71.55<br><strong>DukeMTMC-reID</strong> R1 78.32 mAP 57.61</td>
</tr>
<tr>
<td style="text-align:center">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identitication</td>
<td style="text-align:center">Liang Zhegn<br>UTS</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:left">利用CycleGAN学习两个数据集之间转换，帮助在目标数据集上无监督行人重识别</td>
<td style="text-align:center">CycleGAN</td>
<td style="text-align:center">IDE</td>
<td style="text-align:center">Euclidean distance</td>
<td style="text-align:left">1. 用CycleGAN学习两个数据集之间的风格转换，同时用一个双路网络限制正样本对与负样本对的距离，进行交替优化<br>2. 正样本对用转换前后的图片。<br>3. 双路网络能保留转换前后人的身份信息</td>
<td style="text-align:left"><strong>DukeMTMC-reID</strong> R1 46.4 mAP 26.2<br><strong>Market-1501</strong> R1 57.7 mAP 26.7</td>
</tr>
<tr>
<td style="text-align:center">A Unified Generative Adversarial Framework for Image Generation and Person Re-identification</td>
<td style="text-align:center">Lingyu Duan<br>NLPR</td>
<td style="text-align:center">MM 2018</td>
<td style="text-align:left">针对reID的GAN框架</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:center">-</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R1 92.81 mAP 82.67<br>MQ R1 93.62 mAP 84.5<br><strong>DukeMTMC-reID</strong> R1 88.67 mAP 79.32<br><strong>CUHK03</strong>(100) R1 89.53<br><strong>CUHK01</strong>(486) R1 84.79</td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Part</summary>
<h3>Part</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval</td>
<td style="text-align:center">Qi Tian<br>UTSA</td>
<td style="text-align:center">MM 2017</td>
<td style="text-align:left">基于pose提取特征，并视为检索问题，在匹配时预分类库图片以加速</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 网络整体为分类网络<br>2. 先用pose检测模型提取头，脖子，以及臀部这四个关键点，将人体分为上中下三个部分<br>3对各个部分以及整体，各用一个支路提取特征，网络权值共享，每个支路又有自己单独的分类loss<br>4. 在检索时，先实现将库图片分成不同的group，并pca降维，取整个group的特征的平均作为整个group的表达，再进行检索，加快速度，用的特征是将四个通道输出的特征级联</td>
<td style="text-align:left"><strong>Market1501</strong> SQ R 89.9 mAP 73.9<br>MQ R 81.5 mAP 61.2<br><strong>CUHK03</strong><br>manually 85.0<br>detected 82.2<br><strong>VIPeR</strong> 54.8</td>
</tr>
<tr>
<td style="text-align:center">Pose Invariant Embedding for Deep Person Re-identification [<a href="                                    ">pdf</a>]</td>
<td style="text-align:center">Liang Zheng<br>UTS</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:left">为解决行人匹配时的误对齐问题，加入关键点信息</td>
<td style="text-align:center">AlexNet or ResNet50</td>
<td style="text-align:center">L2norm + Euclidean Distance</td>
<td style="text-align:left">1. 用CPM提取pose,10个关键点<br>2. 用关键点设置特征提取框，框根据位置做相应的仿射变换，所以框有可能是斜着提取的,再将各部分拼一块组成一张图片，即PoseBox<br>3. 当关键点的自信值小于某个门限时，会加入一些随机扰动<br>4. 单路三支流，分别输入原始图像，PoseBox，各个特征点的自信值<br>5. 各支路提取的特征级联后经过一个全连接得到最后表达<br>6. 三个Loss，最后特征表达一个分类Loss，原始图特征一个分类Loss,PoseBox一个分类Loss</td>
<td style="text-align:left"><strong>Market1501</strong><br>SQ R 79.33 mAP 55.95<br><strong>CUHK03</strong><br>detected R 67.10 mAP 71.32<br><strong>VIPeR</strong> 27.44</td>
</tr>
<tr>
<td style="text-align:center">Pose-driven Deep Convolutional model for Person Re-identification</td>
<td style="text-align:center">Qi Tain<br>UTSA</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:left">借助关键点与仿射变换网络，得到局部特征</td>
<td style="text-align:center">GoogleNet修改版</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 头部，上身，四肢，共六部分<br>2. 局部特征与全局特征通过网络融合</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 88.7 detected 78.29<br><strong>Market1501</strong>SQ R1 84.14 mAP 63.41<br><strong>VIPeR</strong> 51.27</td>
</tr>
<tr>
<td style="text-align:center">Pose-Normalized Image Generation for Person Re-identification [<a href="                                    ">pdf</a>] [<a href="                                  ">code</a>]</td>
<td style="text-align:center">Tao Xiang, Xiangyang Xue<br>QMUL &amp; Fudan University</td>
<td style="text-align:center">Arxiv 2017.12</td>
<td style="text-align:left">给定图片和期望的Pose，利用GAN合成基于Pose的图片</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 生成部分根据给定的姿势修改图片<br>2. 生成部分输入包括：预训练的属性预检测子预测的属性，原始图片与姿势图片的级联<br>3. 网络结构由两部分组成，对于给定的图片由A网络提取原始特征，B网络提取给定的典型姿势生成的图片的特征，最终特征由这两种特征融合而成，融合是element-wise maximum。网络A与B的结构相同，但是不共享权值。</td>
<td style="text-align:left"><strong>CUHK03</strong> detected 92.66<br><strong>Market1501</strong><br>SQ R1 95.52 mAP 89.94<br>MQ R1 95.90 mAP 91.37<br><strong>VIPeR</strong> 78.17<br><strong>DukeMTMC</strong>R1 91.47 mAP 81.39<br><strong>CUHK01</strong> 86.22</td>
</tr>
<tr>
<td style="text-align:center">Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline) [<a href="                                              ">code</a>]</td>
<td style="text-align:center">UTSA<br>Qi Tian</td>
<td style="text-align:center">Arxiv 2018.01</td>
<td style="text-align:left">1. 一个更好地基于part的baseline。2.part pooling策略,防止均分时太过粗糙</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. PCB：将提取的特征图T,每个位置特征为f,所有f均分为6个水平条带，每一个均值池化得到part表达g，分别经过一个全连接得到h，用h去训练分类器。<br>2. RPP:训练分类器，输入为f，输出为这个位置的特征属于哪一个条带，最终的G将不用均值池化，而是通过所有f的概率加权得到<br>3.PCB+RPP：标准训练PCB。然后固定参数，加入RPP并训练RPP。最后全部参数一起训练。<br>4. 提升输入图片的大小或者降低网络模型中降采样的操作，以增大T的空间尺寸有利于提升性能</td>
<td style="text-align:left"><strong>Market1501</strong> SQ R1 93.8 mAP 81.6<br><strong>DukeMTMC-reID</strong> SQ R1 83.3 mAP 69.2<br><strong>CUHK03</strong> detected R1 63.7 mAP 57.5</td>
</tr>
</tbody>
</table>
<p>当用肢体关键点框出感兴趣区域后，随之而来的一个问题便是有一些标志性的物体会被排除在外，比如包，雨伞等等</p>
</details>
<details>
<summary>Attribute</summary>
<h3>Attribute</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:center">Detecter</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Person Re-identification by Attributes</td>
<td style="text-align:center">Shaogang Gong<br>QMUL</td>
<td style="text-align:center">BMVC 2012</td>
<td style="text-align:left">用属性辅助识别，标注了VIPeR数据集</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)</td>
<td style="text-align:center">低维特征用巴氏距离，属性特征用欧氏距离</td>
<td style="text-align:center">SVM</td>
<td style="text-align:left">1. 属性检测子都是在VIPeR上训练的，其他数据集上直接用训好的检测子<br>2. 属性是可以高度依赖于视角或者人的姿态<br>3. 标注VIPeR时，分为三个大类：前方，后方，侧面<br>4. 对每个属性训练相应的检测子时确保三个角度的正样本都有，因此检测子有视角不变性<br>5. 属性的加权是在各个数据集上单独做的<br>6. 除了各个属性距离要加权，最后的属性距离与各种低维特征间也要加权求和<br>7. 在加入属性后，Rank1，VIPeR上准确率降低了一点，iLIDS上提升了，在Rank5上都提升了<br><strong>问题</strong> 定义的一些与视角敏感的属性，应该会有损性能吧，因为不同的视角下，虽是同一个人，但是此属性却一个正一个负</td>
<td style="text-align:left"><strong>VIPeR</strong> 16.5<br><strong>iLIDS</strong> 52.1</td>
</tr>
<tr>
<td style="text-align:center"><em>Deep Attributes Driven Multi-camera Person Re-identification</em></td>
<td style="text-align:center">Qi Tian<br>Peking University</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">利用行人属性辅助行人重识别</td>
<td style="text-align:center">AlexNet</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:center">AlexNet</td>
<td style="text-align:left">1. 第一阶段，用一个带属性的独立数据集训练网络，并用该网络为目标数据集初始化属性标签<br>2. 第二阶段，基于属性的Triplet Loss,将属性与ID结合起来训练，让同一个人的属性相似，不同人的属性相差较远<br>3. 第三阶段，为目标数据集重新标定属性标签，将独立属性数据及与此相结合，用其微调属性预测网络</td>
<td style="text-align:left"><strong>VIPeR</strong> 43.5<br><strong>PRID</strong> 22.6<br><strong>GRID</strong> 22.4</td>
</tr>
<tr>
<td style="text-align:center">Attributes-Based Re-identification</td>
<td style="text-align:center">Gong, Shaogang<br>Queen Mary Unifying of London</td>
<td style="text-align:center">Springer London 2014</td>
<td style="text-align:left">将属性与Re-ID结合，标注了PRID数据集</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)</td>
<td style="text-align:center">加权欧氏距离</td>
<td style="text-align:center">LIBSVM and investigate Linear, RBF, X2 and Intersection kernels</td>
<td style="text-align:left">1. 一些属性在数据集中有很多正样本，但有的属性只有少数正样本<br>2. 对于每一个属性，用所有的正样本训练，负样本用相同数量的剩余数据的欠抽样<br>3. 用低层次特征训练属性分类器，由此将高维特征映射到低维的语义属性空间<br>4. 判断距离时，要分别计算各个属性或者低层次特征的距离，再给每个都分配一个加权值,最后使用加权后的距离<br>5. 当人工标出Probe图片的属性去匹配时（gallery还是用检测器得到属性），效果没有用检测器的好，可能是因为虽然检测器再标库的属性时会引入误差，但是在标Probe时也会引入相同的误差</td>
<td style="text-align:left"><strong>PRID</strong> 41.5<br><strong>VIPeR</strong> 21.4</td>
</tr>
<tr>
<td style="text-align:center">Pedestrian Attribute Recognition At Far Distance</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">MM 2014</td>
<td style="text-align:left">标了一个远距离下行人属性数据集，任务是预测行人属性</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)</td>
<td style="text-align:center">–</td>
<td style="text-align:center">ikSVM<br>MRF with Gaussian kernel<br>MRF with random forest</td>
<td style="text-align:left">1. 用 Markov Random Field(MRF) 探索邻近图片间的上下文关系<br>2. MRF能量函数由 Unary Cost 和 Pairwise Cost组成<br>3. unary cost 利用预测属性分类概率（由ikSVM学习）的log函数构成<br>4. 用随机森林去学习Pairwise Cost</td>
<td style="text-align:left"><strong>PETA</strong> 71.1</td>
</tr>
<tr>
<td style="text-align:center">Re-id: Hunting Attributes in the wild</td>
<td style="text-align:center">Shaogang Gong<br>QMUL</td>
<td style="text-align:center">BMVC 2014</td>
<td style="text-align:left">从网络上爬取图片，用以训练属性检测子，以解决大数据集标注属性的问题</td>
<td style="text-align:center">BoG(属性) + 低层特征</td>
<td style="text-align:center">加权欧氏距离</td>
<td style="text-align:center">LDA</td>
<td style="text-align:left">1. 用其他文章提供的行人检测子框出行人，并删去一些不合适的图<br>2. 每张图片的元数据要先预处理，再得到 BoW 表达<br>3. 再用 self-tuning Spectral Clustering 聚成若干类，视为潜在的属性,并用来训练LDA获得属性分类器</td>
<td style="text-align:left"><strong>VIPeR</strong> 17<br><strong>GRID</strong> 22<br><strong>PRID</strong> 4<br><strong>CUHK01</strong> 9</td>
</tr>
<tr>
<td style="text-align:center">Multi-Task Learning with Low Rank Attribute Embedding for Person Re-identification</td>
<td style="text-align:center">Qi Tian<br>Unifying of Texas at San Antonio</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:left">将属性特征与低层次特征结合起来帮助行人重识别</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)</td>
<td style="text-align:center">欧式距离</td>
<td style="text-align:center">PRID，VIPeR : binary SVMs<br>iLIDS,SAIVT-SoftBio : MRFr</td>
<td style="text-align:left">1. 这里的Task指的是不同的摄像头<br>2. 属性之间是相关的，故用一个低秩矩阵Z将原属性映射到一个Embedding空间，可以将一些缺失的属性补全</td>
<td style="text-align:left"><strong>iLIDS</strong> 43.0<br><strong>PRID</strong> 18.0<br><strong>VIPeR</strong> 42.3</td>
</tr>
<tr>
<td style="text-align:center">Improving Person Re-identification by Attribute and Identity Learning</td>
<td style="text-align:center">Liang Zheng<br>University of Technology Sydney</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:left">主要研究属性标签如何在大规模学习问题上帮助Re-ID</td>
<td style="text-align:center">–</td>
<td style="text-align:center">–</td>
<td style="text-align:center">–</td>
<td style="text-align:left">这里的属性主要是与ID层面的属性，比如性别，年龄，而不是持续时间短的，或属于外界环境的属性，比如打电话，骑自行车</td>
<td style="text-align:left">–</td>
</tr>
</tbody>
</table>
<ul>
<li>属性的正负样本之间的不平衡，以及有的属性正样本太少</li>
</ul>
</details>
<details>
<summary>Dataset</summary>
<h2>Dataset</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Syncopate</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Label method</th>
<th style="text-align:center">Video or Image</th>
<th style="text-align:center">Cammera</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Person Re-identification in the Wild [<a href="                                            ">code</a>]</td>
<td style="text-align:center">PRW</td>
<td style="text-align:center">LIang Zheng<br>UTS</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">提供一个端到端的大数据集，将行人检测与匹配一起做</td>
<td style="text-align:center">hand</td>
<td style="text-align:center">image</td>
<td style="text-align:center">6</td>
</tr>
<tr>
<td style="text-align:center"><em>MARS: A Video Benchmark for Large-Scale Person Re-identification</em> [<a href="                                                                              ">pdf</a>] [<a href="                                               ">code</a>]</td>
<td style="text-align:center">MARS</td>
<td style="text-align:center">Qi Tian<br>Tsinghua University</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">基于视频的检测子检测的Re-ID数据集，<br>并阐述了在大数据集下，分类网络要比双路或者三路网络更好</td>
<td style="text-align:center">detected</td>
<td style="text-align:center">Video</td>
<td style="text-align:center">6</td>
</tr>
<tr>
<td style="text-align:center">LVreID: Person Re-Identification with Long Sequence Videos</td>
<td style="text-align:center">LVreID</td>
<td style="text-align:center">Qi Tian<br>Peking University</td>
<td style="text-align:center">Arxiv 2017.12.20</td>
<td style="text-align:left">1. 跨季节，从一月到五月，共四天，每天三个小时，分别是早上中午晚上。<br>2. 视频序列长，平均长度是200帧。<br>3. 用SPP对时间维度做处理，得到固定的时间维度，再用卷积层融合。</td>
<td style="text-align:center">detected</td>
<td style="text-align:center">Video</td>
<td style="text-align:center">13</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Skimmed</summary>
<h2>Skimmed</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Detail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Learning Bidirectional Temporal Cues for Video-based Person Re-identification [<a href="                                                                   ">pdf</a>]</td>
<td style="text-align:center">Xuanyu He</td>
<td style="text-align:center">IEEE TCSVT</td>
<td style="text-align:left">利用双向循环神经网络，在RNN-CNN基础上改进</td>
<td style="text-align:left"><strong>iLIDS</strong> 55.3<br><strong>PRID</strong> 72,8</td>
</tr>
<tr>
<td style="text-align:center">Easy Identification from Better Constraints: Multi-Shot Person Re-Identification from Reference Constraints</td>
<td style="text-align:center">Ying Wu<br>CASA</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:left">Metric Learning</td>
<td style="text-align:left"><strong>iLIDS</strong> 42<br><strong>PRID</strong> 70.9</td>
</tr>
<tr>
<td style="text-align:center">Group Consistent Similarity Learning via Deep CRF for Person Re-Identification</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2018</td>
<td style="text-align:left">考虑gallery之间的相关性，对probe与gallery之间的相似性优化</td>
<td style="text-align:left"><strong>Market1501</strong>SQ R1 93.5 mAP 81.6<br><strong>DukeMTMC</strong> R1 84.9 mAP 69.5<br><strong>CUHK03</strong><br>labeld R1 90.2<br>detected R1 88.8</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Utils</summary>
<p>Multi-shot: In the multi-shot experiments, we return the
average similarity between the probe person image and multiple gallery images of an certain individual.</p>
<h2>Module</h2>
<ul>
<li>Pyramid Matching</li>
<li>atrous convolution</li>
<li>RNN</li>
<li>SPP</li>
</ul>
</details>
<details>
<summary>Code</summary>
<h3>Projects</h3>
<ul>
<li>deep ReID [<a href="                                               ">code</a>]</li>
<li>Baseline Code (with bottleneck) for Person-reID (pytorch) [<a href="                                                         ">code</a>]</li>
<li>Open reid [<a href="                                 ">code</a>]</li>
<li>caffe-PersonReID [<a href="                                             ">code</a>]</li>
</ul>
<h3>Released</h3>
<ul>
<li>A Discriminatively Learned CNN Embedding for Person Re-identification [<a href="                                           ">official</a>] [<a href="                                   ">github</a>]</li>
<li>Unsupervised Person Re-identification: Clustering and Fine-tuning [<a href="                                                                                           ">code</a>]</li>
<li>Pedestrian Alignment Network for Large-scale Person Re-identification [<a href="                                              ">code</a>]</li>
<li>A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking [<a href="                                                   ">code</a>]</li>
<li>Joint Detection and Identification Feature Learning for Person Search [<a href="                                           ">code</a>]</li>
</ul>
</details>
<h1><center>Expansion</center></h1>
<details>
<summary>Neural Networks Architecture</summary>
<h2>Neural Networks Architecture</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Detail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Densely Connected Convolutional Networks</td>
<td style="text-align:center">Kilian Q. Weinberger<br>Cornell University</td>
<td style="text-align:center">CVPR 2017 (best)</td>
<td style="text-align:left">卷积层间密集连接，特征图重用</td>
<td style="text-align:left">1. 网络可以很深，分为很多个block<br>2. 每个block由多个卷积层构成，每一层的特征图都会送到该block内它后面的所有卷积层<br>3. block内每一层的通道数不能太大，block之间用1x1卷积压缩通道数</td>
</tr>
<tr>
<td style="text-align:center">Sequeeze-and-Excitation Networks</td>
<td style="text-align:center">Jie Hu<br>Momenta</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:left">对channels进行加权</td>
<td style="text-align:left">1. 要处理的特征X<br>2. 先Global Average Pooling,得到C维特征<br>3. C维特征经过一层全连接，为降低参数数量，输出为 C/r 维，r为超参数<br>4. 经过relu，再经过一层全连接，输出C维<br>5. 对原特征各通道相乘加权，得到处理后的表达</td>
</tr>
<tr>
<td style="text-align:center">Rethinking the Inception Architecture for Computer Vision [<a href="                                    ">pdf</a>]</td>
<td style="text-align:center">Jonathon Shlens<br>Google</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">降低计算量</td>
<td style="text-align:left">1.  基于大滤波器尺寸分解卷积：分解到更小的卷积；空间分解为不对称卷积。<br>2. 利用辅助分类器，辅助分类器中含有BN或者dropout时主分类器效果会更好。辅助分类器起到正则化的作用。<br> 3. 有效的网格尺寸减少</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Interesting Work</summary>
<h2>Interesting Work</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Detail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</td>
<td style="text-align:center">Lior Worf<br>Tel Aviv University</td>
<td style="text-align:center">CVPR 2014</td>
<td style="text-align:left">联合对齐与表达操作</td>
<td style="text-align:left">1. 用3D Face来对齐<br>2. 9层网络提特征</td>
</tr>
<tr>
<td style="text-align:center">Deep Learning Face Representation by Joint Identification-Verification</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">NIPS 2014</td>
<td style="text-align:left">用multi-task加强特征学习</td>
<td style="text-align:left">双路，每一路都有一个Softmax classification loss。两路联合有一个Contrastive loss。</td>
</tr>
<tr>
<td style="text-align:center">Deep Learning Face Representation from Predicting 10000 Classes</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">CVPR 2014</td>
<td style="text-align:left">用网络提取高层次特征</td>
<td style="text-align:left">最后的特征表达维度只有160维</td>
</tr>
<tr>
<td style="text-align:center">Two-Stream Convolutional Networks for Action Recognition in Videos</td>
<td style="text-align:center">Andrew Zisserman<br>Oxford</td>
<td style="text-align:center">NIPS 2014</td>
<td style="text-align:left">双路结构处理时空信息</td>
<td style="text-align:left">网络一个支路输入图片另一个支路输入光流</td>
</tr>
<tr>
<td style="text-align:center"><em>Recurrent Models of Visual Attention</em></td>
<td style="text-align:center">Koray Kavukcuoglu<br>Google DeepMind</td>
<td style="text-align:center">NIPS 2014</td>
<td style="text-align:left">每次只看图片的一小块，网络会自动寻找下一次观察的点</td>
<td style="text-align:left">1. 主体为RNN，每次输入整个图片和观察点坐标<br>2. 每一步输出两个分支，一个分类，另一个预测下一个位置<br>3. 使用增强学习，每一步分类对了reward为1，否则为0</td>
</tr>
<tr>
<td style="text-align:center">Large-scale Video Classification with Convolutional Neural Networks</td>
<td style="text-align:center">Li FeiFei<br>Stanford University</td>
<td style="text-align:center">CVPR 2014</td>
<td style="text-align:left">利用多分辨率与漏斗状网络结构来更好的利用局部时空信息</td>
<td style="text-align:left">1. 语境流：从低分辨率帧中学习特征<br>2. 中央流：从帧的中心部分的高分辨率区学习特征</td>
</tr>
<tr>
<td style="text-align:center">FlowNet： Learning Optical Flow with Convolutional Networks</td>
<td style="text-align:center">Vladimir Golkov<br>Technical University of Munich</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:left">用网络提取光流</td>
<td style="text-align:left">通过一系列的卷积与反卷积操作</td>
</tr>
<tr>
<td style="text-align:center">Deep Captioning with Multimodal Recurrent Neural Networks</td>
<td style="text-align:center">Junhua Mao<br>UCLA</td>
<td style="text-align:center">ICLR 2015</td>
<td style="text-align:left">用多模型RNN去处理自然图片说明</td>
<td style="text-align:left">1. 为语言和图片分别构建模型，然后融合两者的信息<br>2. RNN的每一步输入都是某一单词的语言模型的输出<br>3. 每一步的RNN输出，语言模型输出，图像模型输出三者分别通过三个矩阵投影到一个共同的空间，再元素级相加得到融合后的表达</td>
</tr>
<tr>
<td style="text-align:center"><em>Learning Spatiotemporal Features with 3D Convolutional Networks</em></td>
<td style="text-align:center">Manohar Paluri<br>Facebook AI Research</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:left">3D卷积核去处理视频</td>
<td style="text-align:left">3D卷积核能有效学习时间与空间特征</td>
</tr>
<tr>
<td style="text-align:center">MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching</td>
<td style="text-align:center">Alexander C. Berg<br>University of North Carolina at Chapel Hill</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:left">块匹配与特征学习一起做</td>
<td style="text-align:left">两个支路通过全连接融合为一路，全连接层则相当于距离度量</td>
</tr>
<tr>
<td style="text-align:center">Deep Mutual Learning</td>
<td style="text-align:center">Huchuan Lu<br>Dalian University of Technology, China</td>
<td style="text-align:center">Arxiv 201706</td>
<td style="text-align:left">两个网络相互学习</td>
<td style="text-align:left">1. 两个网络都对同一个输入做预测，各有一个分类loss，同时两者之间有一个二者输出的概率分布的KL距离的loss。<br>2. 优化时，交替优化，直至收敛</td>
</tr>
<tr>
<td style="text-align:center">Online Video Deblurring via Dynamic Temporal Blending Network</td>
<td style="text-align:center">Micheal Hirsch<br>Seoul National University</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">图片去噪</td>
<td style="text-align:left">1. 采用循环结构<br>2. Encoder + Dynamic blending + Decoder</td>
</tr>
<tr>
<td style="text-align:center">Coherent Online Video Style Transfer</td>
<td style="text-align:center">Gang Hua<br>MSRA</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:left">视频风格转换</td>
<td style="text-align:left">1. 相邻两帧之间提取光流<br>2. 每一帧用encoder获得特征，用光流对前一帧wrap。利用wrap之后的结果与当前帧的特征的差值学习mask，用mask更新特征，最后对当前帧特征解码</td>
</tr>
<tr>
<td style="text-align:center">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">长范围时间结构建模</td>
<td style="text-align:left">TSN使用从整个视频中稀疏地采样一系列短片段，每个片段都将给出其本身对于行为类别的初步预测，从这些片段的“共识”来得到视频级的预测结果。</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Pose Estimation</summary>
<h2>Pose Estimation</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Detail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Convolutional Pose Machines</td>
<td style="text-align:center">Yaser Sheikh<br>CMU</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">用很深的网络不断调整预测</td>
<td style="text-align:left">1. 整体类似RNN,分为很多步<br>2. 第一步输入是用七层网络提取的各个关节点的自信图<br>3. 之后的每个阶段是一样的model，是5层卷积网，输入为前一阶段的自信图以及对原始图提取的特征<br>4. 每一阶段都会额外增加一个loss,是预测与真实自信图的误差，用以减轻梯度消失问题</td>
</tr>
<tr>
<td style="text-align:center">Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos</td>
<td style="text-align:center">Otmar Hilliges<br>ETH Zurich</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">能端到端的训练，能同时表达交界处以及他们之间的时空关系</td>
<td style="text-align:left">1. 先训练CPM，再与后面的网络结合起来优化<br>2. 对于前后帧，用弹簧能量模型定义变形损失</td>
</tr>
<tr>
<td style="text-align:center">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fileds</td>
<td style="text-align:center">Yaser Sheikh<br>CMU</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">定义新的表达来更好的处理多人关节点估计</td>
<td style="text-align:left">1. PAF是同一个人两个相邻关节点之间的向量场，有方向<br>2. 网络分为两路，一路用CPM预测自信图，另一路预测PAF<br>3.PAF主要解决多人情况下关节点的划分问题</td>
</tr>
</tbody>
</table>
</details>
<!--

<details>
<summary>Super-Resolution</summary>

## Super-Resolution
| Name | Author | Conference & Year | Motivation |Detail|
|:----:|:------:|:-----------------:|:-----------|:-----|
|Image Super-Resolution Using Deep Convolutional Networks|Xiaoou Tang<br>CUHK|2016 TPAMI|用CNN处理超分辨|三层CNN结构，先将低分辨率图片上采样到期望的大小，然后经过三个卷积层，中间无下采样，输出的为处理的高分辨率图片，与label的差值平方和为loss|
|Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network|Wezhe Shi<br>Twitter|CVPR 2016|将upscale放到最后一层，降低了计算量|输入为低分辨率图片，在最后一层，通道数为原图通道数的降采样比例的平方倍，这样相当于用不同的通道去学upscale时不同位置的值，将结果重排一下就是高分辨率图了。|
|Perceptual Losses for Real-Time Style Transfer and Super-Resolution|Li FeiFei|ECCV 2016|不使用per-pixel的loss|不直接要求重建的高清图片与真实高清图片的像素相似，而是要求用网络提取的特征相似，这样能获得更好的视觉上的提升|
|Frame-Recurrent Video Super-Resolution|Matthew Brown<br> Google|CVPR 2018|不使用划窗，降低计算量|1. 前一帧与当前帧级联学习光流。<br>2. 利用光流对前一帧的预测的高分辨率图片做warping(即利用光流的值得到前一帧像素应该向何处偏移)。<br>3. 利用重新估算的当前帧高分辨率图片结合当前帧得到最后的估计|

</details>
-->
<details>
<summary>Machine Learning</summary>
<h2>Machine Learning</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Detail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Local Fisher Discriminant Analysis for Supervised Dimensionality Reduction</td>
<td style="text-align:center">Masashi Sugiyama<br>Tokyo Institute pf Technology</td>
<td style="text-align:center">ICML 2006</td>
<td style="text-align:left">传统Fisher Discriminant分析对于从若干独立簇中的类的采样没有区分性</td>
<td style="text-align:left">考虑数据的内部结构，将FDA于LPP结合</td>
</tr>
<tr>
<td style="text-align:center">A Spatial-Temporal Descriptor Based on 3D-Gradients</td>
<td style="text-align:center">Cordelia Schmid<br>INRIA Grenoble</td>
<td style="text-align:center">BMVC 2008</td>
<td style="text-align:left">基于视频的时空描述子</td>
<td style="text-align:left">将一个cell中的累加梯度值量化到中二十面体的面中心方向</td>
</tr>
<tr>
<td style="text-align:center"><em>Large Scale Metric Learning from Equivalence Constraints</em></td>
<td style="text-align:center">Horst Bischof<br>Graz University of Technology</td>
<td style="text-align:center">CVPR 2012</td>
<td style="text-align:left">从统计推理的角度学习距离度量，不依赖于复杂的算法</td>
<td style="text-align:left">利用最大似然估计得到马氏距离度量矩阵</td>
</tr>
<tr>
<td style="text-align:center">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</td>
<td style="text-align:center">Cordelia Schmid<br>Inria</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:left">更好地处理冲突与运动边界的光流估计</td>
<td style="text-align:left">1. 从稀疏匹配的边缘保留插值的密匹配<br>2. 用密匹配初始化的方差能量最小化</td>
</tr>
</tbody>
</table>
</details>
<details>
<summary>Links</summary>
<h1>Links</h1>
<ul>
<li><a href="                                               ">KaiyangZhou</a></li>
<li><a href="                                              ">fangchengjin</a></li>
<li><a href="                             ">handong</a> 的 <a href="                                                                                                         ">Summary</a></li>
<li><a href="                                                                                                            ">数据集总结</a></li>
<li><a href="                                                               ">State of the art on the MARS dataset</a></li>
<li><a href="                                                                                                    ">新的总结</a></li>
</ul>
</details>
<div align="right">Updated Date: 2018/12/17</div>

  </body>
</html>
