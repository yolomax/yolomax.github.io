<!DOCTYPE html>
<html>
  <head>
      <meta charset="utf-8" />
      <title>summary</title>
      <style>.markdown-preview:not([data-use-github-style]) { padding: 2em; font-size: 1.2em; color: rgb(197, 200, 198); background-color: rgb(29, 31, 33); overflow: auto; }
.markdown-preview:not([data-use-github-style]) > :first-child { margin-top: 0px; }
.markdown-preview:not([data-use-github-style]) h1, .markdown-preview:not([data-use-github-style]) h2, .markdown-preview:not([data-use-github-style]) h3, .markdown-preview:not([data-use-github-style]) h4, .markdown-preview:not([data-use-github-style]) h5, .markdown-preview:not([data-use-github-style]) h6 { line-height: 1.2; margin-top: 1.5em; margin-bottom: 0.5em; color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) h1 { font-size: 2.4em; font-weight: 300; }
.markdown-preview:not([data-use-github-style]) h2 { font-size: 1.8em; font-weight: 400; }
.markdown-preview:not([data-use-github-style]) h3 { font-size: 1.5em; font-weight: 500; }
.markdown-preview:not([data-use-github-style]) h4 { font-size: 1.2em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) h5 { font-size: 1.1em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) h6 { font-size: 1em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) strong { color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) del { color: rgb(155, 160, 157); }
.markdown-preview:not([data-use-github-style]) a, .markdown-preview:not([data-use-github-style]) a code { color: white; }
.markdown-preview:not([data-use-github-style]) img { max-width: 100%; }
.markdown-preview:not([data-use-github-style]) > p { margin-top: 0px; margin-bottom: 1.5em; }
.markdown-preview:not([data-use-github-style]) > ul, .markdown-preview:not([data-use-github-style]) > ol { margin-bottom: 1.5em; }
.markdown-preview:not([data-use-github-style]) blockquote { margin: 1.5em 0px; font-size: inherit; color: rgb(155, 160, 157); border-color: rgb(67, 72, 76); border-width: 4px; }
.markdown-preview:not([data-use-github-style]) hr { margin: 3em 0px; border-top: 2px dashed rgb(67, 72, 76); background: none; }
.markdown-preview:not([data-use-github-style]) table { margin: 1.5em 0px; }
.markdown-preview:not([data-use-github-style]) th { color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) th, .markdown-preview:not([data-use-github-style]) td { padding: 0.66em 1em; border: 1px solid rgb(67, 72, 76); }
.markdown-preview:not([data-use-github-style]) code { color: rgb(255, 255, 255); background-color: rgb(48, 51, 55); }
.markdown-preview:not([data-use-github-style]) pre.editor-colors { margin: 1.5em 0px; padding: 1em; font-size: 0.92em; border-radius: 3px; background-color: rgb(39, 41, 44); }
.markdown-preview:not([data-use-github-style]) kbd { color: rgb(255, 255, 255); border-width: 1px 1px 2px; border-style: solid; border-color: rgb(67, 72, 76) rgb(67, 72, 76) rgb(53, 57, 60); border-image: initial; background-color: rgb(48, 51, 55); }
.markdown-preview[data-use-github-style] { font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif; line-height: 1.6; word-wrap: break-word; padding: 30px; font-size: 16px; color: rgb(51, 51, 51); background-color: rgb(255, 255, 255); overflow: scroll; }
.markdown-preview[data-use-github-style] > :first-child { margin-top: 0px !important; }
.markdown-preview[data-use-github-style] > :last-child { margin-bottom: 0px !important; }
.markdown-preview[data-use-github-style] a:not([href]) { color: inherit; text-decoration: none; }
.markdown-preview[data-use-github-style] .absent { color: rgb(204, 0, 0); }
.markdown-preview[data-use-github-style] .anchor { position: absolute; top: 0px; left: 0px; display: block; padding-right: 6px; padding-left: 30px; margin-left: -30px; }
.markdown-preview[data-use-github-style] .anchor:focus { outline: none; }
.markdown-preview[data-use-github-style] h1, .markdown-preview[data-use-github-style] h2, .markdown-preview[data-use-github-style] h3, .markdown-preview[data-use-github-style] h4, .markdown-preview[data-use-github-style] h5, .markdown-preview[data-use-github-style] h6 { position: relative; margin-top: 1em; margin-bottom: 16px; font-weight: bold; line-height: 1.4; }
.markdown-preview[data-use-github-style] h1 .octicon-link, .markdown-preview[data-use-github-style] h2 .octicon-link, .markdown-preview[data-use-github-style] h3 .octicon-link, .markdown-preview[data-use-github-style] h4 .octicon-link, .markdown-preview[data-use-github-style] h5 .octicon-link, .markdown-preview[data-use-github-style] h6 .octicon-link { display: none; color: rgb(0, 0, 0); vertical-align: middle; }
.markdown-preview[data-use-github-style] h1:hover .anchor, .markdown-preview[data-use-github-style] h2:hover .anchor, .markdown-preview[data-use-github-style] h3:hover .anchor, .markdown-preview[data-use-github-style] h4:hover .anchor, .markdown-preview[data-use-github-style] h5:hover .anchor, .markdown-preview[data-use-github-style] h6:hover .anchor { padding-left: 8px; margin-left: -30px; text-decoration: none; }
.markdown-preview[data-use-github-style] h1:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h2:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h3:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h4:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h5:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h6:hover .anchor .octicon-link { display: inline-block; }
.markdown-preview[data-use-github-style] h1 tt, .markdown-preview[data-use-github-style] h2 tt, .markdown-preview[data-use-github-style] h3 tt, .markdown-preview[data-use-github-style] h4 tt, .markdown-preview[data-use-github-style] h5 tt, .markdown-preview[data-use-github-style] h6 tt, .markdown-preview[data-use-github-style] h1 code, .markdown-preview[data-use-github-style] h2 code, .markdown-preview[data-use-github-style] h3 code, .markdown-preview[data-use-github-style] h4 code, .markdown-preview[data-use-github-style] h5 code, .markdown-preview[data-use-github-style] h6 code { font-size: inherit; }
.markdown-preview[data-use-github-style] h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
.markdown-preview[data-use-github-style] h1 .anchor { line-height: 1; }
.markdown-preview[data-use-github-style] h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
.markdown-preview[data-use-github-style] h2 .anchor { line-height: 1; }
.markdown-preview[data-use-github-style] h3 { font-size: 1.5em; line-height: 1.43; }
.markdown-preview[data-use-github-style] h3 .anchor { line-height: 1.2; }
.markdown-preview[data-use-github-style] h4 { font-size: 1.25em; }
.markdown-preview[data-use-github-style] h4 .anchor { line-height: 1.2; }
.markdown-preview[data-use-github-style] h5 { font-size: 1em; }
.markdown-preview[data-use-github-style] h5 .anchor { line-height: 1.1; }
.markdown-preview[data-use-github-style] h6 { font-size: 1em; color: rgb(119, 119, 119); }
.markdown-preview[data-use-github-style] h6 .anchor { line-height: 1.1; }
.markdown-preview[data-use-github-style] p, .markdown-preview[data-use-github-style] blockquote, .markdown-preview[data-use-github-style] ul, .markdown-preview[data-use-github-style] ol, .markdown-preview[data-use-github-style] dl, .markdown-preview[data-use-github-style] table, .markdown-preview[data-use-github-style] pre { margin-top: 0px; margin-bottom: 16px; }
.markdown-preview[data-use-github-style] hr { height: 4px; padding: 0px; margin: 16px 0px; background-color: rgb(231, 231, 231); border: 0px none; }
.markdown-preview[data-use-github-style] ul, .markdown-preview[data-use-github-style] ol { padding-left: 2em; }
.markdown-preview[data-use-github-style] ul.no-list, .markdown-preview[data-use-github-style] ol.no-list { padding: 0px; list-style-type: none; }
.markdown-preview[data-use-github-style] ul ul, .markdown-preview[data-use-github-style] ul ol, .markdown-preview[data-use-github-style] ol ol, .markdown-preview[data-use-github-style] ol ul { margin-top: 0px; margin-bottom: 0px; }
.markdown-preview[data-use-github-style] li > p { margin-top: 16px; }
.markdown-preview[data-use-github-style] dl { padding: 0px; }
.markdown-preview[data-use-github-style] dl dt { padding: 0px; margin-top: 16px; font-size: 1em; font-style: italic; font-weight: bold; }
.markdown-preview[data-use-github-style] dl dd { padding: 0px 16px; margin-bottom: 16px; }
.markdown-preview[data-use-github-style] blockquote { padding: 0px 15px; color: rgb(119, 119, 119); border-left: 4px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] blockquote > :first-child { margin-top: 0px; }
.markdown-preview[data-use-github-style] blockquote > :last-child { margin-bottom: 0px; }
.markdown-preview[data-use-github-style] table { display: block; width: 100%; overflow: auto; word-break: keep-all; }
.markdown-preview[data-use-github-style] table th { font-weight: bold; }
.markdown-preview[data-use-github-style] table th, .markdown-preview[data-use-github-style] table td { padding: 6px 13px; border: 1px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] table tr { background-color: rgb(255, 255, 255); border-top: 1px solid rgb(204, 204, 204); }
.markdown-preview[data-use-github-style] table tr:nth-child(2n) { background-color: rgb(248, 248, 248); }
.markdown-preview[data-use-github-style] img { max-width: 100%; box-sizing: border-box; }
.markdown-preview[data-use-github-style] .emoji { max-width: none; }
.markdown-preview[data-use-github-style] span.frame { display: block; overflow: hidden; }
.markdown-preview[data-use-github-style] span.frame > span { display: block; float: left; width: auto; padding: 7px; margin: 13px 0px 0px; overflow: hidden; border: 1px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] span.frame span img { display: block; float: left; }
.markdown-preview[data-use-github-style] span.frame span span { display: block; padding: 5px 0px 0px; clear: both; color: rgb(51, 51, 51); }
.markdown-preview[data-use-github-style] span.align-center { display: block; overflow: hidden; clear: both; }
.markdown-preview[data-use-github-style] span.align-center > span { display: block; margin: 13px auto 0px; overflow: hidden; text-align: center; }
.markdown-preview[data-use-github-style] span.align-center span img { margin: 0px auto; text-align: center; }
.markdown-preview[data-use-github-style] span.align-right { display: block; overflow: hidden; clear: both; }
.markdown-preview[data-use-github-style] span.align-right > span { display: block; margin: 13px 0px 0px; overflow: hidden; text-align: right; }
.markdown-preview[data-use-github-style] span.align-right span img { margin: 0px; text-align: right; }
.markdown-preview[data-use-github-style] span.float-left { display: block; float: left; margin-right: 13px; overflow: hidden; }
.markdown-preview[data-use-github-style] span.float-left span { margin: 13px 0px 0px; }
.markdown-preview[data-use-github-style] span.float-right { display: block; float: right; margin-left: 13px; overflow: hidden; }
.markdown-preview[data-use-github-style] span.float-right > span { display: block; margin: 13px auto 0px; overflow: hidden; text-align: right; }
.markdown-preview[data-use-github-style] code, .markdown-preview[data-use-github-style] tt { padding: 0.2em 0px; margin: 0px; font-size: 85%; background-color: rgba(0, 0, 0, 0.0392157); border-radius: 3px; }
.markdown-preview[data-use-github-style] code::before, .markdown-preview[data-use-github-style] tt::before, .markdown-preview[data-use-github-style] code::after, .markdown-preview[data-use-github-style] tt::after { letter-spacing: -0.2em; content: " "; }
.markdown-preview[data-use-github-style] code br, .markdown-preview[data-use-github-style] tt br { display: none; }
.markdown-preview[data-use-github-style] del code { text-decoration: inherit; }
.markdown-preview[data-use-github-style] pre > code { padding: 0px; margin: 0px; font-size: 100%; word-break: normal; white-space: pre; background: transparent; border: 0px; }
.markdown-preview[data-use-github-style] .highlight { margin-bottom: 16px; }
.markdown-preview[data-use-github-style] .highlight pre, .markdown-preview[data-use-github-style] pre { padding: 16px; overflow: auto; font-size: 85%; line-height: 1.45; background-color: rgb(247, 247, 247); border-radius: 3px; }
.markdown-preview[data-use-github-style] .highlight pre { margin-bottom: 0px; word-break: normal; }
.markdown-preview[data-use-github-style] pre { word-wrap: normal; }
.markdown-preview[data-use-github-style] pre code, .markdown-preview[data-use-github-style] pre tt { display: inline; max-width: initial; padding: 0px; margin: 0px; overflow: initial; line-height: inherit; word-wrap: normal; background-color: transparent; border: 0px; }
.markdown-preview[data-use-github-style] pre code::before, .markdown-preview[data-use-github-style] pre tt::before, .markdown-preview[data-use-github-style] pre code::after, .markdown-preview[data-use-github-style] pre tt::after { content: normal; }
.markdown-preview[data-use-github-style] kbd { display: inline-block; padding: 3px 5px; font-size: 11px; line-height: 10px; color: rgb(85, 85, 85); vertical-align: middle; background-color: rgb(252, 252, 252); border-width: 1px; border-style: solid; border-color: rgb(204, 204, 204) rgb(204, 204, 204) rgb(187, 187, 187); border-image: initial; border-radius: 3px; box-shadow: rgb(187, 187, 187) 0px -1px 0px inset; }
.markdown-preview[data-use-github-style] a { color: rgb(51, 122, 183); }
.markdown-preview[data-use-github-style] code { color: inherit; }
.markdown-preview[data-use-github-style] pre.editor-colors { padding: 0.8em 1em; margin-bottom: 1em; font-size: 0.85em; border-radius: 4px; overflow: auto; }
.scrollbars-visible-always .markdown-preview pre.editor-colors .vertical-scrollbar, .scrollbars-visible-always .markdown-preview pre.editor-colors .horizontal-scrollbar { visibility: hidden; }
.scrollbars-visible-always .markdown-preview pre.editor-colors:hover .vertical-scrollbar, .scrollbars-visible-always .markdown-preview pre.editor-colors:hover .horizontal-scrollbar { visibility: visible; }
.markdown-preview .task-list-item-checkbox { position: absolute; margin: 0.25em 0px 0px -1.4em; }
.markdown-preview code { text-shadow: none; }
.bracket-matcher .region {
  border-bottom: 1px dotted lime;
  position: absolute;
}
.line-number.bracket-matcher {
  background-color: #777;
}

.spell-check-misspelling .region {
  border-bottom: 2px dotted rgba(255, 51, 51, 0.75);
}
.spell-check-corrections {
  width: 25em !important;
}

pre.editor-colors {
  background-color: #1d1f21;
  color: #c5c8c6;
}
pre.editor-colors .invisible-character {
  color: rgba(197, 200, 198, 0.2);
}
pre.editor-colors .indent-guide {
  color: rgba(197, 200, 198, 0.2);
}
pre.editor-colors .wrap-guide {
  background-color: rgba(197, 200, 198, 0.1);
}
pre.editor-colors .gutter {
  background-color: #292c2f;
}
pre.editor-colors .gutter .cursor-line {
  background-color: rgba(255, 255, 255, 0.14);
}
pre.editor-colors .line-number.cursor-line-no-selection {
  background-color: rgba(255, 255, 255, 0.14);
}
pre.editor-colors .gutter .line-number.folded,
pre.editor-colors .gutter .line-number:after,
pre.editor-colors .fold-marker:after {
  color: #fba0e3;
}
pre.editor-colors .invisible {
  color: #c5c8c6;
}
pre.editor-colors .cursor {
  border-color: white;
}
pre.editor-colors .selection .region {
  background-color: #444;
}
pre.editor-colors .bracket-matcher .region {
  border-bottom: 1px solid #f8de7e;
  margin-top: -1px;
  opacity: .7;
}
pre.editor-colors .syntax--source.syntax--gfm {
  color: #999;
}
pre.editor-colors .syntax--gfm .syntax--markup.syntax--heading {
  color: #eee;
}
pre.editor-colors .syntax--gfm .syntax--link {
  color: #555;
}
pre.editor-colors .syntax--gfm .syntax--variable.syntax--list,
pre.editor-colors .syntax--gfm .syntax--support.syntax--quote {
  color: #555;
}
pre.editor-colors .syntax--gfm .syntax--link .syntax--entity {
  color: #ddd;
}
pre.editor-colors .syntax--gfm .syntax--raw {
  color: #aaa;
}
pre.editor-colors .syntax--markdown .syntax--paragraph {
  color: #999;
}
pre.editor-colors .syntax--markdown .syntax--heading {
  color: #eee;
}
pre.editor-colors .syntax--markdown .syntax--raw {
  color: #aaa;
}
pre.editor-colors .syntax--markdown .syntax--link {
  color: #555;
}
pre.editor-colors .syntax--markdown .syntax--link .syntax--string {
  color: #555;
}
pre.editor-colors .syntax--markdown .syntax--link .syntax--string.syntax--title {
  color: #ddd;
}
.syntax--comment {
  color: #7C7C7C;
}
.syntax--entity {
  color: #FFD2A7;
}
.syntax--entity.syntax--name.syntax--type {
  text-decoration: underline;
  color: #FFFFB6;
}
.syntax--entity.syntax--other.syntax--inherited-class {
  color: #9B5C2E;
}
.syntax--keyword {
  color: #96CBFE;
}
.syntax--keyword.syntax--control {
  color: #96CBFE;
}
.syntax--keyword.syntax--operator {
  color: #EDEDED;
}
.syntax--storage {
  color: #CFCB90;
}
.syntax--storage.syntax--modifier {
  color: #96CBFE;
}
.syntax--constant {
  color: #99CC99;
}
.syntax--constant.syntax--numeric {
  color: #FF73FD;
}
.syntax--variable {
  color: #C6C5FE;
}
.syntax--invalid.syntax--deprecated {
  text-decoration: underline;
  color: #FD5FF1;
}
.syntax--invalid.syntax--illegal {
  color: #FD5FF1;
  background-color: rgba(86, 45, 86, 0.75);
}
.syntax--string .syntax--source,
.syntax--string .syntax--meta.syntax--embedded.syntax--line {
  color: #EDEDED;
}
.syntax--string .syntax--punctuation.syntax--section.syntax--embedded {
  color: #00A0A0;
}
.syntax--string .syntax--punctuation.syntax--section.syntax--embedded .syntax--source {
  color: #00A0A0;
}
.syntax--string {
  color: #A8FF60;
}
.syntax--string .syntax--constant {
  color: #00A0A0;
}
.syntax--string.syntax--regexp {
  color: #E9C062;
}
.syntax--string.syntax--regexp .syntax--constant.syntax--character.syntax--escape,
.syntax--string.syntax--regexp .syntax--source.syntax--ruby.syntax--embedded,
.syntax--string.syntax--regexp .syntax--string.syntax--regexp.syntax--arbitrary-repetition {
  color: #FF8000;
}
.syntax--string.syntax--regexp.syntax--group {
  color: #C6A24F;
  background-color: rgba(255, 255, 255, 0.06);
}
.syntax--string.syntax--regexp.syntax--character-class {
  color: #B18A3D;
}
.syntax--string .syntax--variable {
  color: #8A9A95;
}
.syntax--support {
  color: #FFFFB6;
}
.syntax--support.syntax--function {
  color: #DAD085;
}
.syntax--support.syntax--constant {
  color: #FFD2A7;
}
.syntax--support.syntax--type.syntax--property-name.syntax--css {
  color: #EDEDED;
}
.syntax--source .syntax--entity.syntax--name.syntax--tag,
.syntax--source .syntax--punctuation.syntax--tag {
  color: #96CBFE;
}
.syntax--source .syntax--entity.syntax--other.syntax--attribute-name {
  color: #C6C5FE;
}
.syntax--entity.syntax--other.syntax--attribute-name {
  color: #C6C5FE;
}
.syntax--entity.syntax--name.syntax--tag.syntax--namespace,
.syntax--entity.syntax--other.syntax--attribute-name.syntax--namespace {
  color: #E18964;
}
.syntax--meta.syntax--preprocessor.syntax--c {
  color: #8996A8;
}
.syntax--meta.syntax--preprocessor.syntax--c .syntax--keyword {
  color: #AFC4DB;
}
.syntax--meta.syntax--cast {
  color: #676767;
}
.syntax--meta.syntax--sgml.syntax--html .syntax--meta.syntax--doctype,
.syntax--meta.syntax--sgml.syntax--html .syntax--meta.syntax--doctype .syntax--entity,
.syntax--meta.syntax--sgml.syntax--html .syntax--meta.syntax--doctype .syntax--string,
.syntax--meta.syntax--xml-processing,
.syntax--meta.syntax--xml-processing .syntax--entity,
.syntax--meta.syntax--xml-processing .syntax--string {
  color: #494949;
}
.syntax--meta.syntax--tag .syntax--entity,
.syntax--meta.syntax--tag > .syntax--punctuation,
.syntax--meta.syntax--tag.syntax--inline .syntax--entity {
  color: #C6C5FE;
}
.syntax--meta.syntax--tag .syntax--name,
.syntax--meta.syntax--tag.syntax--inline .syntax--name,
.syntax--meta.syntax--tag > .syntax--punctuation {
  color: #96CBFE;
}
.syntax--meta.syntax--selector.syntax--css .syntax--entity.syntax--name.syntax--tag {
  text-decoration: underline;
  color: #96CBFE;
}
.syntax--meta.syntax--selector.syntax--css .syntax--entity.syntax--other.syntax--attribute-name.syntax--tag.syntax--pseudo-class {
  color: #8F9D6A;
}
.syntax--meta.syntax--selector.syntax--css .syntax--entity.syntax--other.syntax--attribute-name.syntax--id {
  color: #8B98AB;
}
.syntax--meta.syntax--selector.syntax--css .syntax--entity.syntax--other.syntax--attribute-name.syntax--class {
  color: #62B1FE;
}
.syntax--meta.syntax--property-group .syntax--support.syntax--constant.syntax--property-value.syntax--css,
.syntax--meta.syntax--property-value .syntax--support.syntax--constant.syntax--property-value.syntax--css {
  color: #F9EE98;
}
.syntax--meta.syntax--preprocessor.syntax--at-rule .syntax--keyword.syntax--control.syntax--at-rule {
  color: #8693A5;
}
.syntax--meta.syntax--property-value .syntax--support.syntax--constant.syntax--named-color.syntax--css,
.syntax--meta.syntax--property-value .syntax--constant {
  color: #87C38A;
}
.syntax--meta.syntax--constructor.syntax--argument.syntax--css {
  color: #8F9D6A;
}
.syntax--meta.syntax--diff,
.syntax--meta.syntax--diff.syntax--header {
  color: #F8F8F8;
  background-color: #0E2231;
}
.syntax--meta.syntax--separator {
  color: #60A633;
  background-color: #242424;
}
.syntax--meta.syntax--line.syntax--entry.syntax--logfile,
.syntax--meta.syntax--line.syntax--exit.syntax--logfile {
  background-color: rgba(238, 238, 238, 0.16);
}
.syntax--meta.syntax--line.syntax--error.syntax--logfile {
  background-color: #751012;
}
</style>
  </head>
  <body class='markdown-preview' data-use-github-style><h1>Image</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:center">CUHK03</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Viewpoint Invariant Pedestrian Recognition with an Ensemble of Localized Features</td>
<td style="text-align:center">Hai Tao<br>University of California, Santa Cruz</td>
<td style="text-align:center">ECCV 2008</td>
<td style="text-align:left">定义了一个特征空间，让机器学习算法去寻找最好的表达</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)<br>Feature Regions<br>Feature Binning</td>
<td style="text-align:center">L1 Distance</td>
<td style="text-align:left">使用了AdaBoost</td>
<td style="text-align:center">--</td>
<td style="text-align:left"><strong>VIPeR</strong> 12</td>
</tr>
<tr>
<td style="text-align:center">DeepReID: Deep Filter Pairing Neural Network for Person Re-identification</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2014</td>
<td style="text-align:left">1. 学习到的特征对能编码光照变化<br>2. 提出一个新的数据集</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">网络输出为二分类，直接判断两者是否为同一个人</td>
<td style="text-align:center">manually 20.65<br>detected 19.89</td>
<td style="text-align:left"><strong>CUHK01</strong>(100 testID) 27.87</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification with Discriminatively Trained Viewpoint Invariant Dictionaries</td>
<td style="text-align:center">Richard J. Radke<br>RPI</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:left">学习一个字典，能同时学习 Probe 和 Gallery 的表达</td>
<td style="text-align:center">Color Histograms<br>Schmid &amp; Gabor Filters</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 用LFDA为特征降维<br>2. 训练时，在特征向量的稀疏表达上加上明确的限制去训练一个字典<br>3. 在测试时，从库图片中找出与榻侧图片，两者的稀疏表达在欧氏距离最近的一个</td>
<td style="text-align:center">--</td>
<td style="text-align:left"><strong>PRID</strong> 40.6<br><strong>iLIDS</strong> 25.9</td>
</tr>
<tr>
<td style="text-align:center"><em>An Improved Deep Learning Architecture for Person Re-identification</em></td>
<td style="text-align:center">Tim K. Marks<br>MERL</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:left">新的块匹配方法</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. probe某区域块与同位置的邻域内gallery块皆做差分<br>2. 这样对于错位有一定的容忍性</td>
<td style="text-align:center">manually 54.74<br>detected 44.96</td>
<td style="text-align:left"><strong>CUHK01</strong> (100) 65 (486) 47.53<br><strong>VIPeR</strong> 34.81</td>
</tr>
<tr>
<td style="text-align:center">Deep Feature Learning with Relative Distance Comparison for Person Re-identification</td>
<td style="text-align:center">Hongyang Chao<br>Sun Yat-sen University</td>
<td style="text-align:center">PR 2015</td>
<td style="text-align:left">基于深度学习的三路网络框架</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 在反向传播上做了优化，减少重复计算<br>2. 提供了构建Triplet三元组的方法</td>
<td style="text-align:center">--</td>
<td style="text-align:left"><strong>iLIDS</strong> 52.1<br> <strong>VIPeR</strong> 40.5</td>
</tr>
<tr>
<td style="text-align:center">Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">当在多个数据集上学习时，会发现有一些神经元跨域表达而一些神经元只会对指定的域有效</td>
<td style="text-align:center">inception</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">通过比较某个神经元被置零前后loss的变化得到其是否为域敏感</td>
<td style="text-align:center">all 75.3</td>
<td style="text-align:left"><strong>CUHK01</strong>(485 testID) 66.6<br><strong>PRID</strong> 64</td>
</tr>
<tr>
<td style="text-align:center">Deep Neural Networks with Inexact Matching for Person Re-identification</td>
<td style="text-align:center">Anurag Mittal<br>IITM</td>
<td style="text-align:center">NIPS 2016</td>
<td style="text-align:left">用相关系数改进块相似性度量</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">对于Probe图片，将区域块与Gallery相应位置整个条带上的块做相关性系数计算</td>
<td style="text-align:center">manually 72.43<br>detected 72.04</td>
<td style="text-align:left"><strong>CUHK01</strong> (100) 81.23 (486) 65.04<br><strong>GRID</strong> 19.20</td>
</tr>
<tr>
<td style="text-align:center"><em>Gated Siamese Convolutional Neural Network Architecture for Huam Re-identification</em></td>
<td style="text-align:center">Gang Wang<br>Nanyang Technology University</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">提出一个门函数，通过对比图片的中层特征，能选择性增强精细的局部模式</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 双路结构，信息向上传播时会有门结构对特征进行选择<br>2. 门结构是用两者的特征差，借助高斯激活函数作为门值</td>
<td style="text-align:center">detected SQ 61.8 MQ 68.1</td>
<td style="text-align:left"><strong>Market 1501</strong><br>SQ rank 65.88 map 39.55<br>MQ rank 76.4 map 48.45<br><strong>VIPeR</strong> 37.8</td>
</tr>
<tr>
<td style="text-align:center">Semantics-Aware Deep Correspondence Structure Learning for Robust Person Re-identification</td>
<td style="text-align:center">Zhongfei Zhang<br>Zhejiang University</td>
<td style="text-align:center">IJCAL 2016</td>
<td style="text-align:left">希望得到语义层面的图像表达</td>
<td style="text-align:center">改编的GoogleNet</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 双路融合为一路<br>2. 融合时对两者特征图分别构建金字塔特征图<br>3. 两者同尺度的特征取Max操作<br>4. 网络最后为二分类</td>
<td style="text-align:center">manually 80.2</td>
<td style="text-align:left"><strong>CUHK01</strong><br>(100 testID) 89.60<br>(486 testID) 76.54<br><strong>VIPeR</strong>(316 testID) 44.62</td>
</tr>
<tr>
<td style="text-align:center">Joint Learning of Single-image and Cross-image Representations for Person Re-identification</td>
<td style="text-align:center">Lei Zhang<br>Sun Yat-sen University</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">将分类与验证相结合</td>
<td style="text-align:center">三层CNN</td>
<td style="text-align:center">Euclidean Distance+RankSVM</td>
<td style="text-align:left">1. 可以应用到双路网络与三路网络</td>
<td style="text-align:center">detected 52.17</td>
<td style="text-align:left"><strong>CUHK01</strong>(100 testID) 71.80<br><strong>VIPeR</strong> 35.76</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function</td>
<td style="text-align:center">Nanning Zheng<br>Xi'an JiaoTong University</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">1. 多通道学习整个身体与局部肢体<br>2. 用新的Triplet loss 去加强学习效果</td>
<td style="text-align:center">三层CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. Conv1对整个图提特征<br>2. 沿高度将Conv1分为四部分，分别用四个支路去学习局部特征<br>3. 再训练一个支路对Conv1直接学习<br>4. 五个支路输出特征级联<br>5. 不仅要求正样本对距离小于负样本对，还要求正样本对距离小于某个值</td>
<td style="text-align:center">--</td>
<td style="text-align:left"><strong>iLIDS</strong> 60.4<br><strong>PRID</strong> 22.0<br><strong>VIPeR</strong> 47.8<br><strong>CUHK01</strong>(486 testID) 53.7</td>
</tr>
<tr>
<td style="text-align:center">A siamese Long Short-Term Memory Architecture for Human Re-identification</td>
<td style="text-align:center">Gang Wang<br>University of Sydney</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">通过上下文信息增强区分局部特征的能力</td>
<td style="text-align:center">LOMO</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">沿高度分为多个水平条带，并依次送入LSTM聚合</td>
<td style="text-align:center">detected 57.3</td>
<td style="text-align:left"><strong>Market 1501</strong><br>MQ rank 61.6 map 35.3<br><strong>VIPeR</strong> 42.4</td>
</tr>
<tr>
<td style="text-align:center">End-to-End Comparative Attention Networks for Person Re-identification</td>
<td style="text-align:center">Shucheng Yan<br></td>
<td style="text-align:center">TIP 2016</td>
<td style="text-align:left">学习到注意力模型</td>
<td style="text-align:center">AlexNet VGG</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">CNN提取特征，再送入LSTM，用LSTM每一步的h学习mask矩阵，利用mask矩阵与原特征相乘得到有注意力的结果</td>
<td style="text-align:center">manually 77.6<br>detected 69.2</td>
<td style="text-align:left"><strong>CUHK01</strong> (100 testID) 87.2<br><strong>Market 1501</strong><br>SQ rank 60.3 map 35.9<br>MQ rank 72.1 map 47.9<br><strong>VIPeR</strong> 54.1</td>
</tr>
<tr>
<td style="text-align:center">Multi-scale Triplet CNN for Person Re-identification</td>
<td style="text-align:center">Tao Mei<br>MSRA</td>
<td style="text-align:center">MM 2016</td>
<td style="text-align:left">结合多尺度信息</td>
<td style="text-align:center">AlexNet</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 整体为三路网络，修改的Triplet Loss外加正样本对的距离Loss<br>2. 多尺度信息体现在将原始图片下采样为两种尺度，加上原尺寸，共三种尺寸，分别训三个AlexNet，再融合</td>
<td style="text-align:center">--</td>
<td style="text-align:left"><strong>Market 1501</strong><br>SQ rank 45.1<br>MQ 55.4</td>
</tr>
<tr>
<td style="text-align:center">Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">利用行人的身体关键点辅助识别</td>
<td style="text-align:center">inception</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 将人的肢体分为不同的粒度去提取特征，三个大区域，四个小区域<br>2. 先用CPM对图片提取关节点，再根据关节点位置框出7个区域<br> 3. 在提取特征时考虑了不同粒度，融合不同粒度时也有先后之分</td>
<td style="text-align:center">all 88.5</td>
<td style="text-align:left"><strong>CUHK01</strong>(485 testID) 79.9<br><strong>PRID</strong> 67<br><strong>VIPeR</strong> 53.8<br><strong>3DPeS</strong> 62.1<br><strong>iLIDS</strong> 66.3<br><strong>Market 1501</strong> SQ rank 76.9</td>
</tr>
<tr>
<td style="text-align:center">Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification</td>
<td style="text-align:center">Kaiqihuang<br>CRIPAC &amp; NLPR, CASIA</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">学习更好的基于整个身体和局部身体的特征</td>
<td style="text-align:center">四层CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 用不同膨胀率的卷积核构建类inception结构，可得到不同大小的感受野<br>2. 用google的STN网络学习抠图，得到身体划分，划分为三个部分<br>3. 整体与局部的特征级联的到最后的表达</td>
<td style="text-align:center">manually 74.21<br>detected 67.99</td>
<td style="text-align:left"><strong>Market 1501</strong><br>SQ rank 80.31 map 57.53<br>MQ rank 86.79 map 66.7<br><strong>MARS</strong> SQ 71.77 MQ 83.03</td>
</tr>
<tr>
<td style="text-align:center">Beyond Triplet Loss: a Deep Quadruplet Network for Person Re-identification</td>
<td style="text-align:center">Kaiqi Huang<br>CRIPAC&amp;NLPR,CASIA</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">四路网络+基于阈值的hard negative mining</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 测试时相当于二分类<br>2. 相对于三路网络增加了负样本与负样本的限制</td>
<td style="text-align:center">manually 75.53</td>
<td style="text-align:left"><strong>CUHK01</strong><br>(486 testID) 62.55<br>(100 testID) 81<br><strong>VIPeR</strong> 49.05</td>
</tr>
<tr>
<td style="text-align:center"><em>A Multi-task Deep Network for Person Re-identification</em></td>
<td style="text-align:center">Kaiqihuang<br>CRIPAC &amp; NLPR, CASIA</td>
<td style="text-align:center">AAAI 2017</td>
<td style="text-align:left">多任务框架，二分类模型与排序模型同时做，同时也考虑了跨数据集的半监督学习</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">主体为三路网络，在其基础上，正对与负对也被用来训练一个二分类分支</td>
<td style="text-align:center">manually 74.68</td>
<td style="text-align:left"><strong>CUHK01</strong> (100) 78.5 (486) 59.67<br><strong>VIPeR</strong> 47.47<br><strong>iLIDS</strong> 58.38<br><strong>PRID</strong> 31</td>
</tr>
<tr>
<td style="text-align:center">Point to Set Similarity Based Deep Feature Learning for Person Re-identification</td>
<td style="text-align:center">Nanning Zheng<br> Xi'an Jiaotong University</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">用点对集合来作为相似性度量</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. P2S改进的Triplet Loss + Contrastive Loss<br>2. 身体局部与整体的不同尺度学习</td>
<td style="text-align:center">--</td>
<td style="text-align:left"><strong>3DPeS</strong> 71.16<br><strong>CUHK01</strong> 77.34<br><strong>PRID</strong> 70.71<br><strong>Market 1501</strong> SQ rank 70.72 map 44.27<br>MQ rank 85.78 map 55.73</td>
</tr>
<tr>
<td style="text-align:center">Consistent-Aware Deep Learning for Person Re-identification in a Cammera Network</td>
<td style="text-align:center">Jie Zhou<br>Tsinghua University</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">最大化整个网络的人物匹配，而不是每次只关注一个图片对或几个图片对</td>
<td style="text-align:center">训练好的Domain Guide Model</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. 用网络提取的特征计算余弦距离，构建相似性矩阵C，行为一个摄像头下的人，列为另一个摄像头下的人<br>2. 构建C相对应的邻接矩阵，同一个人则为1否则为0<br>3. 训练时要最大化C与H的点点相乘，并使H的预测值与真实值误差尽可能小<br>4. C与H是交替优化的</td>
<td style="text-align:center">--</td>
<td style="text-align:left"><strong>Market 1501</strong><br>SQ rank 73.84 map 47.11<br>MQ rank 80.85 map 55.58</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification by Deep Joint Learning of Multi-Loss Classification</td>
<td style="text-align:center">Shaogang Gong<br>Queen Mary University of London</td>
<td style="text-align:center">IJCAI 2017</td>
<td style="text-align:left">局部特征与整体特征一起学习</td>
<td style="text-align:center">改编的ResNet</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 单路网络，作为多分类任务<br>2. 先在ImageNet上预训练，再在目标数据集上训练<br>3. 在两层公用结构之后便分为两部分，一部分是整体特征学习，另一部分是四个水平条带对应学习局部特征<br>5. 这两个部分各自有一个分类loss，并不融合,并用实验表明不融合更好<br>6. 测试时将两部分特征级联作为最后表达</td>
<td style="text-align:center">manually 83.2<br>detected 80.6</td>
<td style="text-align:left"><strong>Market 1501</strong><br>SQ ( rank 85.1 ) ( map 65.5 )<br>MQ ( rank 89.7 ) ( map 74.5 )<br><strong>CUHK01</strong><br>(100) SQ 87.0 MQ 91.2 <br>(486) SQ 69.8 MQ 76.7<br><strong>VIPeR</strong> 50.2<br><strong>GRID</strong> 37.5</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://github.com/zlmzju/part_reid"><em>Deeply-Learned Part-Aligned Representations for Person Re-identification</em></a></td>
<td style="text-align:center">Jingdong Wang<br>MSRA</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:left">学习对特征图加权，以此选出特征图中较为显著的区域</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 用GoogleNet提取的特征 HxWxC,用一个卷积层学习k个HxW的特征图:M<br>2. M 可以视为mask,即为对原特征的不同部分的响应，用每个HxW响应对原HxWxC加权得到新的k个HxWxC<br>对于新的特征图，经过Global Average Pooling和全连接层得到固定长度表达</td>
<td style="text-align:center">manually 85.4<br>detected 81.6</td>
<td style="text-align:left"><strong>Market 1501</strong> SQ rank 81.0 map 63.4<br><strong>CUHK01</strong><br>(100) 88.5<br>(486) 75<br><strong>VIPeR</strong> 48.7</td>
</tr>
<tr>
<td style="text-align:center">Multi-scale Deep Learning Architectures for Person Re-identification</td>
<td style="text-align:center">Xiangyang Xue<br>Fudan University</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:left">利用多尺度特征来充分利用图片的细节信息，同时在级联多尺度信息时，利用加权做了选择筛选</td>
<td style="text-align:center">GoogleNet修改版</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 整体为双路网路，两个分支各有一个分类Loss，中间是将两路的特征相减后取平方值，经一个全连接层得到最后表达，再接二分类。<br>2. 多尺度信息是利用不同大小的卷积核实现的，整体结构类似GoogleNet<br>3. 对于最后的特征，每个channel都学习一个对应的加权值。加权值是两个支路共享的,直接学习，未加先验和限制。<br>4. <strong>从作者的实验中可以看出多尺度网络对于detected的图片效果依旧很好，可能图中只有小部分是人的，但是因为多尺度而能被网络注意到</strong></td>
<td style="text-align:center">manually 76.87<br>detected 75.64</td>
<td style="text-align:left"><strong>CUHK01</strong> (100)79.01<br><strong>VIPeR</strong> 43.03</td>
</tr>
<tr>
<td style="text-align:center">AlignedReID: Surpassing Human-Level Performance in Person Re-identification</td>
<td style="text-align:center">Jian Sun<br>Face++</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:left">用局部特征去帮助全局特征的学习</td>
<td style="text-align:center">Resnet50-X</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. triplet loss(in denfense of the triplet loss for ReID)<br>2. 局部特征是最后特征图水平方向GAP。全局特征是水平垂直都GAP<br>3. 比较两者局部特征使用了动态规划<br> 4. 训练时loss由全局特征距离与局部特征距离共同组成<br>5. 用两个这种网络协同学习<br>6. 测试时只是用全局特征算欧氏距离</td>
<td style="text-align:center">manually 96.1</td>
<td style="text-align:left"><strong>Market 1501</strong> SQ rank 94.0 map 91.2<br><strong>MARS</strong> SQ rank 87.5 map 85.6<br><strong>CUHK-SYSU</strong> rank 95.3 map 93.7</td>
</tr>
</tbody>
</table>
<hr>
<h1>Video</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Fusion</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:center">iLIDS</th>
<th style="text-align:center">PRID</th>
<th style="text-align:center">MARS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Person Re-identification by Video Ranking</td>
<td style="text-align:center">Shenjing Wang<br>Queen Mary University of London</td>
<td style="text-align:center">ECCV 2014</td>
<td style="text-align:left">1. 能从有噪声的帧序列中选出关键帧<br> 2. 学习一个视频排序函数</td>
<td style="text-align:center">HOG3D</td>
<td style="text-align:center">不融合，放在特征池中，供比较.</td>
<td style="text-align:center">1. 学习一个矩阵，矩阵与两人特征差的乘即代表距离<br>2. 将两人的特征两两比较距离，最大的距离代表最后的距离</td>
<td style="text-align:left">1. 只取图片的下半部分，定义能量函数FEP，能量值随帧变化<br> 2. 对每个图，在极大值与极小值点前后取共10帧<br></td>
<td style="text-align:center">28.9</td>
<td style="text-align:center">23.3</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center">Sparse Re-ID: Block Sparsity for Person Re-identification</td>
<td style="text-align:center">Richard J. Radke<br>RPI</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:left">Probe图片的特征向量可以近似看成<br>处于Gallery图片特征向量所处的embedding space</td>
<td style="text-align:center">Color Histograms<br>Schmid &amp; Gabor Filters</td>
<td style="text-align:center">级联构成字典的一部分</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">构建一个字典</td>
<td style="text-align:center">24.9</td>
<td style="text-align:center">35.1</td>
<td style="text-align:center">---</td>
</tr>
<tr>
<td style="text-align:center">A Spatio-temporal Appearance Representation for <br>Video-based Pedestrian Re-identification</td>
<td style="text-align:center">Rui Huang<br>Shandong University &amp; UCAS</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:left">1. 处理时间空间对齐问题 <br>2. 空间上按身体部位划分为不同的块<br>3. 时间上用FEP</td>
<td style="text-align:center">Fiser vector</td>
<td style="text-align:center">1. 空间上6部分特征级联得到帧表达<br>2. 时间上级联得到视频表达</td>
<td style="text-align:center">最近邻分类器</td>
<td style="text-align:left">1. 用傅里叶变换对FEP去噪 <br>2. 空间上按头，四肢，上身，将身体分为6个部分</td>
<td style="text-align:center">44.3</td>
<td style="text-align:center">64.1</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center">Deep Recurrent Convolutional Networks for Video-based <br>Person Re-identification: An End-to-End Approach</td>
<td style="text-align:center">Chunhua Shen<br>The University of Adelaide</td>
<td style="text-align:center">Arxiv 2016</td>
<td style="text-align:left">同时学习时间空间特征和相似性矩阵</td>
<td style="text-align:center">四层卷积网络</td>
<td style="text-align:center">GRU+average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">GRU中用卷积操作代替了全连接层</td>
<td style="text-align:center">42.6</td>
<td style="text-align:center">49.8</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center">Top-push Video-based Person Re-identification</td>
<td style="text-align:center">Weishi Zheng<br>Sun Yat-sen University</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">不同的人有相似的表现而引发类间距离较小</td>
<td style="text-align:center">HOG3D+color histograms+LBP</td>
<td style="text-align:center">级联</td>
<td style="text-align:center">马氏距离</td>
<td style="text-align:left">学习马氏距离的矩阵M</td>
<td style="text-align:center">56.33</td>
<td style="text-align:center">56.74</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification by Exploiting <br>Spatio-temporal Cues and Multi-view Metric Learning</td>
<td style="text-align:center">Yuanyan Wang<br>Bejing Forestry University</td>
<td style="text-align:center">IEEE SRL 2016</td>
<td style="text-align:left">提出新的时空特征及匹配方法</td>
<td style="text-align:center">从标准化的光流能量图中提取LBP</td>
<td style="text-align:center">级联</td>
<td style="text-align:center">马氏距离</td>
<td style="text-align:left">优化马氏距离中的W</td>
<td style="text-align:center">69.13</td>
<td style="text-align:center">66.78</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center">Person Re-identification via Recurrent Feature Aggregation</td>
<td style="text-align:center">Xiaokang Yang<br>Shanghai Jioa Tong University</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">用LSTM融合时间信息</td>
<td style="text-align:center">LBP+HSV+lab color channels</td>
<td style="text-align:center">LSTM融合后再级联各时间输出</td>
<td style="text-align:center">RankSVM</td>
<td style="text-align:left">对噪声鲁棒性强</td>
<td style="text-align:center">49.3</td>
<td style="text-align:center">64.1</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://github.com/niallmcl/Recurrent-Convolutional-Video-ReID"><em>Recurrent Convolutional Network for Video-based Person Re-identification</em></a></td>
<td style="text-align:center">Paul Miller<br>Queen's University Belfast</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">利用CNN提取空间特征，RNN提取空间特征</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">RNN + average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">Softmax Loss + Contrastive Loss</td>
<td style="text-align:center">58</td>
<td style="text-align:center">70</td>
<td style="text-align:center">Rank1 40</td>
</tr>
<tr>
<td style="text-align:center">Video-based Person Re-identification with Accumulative Motion Context</td>
<td style="text-align:center">JiaShi Feng<br>Hefei University of Technology</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:left">用网络学习光流，并整合到网络中</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">RNN+average pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 先用光流训练一个网络，让其能预测光流<br>2. 将光流网络加入到原网络中一起使用</td>
<td style="text-align:center">65.3</td>
<td style="text-align:center">78</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center">Learning Compact Appearance Representation for Video-based Person Re-identification</td>
<td style="text-align:center">Kan Liu<br>Shandong University</td>
<td style="text-align:center">Arxiv 2017.02</td>
<td style="text-align:left">从若干帧中提取特征而不是使用整个视频</td>
<td style="text-align:center">五层卷积网络</td>
<td style="text-align:center">Max Pooling</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 利用对于不同视频段提取到的特征，可以得到两个人之间的平均距离和最小距离<br>2. FEP选取关键帧</td>
<td style="text-align:center">60.4</td>
<td style="text-align:center">83.3</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center">See the Forest for the Trees: Joint Spatial and Temporal <br>Recurrent Neural Networks for Video-based Person Re-identification</td>
<td style="text-align:center">Tieniu Tan<br>UCAS,CASIA,CEBSIT</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">能挑出关键帧并充分利用环境信息</td>
<td style="text-align:center">CaffeNet</td>
<td style="text-align:center">TAM+SRM</td>
<td style="text-align:center">1. TAM输出特征间的标准化的欧氏距离<br>2.SRM输出的相似概率<br>3. 两个相似性度量的加权和</td>
<td style="text-align:left">1.时间循环网络对两个人各时间空间特征级联后的差值的六个方向用RNNN聚合，最后二分类，得到相似概率 <br>2. 时间上，每一步都接收所有时间的特征，学习加权值，得到特征的加权和，并将其送入RNN得到此时的表达，最终表达是各时间特征均值<br>3. 整体结构为三路与双路的结合</td>
<td style="text-align:center">55.2</td>
<td style="text-align:center">79.4</td>
<td style="text-align:center">Rank1 70.6<br>Map 50.7</td>
</tr>
<tr>
<td style="text-align:center"><em>Quality Aware Network for Set to Set Recognition</em></td>
<td style="text-align:center">Wanli Ouyang<br>University of Sydney</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">能自动学到图片的质量并用以加权图片特征</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">通过学习到的质量分数加权</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">代码中的升级版本<br>1. 三路网络，Triplet Loss,每一路又有一个分类Loss,正样本对又构建Contrastive loss(相当于只有正样本情况)<br>2. 对于每一个支路，都由GoogleNet组成，其后便是分类loss。每个支路中还有一个QAN网络，用于产生质量分数<br>3. QAN是两层卷积网络加全连接层，全连接输出维度为3，结构CPCPF，直接由原始图片数据学得<br>4. GoogleNet中间特征中沿高度均分得到三个特征，每个特征再均值池化压缩h维度。每个支路的QAN输出的3个数值标准化后分别对其加权<br>5. 加权后的特征经过L2 Norm便得到最后表达，进入Triplet Loss与Contrastive Loss</td>
<td style="text-align:center">68.0</td>
<td style="text-align:center">90.3</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://github.com/shuangjiexu/Spatial-Temporal-Pooling-Networks-ReID">Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-identification</a></td>
<td style="text-align:center">Pan Zhou<br>Huazhong University of Science and Technology</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:left">在空间上与时间上都是注意力模型</td>
<td style="text-align:center">三层CNN</td>
<td style="text-align:center">RNN+注意力时间池化</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 双路结构，分类loss+Contrastive Loss<br>2. 对每一个支路，输入为原始图片加光流，对于每一帧的特征用SPP得到不同尺度的特征并级联，得到单帧表达<br>3. 将每一帧的表达依次送入RNN，每一步的输出为每一帧的最终表达<br>4. 利用注意力模型得到每一帧的加权值，利用加权求和得到视频表达</td>
<td style="text-align:center">62</td>
<td style="text-align:center">77</td>
<td style="text-align:center">Rank1 44</td>
</tr>
<tr>
<td style="text-align:center">A Two Stream Siamese Convolutional Neural Network For Person Re-identifcation</td>
<td style="text-align:center">Dahjung Chuang<br>Purdue</td>
<td style="text-align:center">ICCV 2017</td>
<td style="text-align:left">将光流与RGB分开，分别在两个Siamese网络中</td>
<td style="text-align:center">RNN+注意力时间池化</td>
<td style="text-align:center">光流与RGB的Euclidean Distance</td>
<td style="text-align:center">3层CNN</td>
<td style="text-align:left">在RNN-ReID结构上，用两个相同结构的Siamese网络，分别提取RGB与光流中的特征，loss与特征是两者的加权</td>
<td style="text-align:center">60</td>
<td style="text-align:center">78</td>
<td style="text-align:center">--</td>
</tr>
<tr>
<td style="text-align:center">Region-based Quality Estimation Network for Large-scale Person Re-identification</td>
<td style="text-align:center">Shaofan Cai<br>SenseTime</td>
<td style="text-align:center">Arxiv 201711</td>
<td style="text-align:left">借助关键点检测，基于区域的质量估计，并提出新的视频数据集</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">Region-based quality</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:left">1. 之前的数据集因为检测或跟踪失败而导致清洁度太低，人工标注的又对齐的太好<br>2. 新数据集特点：590000张图片，检测子检测，场景拥挤，年龄分布大<br>3. 用CPM检测关键点，产生上中下三个框，基于框预测三个框的质量分数<br>4. 对所有帧的同一个框的质量分数L1标准化，求特征加权和，最后级联三个框的特征</td>
<td style="text-align:center">76.1</td>
<td style="text-align:center">92.4</td>
<td style="text-align:center">Rank1 77.83 Map 71.14</td>
</tr>
</tbody>
</table>
<hr>
<h1>Metric</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Relaxed Pairwise Learned Metric for Person Re-identification</td>
<td style="text-align:center">Horst Bischof<br>Graz University of Technology</td>
<td style="text-align:center">ECCV 2012</td>
<td style="text-align:left">从不同摄像头下的采样中学习矩阵，注重摄像头之间的变换</td>
<td style="text-align:center">Color + LBP</td>
<td style="text-align:center">马氏距离</td>
<td style="text-align:left">在距离度量学习前先对特征进行PCA降维</td>
<td style="text-align:left"><strong>VIPeR</strong> 27<br><strong>PRID</strong> 15</td>
</tr>
<tr>
<td style="text-align:center">Deep Metric Learning for Practical Person Re-identification</td>
<td style="text-align:center">Stan Z. Li<br> NLPR, CASIA</td>
<td style="text-align:center">ICPR 2014</td>
<td style="text-align:left">提出一个更通用的方式去从原始图片上学习距离度量</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Cosine + Binomial Distance</td>
<td style="text-align:left">1. 双路网络，当做二分类，输出相似度<br>2. 每一支路分为三个小支路，分别输入图片的上中下三部分，最后级联再经全连接得到最后表达</td>
<td style="text-align:left"><strong>VIPeR</strong> 34.4<br><strong>PRID</strong> 17.9</td>
</tr>
<tr>
<td style="text-align:center">Multi-shot Re-identification with Random-Projection-Based Random Forests</td>
<td style="text-align:center">Richard J. Radke<br>RPI</td>
<td style="text-align:center">WACV 2015</td>
<td style="text-align:left">基于视频的距离度量学习</td>
<td style="text-align:center">Color Histograms<br>Schmid &amp; Gabor Filters</td>
<td style="text-align:center">随机森林输出的相似性值</td>
<td style="text-align:left">1. 通过随机投影对图片的特征向量降维<br>2. 在投影出的亚空间中，基于对层面训练随机森林<br>3. 随机投影增加了随机森林的分类多样性<br>4. 融合多个视频帧的方法：计算两者所有图片对的相似性值，再取平均</td>
<td style="text-align:left"><strong>3DPeS</strong> 43(估计)</td>
</tr>
<tr>
<td style="text-align:center"><em>Person Re-identification by Local Maximal Occurrence Representation and Metric Learning</em></td>
<td style="text-align:center">Stan Z. Li<br>NLPR</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:left">新的手工特征和距离学习方法</td>
<td style="text-align:center">SILTP histograms<br>Color Bins</td>
<td style="text-align:center">在kissme的基础上加入了低维投影</td>
<td style="text-align:left">1. 选取特征时有一系列的子窗口，并对窗口特征做max pooling<br>为了获得多尺度信息，用了有三种大小的图片金字塔</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 52.20 detected 46.25<br><strong>VIPeR</strong> 40.00<br><strong>GRID</strong> 16.56</td>
</tr>
<tr>
<td style="text-align:center"><em>Embedding Deep Metric for Person Re-identification: A Study Against Large Variations</em></td>
<td style="text-align:center">Stan Z. Li<br>NLPR</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">提供了新的正样本对采集方法以及距离度量的方法</td>
<td style="text-align:center">CNN</td>
<td style="text-align:center">Euclidean Distance</td>
<td style="text-align:left">1. 构成正样本对时，应选取与样本距离小的一些图片，距离太大的样本对会有害训练<br>2. 用全连接层将马氏距离的学习转化为欧氏距离</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 61.32 detected 52.09<br><strong>CUHK01</strong> (100) 86.59<br><strong>VIPeR</strong> 40.91</td>
</tr>
<tr>
<td style="text-align:center">Re-ranking Person Re-identification with k-reciprocal Encoding</td>
<td style="text-align:center">Shaozi Li<br>Xiamen University</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">对排序得到的结果再次处理重排</td>
<td style="text-align:center">CaffeNet</td>
<td style="text-align:center">Jaccard Distance + L2 Distance</td>
<td style="text-align:left">1. 利用近邻关系组成集合，生成Jaccard Distance<br>2. 最后的距离是两种距离的加权和</td>
<td style="text-align:left"><strong>Market 1501</strong> SQ 77.11<br><strong>CUHK03</strong> detected 61.6 manually 58.5<br><strong>MARS</strong> 73.94<br><strong>PRW</strong> 52.54</td>
</tr>
<tr>
<td style="text-align:center">Scalable Person Re-identification on Supervised Smoothed Manifold</td>
<td style="text-align:center">Qi Tian<br>UTSA</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">对获得的相似性矩阵再处理，获得平滑的流形相似性度量</td>
<td style="text-align:center">LOMO,GOG,ELF6</td>
<td style="text-align:center">欧氏距离及其他相似性度量方式</td>
<td style="text-align:left">1. 通过转移矩阵不断迭代<br>2. 可以和其他距离度量方法协同使用，先提取特征，再进行距离度量学习，然后用这个方法优化相似性矩阵，得到最后的结果</td>
<td style="text-align:left"><strong>CUHK03</strong>SQ manually 76.6 detected 72.7<br><strong>VIPeR</strong> 53.73<br><strong>PRID450S</strong> 72.98</td>
</tr>
</tbody>
</table>
<hr>
<h1>Loss</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">loss</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification</td>
<td style="text-align:center">Chi Zhang<br>Megvii</td>
<td style="text-align:center">Arxiv 2017 Oct</td>
<td style="text-align:left">限制最大的正样本对距离小于最小的负样本对距离</td>
<td style="text-align:center">Resnet50-X</td>
<td style="text-align:center">标准化的欧式距离</td>
<td style="text-align:left">对于整个batch，找到最大的正样本对距离，和最小的副样本对距离，让他们距离超过margin</td>
<td style="text-align:left">输入为P个人，每人K个图片</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 87.5<br><strong>Market 1501</strong><br>SQ rank 88.9 map 76.7<br><strong>MARS</strong><br>SQ rank 84.2 map 74.6</td>
</tr>
<tr>
<td style="text-align:center">[In Defense of the Triplet Loss for Person Re-identification</td>
<td style="text-align:center">Bastian Leibe<br>RWTH Aachen University](<a href="https://github.com/VisualComputingInstitute/triplet-reid">https://github.com/VisualComputingInstitute/triplet-reid</a>)</td>
<td style="text-align:center">Arxiv 201711</td>
<td style="text-align:left">在一个batch中，寻找最困难的正负样本组成三元组</td>
<td style="text-align:center">Resnet50 or LuNet</td>
<td style="text-align:center">欧氏距离</td>
<td style="text-align:left">1. 一个batch中有P个人，每个人K张图片<br>2. 对每个人，每一张图片，在batch内寻找最困难的正样本与负样本计算triplet loss<br>3. 最后一共有PK个loss用于计算和平均</td>
<td style="text-align:left">用了soft-margin</td>
<td style="text-align:left"><strong>CUHK03</strong> manually 89.63 detected 87.58<br><strong>Market 1501</strong><br>SQ rank 86.67 map 81.07<br>MQ rank 91.75 map 87.18<br><strong>MARS</strong><br>MQ rank 81.21 map 77.43</td>
</tr>
</tbody>
</table>
<h1>New Perspective</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Recurrent Attention Models for Depth-Based Person Identification</td>
<td style="text-align:center">Li FeiFei<br>Stanford University</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">数据集是人的深度信息，立体的，无RGB信息</td>
<td style="text-align:center">--</td>
<td style="text-align:center">--</td>
<td style="text-align:left">因数据集较大，结合了循环注意力模型，自动选择下一个关注点</td>
<td style="text-align:left">--</td>
</tr>
<tr>
<td style="text-align:center">End-to-End Deep Learning for Person Search</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">将检测与匹配结合起来做</td>
<td style="text-align:center">Faster RCNN</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 分类的时候，一个batch只有少数图片，但整体类别很多，所以Softmax目标会很稀疏<br>2. 提出随机采样的Softmax loss，即每次随机选取Softmax神经元的一个子集</td>
<td style="text-align:left">--</td>
</tr>
<tr>
<td style="text-align:center">Person Search with Natural Language Description</td>
<td style="text-align:center">Xiaogang Wang<br>CUHK</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">根据自然语言描述去搜索人物</td>
<td style="text-align:center">VGG16</td>
<td style="text-align:center">--</td>
<td style="text-align:left">1. 单元级的注意力与单词级的门控制</td>
<td style="text-align:left">--</td>
</tr>
<tr>
<td style="text-align:center">GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval</td>
<td style="text-align:center">Qi Tian<br>UTSA</td>
<td style="text-align:center">MM 2017</td>
<td style="text-align:left">基于pose提取特征，并视为检索问题，在匹配时预分类库图片以加速</td>
<td style="text-align:center">GoogleNet</td>
<td style="text-align:center">Softmax Score</td>
<td style="text-align:left">1. 网络整体为分类网络<br>2. 先用pose检测模型提取头，脖子，以及臀部这四个关键点，将人体分为上中下三个部分<br>3对各个部分以及整体，各用一个支路提取特征，网络权值共享，每个支路又有自己单独的分类loss<br>4. 在检索时，先实现将库图片分成不同的group，并pca降维，取整个group的特征的平均作为整个group的表达，再进行检索，加快速度，用的特征是将四个通道输出的特征级联</td>
<td style="text-align:left"><strong>Market 1501</strong> SQ rank 89.9 map 73.9<br>MQ rank 81.5 map 61.2<br><strong>CUHK03</strong><br>manually 85.0<br>detected 82.2<br><strong>VIPeR</strong> 54.8</td>
</tr>
<tr>
<td style="text-align:center">Pose Invariant Embedding for Deep Person Re-identification</td>
<td style="text-align:center">Liang Zheng<br>UTS</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:left">为解决行人匹配时的误对齐问题，加入关键点信息</td>
<td style="text-align:center">AlexNet or ResNet-50</td>
<td style="text-align:center">L2norm + Euclidean Distance</td>
<td style="text-align:left">1. 用CPM提取pose,10个关键点<br>2. 用关键点设置特征提取框，框根据位置做相应的仿射变换，所以框有可能是斜着提取的,再将各部分拼一块组成一张图片，即PoseBox<br>3. 当关键点的自信值小于某个门限时，会加入一些随机扰动<br>4. 单路三支流，分别输入原始图像，PoseBox，各个特征点的自信值<br>5. 各支路提取的特征级联后经过一个全连接得到最后表达<br>6. 三个Loss，最后特征表达一个分类Loss，原始图特征一个分类Loss,PoseBox一个分类Loss<br><a href="../../../data/lab/images/piedpr.png">Architecture</a></td>
<td style="text-align:left"><strong>Market 1501</strong><br>SQ rank 79.33 map 55.95<br><strong>CUHK03</strong><br>detected rank 67.10 map 71.32<br><strong>VIPeR</strong> 27.44</td>
</tr>
</tbody>
</table>
<h2>当用肢体关键点框出感兴趣区域后，随之而来的一个问题便是有一些标志性的物体会被排除在外，比如包，雨伞等等</h2>
<hr>
<h1>Attribute</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Metric</th>
<th style="text-align:center">Detecter</th>
<th style="text-align:left">Detail</th>
<th style="text-align:left">Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Person Re-identification by Attributes</td>
<td style="text-align:center">Shaogang Gong<br>QMUL</td>
<td style="text-align:center">BMVC 2012</td>
<td style="text-align:left">用属性辅助识别，标注了VIPeR数据集</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)</td>
<td style="text-align:center">低维特征用巴氏距离，属性特征用欧氏距离</td>
<td style="text-align:center">SVM</td>
<td style="text-align:left">1. 属性检测子都是在VIPeR上训练的，其他数据集上直接用训好的检测子<br>2. 属性是可以高度依赖于视角或者人的姿态<br>3. 标注VIPeR时，分为三个大类：前方，后方，侧面<br>4. 对每个属性训练相应的检测子时确保三个角度的正样本都有，因此检测子有视角不变性<br>5. 属性的加权是在各个数据集上单独做的<br>6. 除了各个属性距离要加权，最后的属性距离与各种低维特征间也要加权求和<br>7. 在加入属性后，Rank1，VIPeR上准确率降低了一点，iLIDS上提升了，在Rank5上都提升了<br><strong>问题</strong> 定义的一些与视角敏感的属性，应该会有损性能吧，因为不同的视角下，虽是同一个人，但是此属性却一个正一个负</td>
<td style="text-align:left"><strong>VIPeR</strong> 16.5<br><strong>iLIDS</strong> 52.1</td>
</tr>
<tr>
<td style="text-align:center"><em>Deep Attributes Driven Multi-camera Person Re-identification</em></td>
<td style="text-align:center">Qi Tian<br>Peking University</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">利用行人属性辅助行人重识别</td>
<td style="text-align:center">AlexNet</td>
<td style="text-align:center">Cosine Distance</td>
<td style="text-align:center">AlexNet</td>
<td style="text-align:left">1. 第一阶段，用一个带属性的独立数据集训练网络，并用该网络为目标数据集初始化属性标签<br>2. 第二阶段，基于属性的Triplet Loss,将属性与ID结合起来训练，让同一个人的属性相似，不同人的属性相差较远<br>3. 第三阶段，为目标数据集重新标定属性标签，将独立属性数据及与此相结合，用其微调属性预测网络</td>
<td style="text-align:left"><strong>VIPeR</strong> 43.5<br><strong>PRID</strong> 22.6<br><strong>GRID</strong> 22.4</td>
</tr>
<tr>
<td style="text-align:center">Attributes-Based Re-identification</td>
<td style="text-align:center">Gong, Shaogang<br>Queen Mary Unifying of London</td>
<td style="text-align:center">Springer London 2014</td>
<td style="text-align:left">将属性与Re-ID结合，标注了PRID数据集</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)</td>
<td style="text-align:center">加权欧氏距离</td>
<td style="text-align:center">LIBSVM and investigate Linear, RBF, X2 and Intersection kernels</td>
<td style="text-align:left">1. 一些属性在数据集中有很多正样本，但有的属性只有少数正样本<br>2. 对于每一个属性，用所有的正样本训练，负样本用相同数量的剩余数据的欠抽样<br>3. 用低层次特征训练属性分类器，由此将高维特征映射到低维的语义属性空间<br>4. 判断距离时，要分别计算各个属性或者低层次特征的距离，再给每个都分配一个加权值,最后使用加权后的距离<br>5. 当人工标出Probe图片的属性去匹配时（gallery还是用检测器得到属性），效果没有用检测器的好，可能是因为虽然检测器再标库的属性时会引入误差，但是在标Probe时也会引入相同的误差</td>
<td style="text-align:left"><strong>PRID</strong> 41.5<br><strong>VIPeR</strong> 21.4</td>
</tr>
<tr>
<td style="text-align:center">Pedestrian Attribute Recognition At Far Distance</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">MM 2014</td>
<td style="text-align:left">标了一个远距离下行人属性数据集，任务是预测行人属性</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)</td>
<td style="text-align:center">--</td>
<td style="text-align:center">ikSVM<br>MRF with Gaussian kernel<br>MRF with random forest</td>
<td style="text-align:left">1. 用 Markov Random Field(MRF) 探索邻近图片间的上下文关系<br>2. MRF能量函数由 Unary Cost 和 Pairwise Cost组成<br>3. unary cost 利用预测属性分类概率（由ikSVM学习）的log函数构成<br>4. 用随机森林去学习Pairwise Cost</td>
<td style="text-align:left"><strong>PETA</strong> 71.1</td>
</tr>
<tr>
<td style="text-align:center">Re-id: Hunting Attributes in the wild</td>
<td style="text-align:center">Shaogang Gong<br>QMUL</td>
<td style="text-align:center">BMVC 2014</td>
<td style="text-align:left">从网络上爬取图片，用以训练属性检测子，以解决大数据集标注属性的问题</td>
<td style="text-align:center">BoG(属性) + 低层特征</td>
<td style="text-align:center">加权欧氏距离</td>
<td style="text-align:center">LDA</td>
<td style="text-align:left">1. 用其他文章提供的行人检测子框出行人，并删去一些不合适的图<br>2. 每张图片的元数据要先预处理，再得到 BoW 表达<br>3. 再用 self-tuning Spectral Clustering 聚成若干类，视为潜在的属性,并用来训练LDA获得属性分类器</td>
<td style="text-align:left"><strong>VIPeR</strong> 17<br><strong>GRID</strong> 22<br><strong>PRID</strong> 4<br><strong>CUHK01</strong> 9</td>
</tr>
<tr>
<td style="text-align:center">Multi-Task Learning with Low Rank Attribute Embedding for Person Re-identification</td>
<td style="text-align:center">Qi Tian<br>Unifying of Texas at San Antonio</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:left">将属性特征与低层次特征结合起来帮助行人重识别</td>
<td style="text-align:center">Color Channels<br>Texture Filters(Schmid &amp; Gabor)</td>
<td style="text-align:center">欧式距离</td>
<td style="text-align:center">PRID，VIPeR : binary SVMs<br>iLIDS,SAIVT-SoftBio : MRFr</td>
<td style="text-align:left">1. 这里的Task指的是不同的摄像头<br>2. 属性之间是相关的，故用一个低秩矩阵Z将原属性映射到一个Embedding空间，可以将一些缺失的属性补全</td>
<td style="text-align:left"><strong>iLIDS</strong> 43.0<br><strong>PRID</strong> 18.0<br><strong>VIPeR</strong> 42.3</td>
</tr>
<tr>
<td style="text-align:center">Improving Person Re-identification by Attribute and Identity Learning</td>
<td style="text-align:center">Liang Zheng<br>University of Technology Sydney</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:left">主要研究属性标签如何在大规模学习问题上帮助Re-ID</td>
<td style="text-align:center">--</td>
<td style="text-align:center">--</td>
<td style="text-align:center">--</td>
<td style="text-align:left">这里的属性主要是与ID层面的属性，比如性别，年龄，而不是持续时间短的，或属于外界环境的属性，比如打电话，骑自行车</td>
<td style="text-align:left">--</td>
</tr>
</tbody>
</table>
<ul>
<li>属性的正负样本之间的不平衡，以及有的属性正样本太少</li>
</ul>
<hr>
<h1>Pose Estimation</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Detail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Convolutional Pose Machines</td>
<td style="text-align:center">Yaser Sheikh<br>CMU</td>
<td style="text-align:center">CVPR 2016</td>
<td style="text-align:left">用很深的网络不断调整预测</td>
<td style="text-align:left">1. 整体类似RNN,分为很多步<br>2. 第一步输入是用七层网络提取的各个关节点的自信图<br>3. 之后的每个阶段是一样的model，是5层卷积网，输入为前一阶段的自信图以及对原始图提取的特征<br>4. 每一阶段都会额外增加一个loss,是预测与真实自信图的误差，用以减轻梯度消失问题</td>
</tr>
<tr>
<td style="text-align:center">Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos</td>
<td style="text-align:center">Otmar Hilliges<br>ETH Zurich</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">能端到端的训练，能同时表达交界处以及他们之间的时空关系</td>
<td style="text-align:left">1. 先训练CPM，再与后面的网络结合起来优化<br>2. 对于前后帧，用弹簧能量模型定义变形损失</td>
</tr>
<tr>
<td style="text-align:center">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fileds</td>
<td style="text-align:center">Yaser Sheikh<br>CMU</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">定义新的表达来更好的处理多人关节点估计</td>
<td style="text-align:left">1. PAF是同一个人两个相邻关节点之间的向量场，有方向<br>2. 网络分为两路，一路用CPM预测自信图，另一路预测PAF<br>3.PAF主要解决多人情况下关节点的划分问题</td>
</tr>
</tbody>
</table>
<hr>
<h1>Dataset</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:center">Name of Dataset</th>
<th style="text-align:center">Label method</th>
<th style="text-align:center">Video or Image</th>
<th style="text-align:center">Cammera</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Person Re-identification in the Wild</td>
<td style="text-align:center">LIang Zheng<br>UTS</td>
<td style="text-align:center">CVPR 2017</td>
<td style="text-align:left">提供一个端到端的大数据集，将行人检测与匹配一起做</td>
<td style="text-align:center">PRW</td>
<td style="text-align:center">hand</td>
<td style="text-align:center">image</td>
<td style="text-align:center">6</td>
</tr>
<tr>
<td style="text-align:center"><em>MARS: A Video Benchmark for Large-Scale Person Re-identification</em></td>
<td style="text-align:center">Qi Tian<br>Tsinghua University</td>
<td style="text-align:center">ECCV 2016</td>
<td style="text-align:left">基于视频的检测子检测的Re-ID数据集，<br>并阐述了在大数据集下，分类网络要比双路或者三路网络更好</td>
<td style="text-align:center">MARS</td>
<td style="text-align:center">detected</td>
<td style="text-align:center">Video</td>
<td style="text-align:center">6</td>
</tr>
</tbody>
</table>
<hr>
<h1>Expansion</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Detail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Local Fisher Discriminant Analysis for Supervised Dimensionality Reduction</td>
<td style="text-align:center">Masashi Sugiyama<br>Tokyo Institute pf Technology</td>
<td style="text-align:center">ICML 2006</td>
<td style="text-align:left">传统Fisher Discriminant分析对于从若干独立簇中的类的采样没有区分性</td>
<td style="text-align:left">考虑数据的内部结构，将FDA于LPP结合</td>
</tr>
<tr>
<td style="text-align:center">A Spatial-Temporal Descriptor Based on 3D-Gradients</td>
<td style="text-align:center">Cordelia Schmid<br>INRIA Grenoble</td>
<td style="text-align:center">BMVC 2008</td>
<td style="text-align:left">基于视频的时空描述子</td>
<td style="text-align:left">将一个cell中的累加梯度值量化到中二十面体的面中心方向</td>
</tr>
<tr>
<td style="text-align:center"><em>Large Scale Metric Learning from Equivalence Constraints</em></td>
<td style="text-align:center">Horst Bischof<br>Graz University of Technology</td>
<td style="text-align:center">CVPR 2012</td>
<td style="text-align:left">从统计推理的角度学习距离度量，不依赖于复杂的算法</td>
<td style="text-align:left">利用最大似然估计得到马氏距离度量矩阵</td>
</tr>
<tr>
<td style="text-align:center">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</td>
<td style="text-align:center">Lior Worf<br>Tel Aviv University</td>
<td style="text-align:center">CVPR 2014</td>
<td style="text-align:left">联合对齐与表达操作</td>
<td style="text-align:left">1. 用3D Face来对齐<br>2. 9层网络提特征</td>
</tr>
<tr>
<td style="text-align:center">Deep Learning Face Representation by Joint Identification-Verification</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">NIPS 2014</td>
<td style="text-align:left">用multi-task加强特征学习</td>
<td style="text-align:left">双路，每一路都有一个Softmax classification loss。两路联合有一个Contrastive loss。</td>
</tr>
<tr>
<td style="text-align:center">Deep Learning Face Representation from Predicting 10000 Classes</td>
<td style="text-align:center">Xiaoou Tang<br>CUHK</td>
<td style="text-align:center">CVPR 2014</td>
<td style="text-align:left">用网络提取高层次特征</td>
<td style="text-align:left">最后的特征表达维度只有160维</td>
</tr>
<tr>
<td style="text-align:center">Two-Stream Convolutional Networks for Action Recognition in Videos</td>
<td style="text-align:center">Andrew Zisserman<br>Oxford</td>
<td style="text-align:center">NIPS 2014</td>
<td style="text-align:left">双路结构处理时空信息</td>
<td style="text-align:left">网络一个支路输入图片另一个支路输入光流</td>
</tr>
<tr>
<td style="text-align:center"><em>Recurrent Models of Visual Attention</em></td>
<td style="text-align:center">Koray Kavukcuoglu<br>Google DeepMind</td>
<td style="text-align:center">NIPS 2014</td>
<td style="text-align:left">每次只看图片的一小块，网络会自动寻找下一次观察的点</td>
<td style="text-align:left">1. 主体为RNN，每次输入整个图片和观察点坐标<br>2. 每一步输出两个分支，一个分类，另一个预测下一个位置<br>3. 使用增强学习，每一步分类对了reward为1，否则为0</td>
</tr>
<tr>
<td style="text-align:center">Large-scale Video Classification with Convolutional Neural Networks</td>
<td style="text-align:center">Li FeiFei<br>Stanford University</td>
<td style="text-align:center">CVPR 2014</td>
<td style="text-align:left">利用多分辨率与漏斗状网络结构来更好的利用局部时空信息</td>
<td style="text-align:left">1. 语境流：从低分辨率帧中学习特征<br>2. 中央流：从帧的中心部分的高分辨率区学习特征</td>
</tr>
<tr>
<td style="text-align:center">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</td>
<td style="text-align:center">Cordelia Schmid<br>Inria</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:left">更好地处理冲突与运动边界的光流估计</td>
<td style="text-align:left">1. 从稀疏匹配的边缘保留插值的密匹配<br>2. 用密匹配初始化的方差能量最小化</td>
</tr>
<tr>
<td style="text-align:center">FlowNet： Learning Optical Flow with Convolutional Networks</td>
<td style="text-align:center">Vladimir Golkov<br>Technical University of Munich</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:left">用网络提取光流</td>
<td style="text-align:left">通过一系列的卷积与反卷积操作</td>
</tr>
<tr>
<td style="text-align:center">Deep Captioning with Multimodal Recurrent Neural Networks</td>
<td style="text-align:center">Junhua Mao<br>UCLA</td>
<td style="text-align:center">ICLR 2015</td>
<td style="text-align:left">用多模型RNN去处理自然图片说明</td>
<td style="text-align:left">1. 为语言和图片分别构建模型，然后融合两者的信息<br>2. RNN的每一步输入都是某一单词的语言模型的输出<br>3. 每一步的RNN输出，语言模型输出，图像模型输出三者分别通过三个矩阵投影到一个共同的空间，再元素级相加得到融合后的表达</td>
</tr>
<tr>
<td style="text-align:center"><em>Learning Spatiotemporal Features with 3D Convolutional Networks</em></td>
<td style="text-align:center">Manohar Paluri<br>Facebook AI Research</td>
<td style="text-align:center">ICCV 2015</td>
<td style="text-align:left">3D卷积核去处理视频</td>
<td style="text-align:left">3D卷积核能有效学习时间与空间特征</td>
</tr>
<tr>
<td style="text-align:center">MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching</td>
<td style="text-align:center">Alexander C. Berg<br>University of North Carolina at Chapel Hill</td>
<td style="text-align:center">CVPR 2015</td>
<td style="text-align:left">块匹配与特征学习一起做</td>
<td style="text-align:left">两个支路通过全连接融合为一路，全连接层则相当于距离度量</td>
</tr>
<tr>
<td style="text-align:center">Deep Mutual Learning</td>
<td style="text-align:center">Huchuan Lu<br>Dalian University of Technology, China</td>
<td style="text-align:center">Arxiv 201706</td>
<td style="text-align:left">两个网络相互学习</td>
<td style="text-align:left">1. 两个网络都对同一个输入做预测，各有一个分类loss，同时两者之间有一个二者输出的概率分布的KL距离的loss。<br>2. 优化时，交替优化，直至收敛</td>
</tr>
</tbody>
</table>
<hr>
<h1>Network Architecture</h1>
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Author</th>
<th style="text-align:center">Conference &amp; Year</th>
<th style="text-align:left">Motivation</th>
<th style="text-align:left">Detail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Densely Connected Convolutional Networks</td>
<td style="text-align:center">Kilian Q. Weinberger<br>Cornell University</td>
<td style="text-align:center">CVPR 2017 (best)</td>
<td style="text-align:left">卷积层间密集连接，特征图重用</td>
<td style="text-align:left">1. 网络可以很深，分为很多个block<br>2. 每个block由多个卷积层构成，每一层的特征图都会送到该block内它后面的所有卷积层<br>3. block内每一层的通道数不能太大，block之间用1x1卷积压缩通道数</td>
</tr>
<tr>
<td style="text-align:center">Sequeeze-and-Excitation Networks</td>
<td style="text-align:center">Jie Hu<br>Momenta</td>
<td style="text-align:center">Arxiv 2017</td>
<td style="text-align:left">对channels进行加权</td>
<td style="text-align:left">1. 要处理的特征X<br>2. 先Global Average Pooling,得到C维特征<br>3. C维特征经过一层全连接，为降低参数数量，输出为 C/r 维，r为超参数<br>4. 经过relu，再经过一层全连接，输出C维<br>5. 对原特征各通道相乘加权，得到处理后的表达</td>
</tr>
</tbody>
</table>
<hr>
<p></p><div align="right">Updated Date: 2017/12/13</div><p></p></body>
</html>
